{
 "<h1>Proximal Policy Optimization - PPO</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of <a href=\"https://papers.labml.ai/paper/1707.06347\">Proximal Policy Optimization - PPO</a>.</p>\n<p>PPO is a policy gradient method for reinforcement learning. Simple policy gradient methods do a single gradient update per sample (or a set of samples). Doing multiple gradient steps for a single sample causes problems because the policy deviates too much, producing a bad policy. PPO lets us do multiple gradient updates per sample by trying to keep the policy close to the policy that was used to sample data. It does so by clipping gradient flow if the updated policy is not close to the policy used to sample the data.</p>\n<p>You can find an experiment that uses it <a href=\"experiment.html\">here</a>. The experiment uses <a href=\"gae.html\">Generalized Advantage Estimation</a>.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/ppo/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1>\u8fd1\u7aef\u7b56\u7565\u4f18\u5316-PPO</h1>\n<p>\u8fd9\u662f P <a href=\"https://pytorch.org\">yTorch</a> \u5b9e\u73b0\u7684<a href=\"https://papers.labml.ai/paper/1707.06347\">\u8fd1\u7aef\u7b56\u7565\u4f18\u5316-PPO</a>\u3002</p>\n<p>PPO \u662f\u4e00\u79cd\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002\u7b80\u5355\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5bf9\u6bcf\u4e2a\u6837\u672c\uff08\u6216\u4e00\u7ec4\u6837\u672c\uff09\u8fdb\u884c\u4e00\u6b21\u68af\u5ea6\u66f4\u65b0\u3002\u5bf9\u5355\u4e2a\u6837\u672c\u6267\u884c\u591a\u4e2a\u68af\u5ea6\u6b65\u9aa4\u4f1a\u5bfc\u81f4\u95ee\u9898\uff0c\u56e0\u4e3a\u7b56\u7565\u504f\u5dee\u592a\u5927\uff0c\u4ece\u800c\u4ea7\u751f\u9519\u8bef\u7684\u7b56\u7565\u3002PPO \u5141\u8bb8\u6211\u4eec\u5728\u6bcf\u4e2a\u6837\u672c\u4e2d\u8fdb\u884c\u591a\u6b21\u68af\u5ea6\u66f4\u65b0\uff0c\u65b9\u6cd5\u662f\u5c3d\u91cf\u4f7f\u7b56\u7565\u4e0e\u7528\u4e8e\u91c7\u6837\u6570\u636e\u7684\u7b56\u7565\u4fdd\u6301\u4e00\u81f4\u3002\u5982\u679c\u66f4\u65b0\u540e\u7684\u7b56\u7565\u4e0e\u7528\u4e8e\u91c7\u6837\u6570\u636e\u7684\u7b56\u7565\u4e0d\u63a5\u8fd1\uff0c\u5219\u901a\u8fc7\u524a\u51cf\u68af\u5ea6\u6d41\u6765\u5b9e\u73b0\u6b64\u76ee\u7684\u3002</p>\n<p>\u4f60\u53ef\u4ee5<a href=\"experiment.html\">\u5728\u8fd9\u91cc</a>\u627e\u5230\u4e00\u4e2a\u4f7f\u7528\u5b83\u7684\u5b9e\u9a8c\u3002\u8be5\u5b9e\u9a8c\u4f7f\u7528<a href=\"gae.html\">\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/ppo/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>Clipped Value Function Loss</h2>\n<p>Similarly we clip the value function update also.</p>\n<span translate=no>_^_0_^_</span><p>Clipping makes sure the value function <span translate=no>_^_1_^_</span> doesn&#x27;t deviate  significantly from <span translate=no>_^_2_^_</span>.</p>\n": "<h2>\u524a\u51cf\u503c\u51fd\u6570\u635f\u5931</h2>\n<p>\u540c\u6837\uff0c\u6211\u4eec\u4e5f\u88c1\u526a\u503c\u51fd\u6570\u7684\u66f4\u65b0\u3002</p>\n<span translate=no>_^_0_^_</span><p>\u88c1\u526a\u53ef\u786e\u4fdd\u503c\u51fd\u6570<span translate=no>_^_1_^_</span>\u4e0d\u4f1a\u660e\u663e\u504f\u79bb<span translate=no>_^_2_^_</span>\u3002</p>\n",
 "<h2>PPO Loss</h2>\n<p>Here&#x27;s how the PPO update rule is derived.</p>\n<p>We want to maximize policy reward  <span translate=no>_^_0_^_</span>  where <span translate=no>_^_1_^_</span> is the reward, <span translate=no>_^_2_^_</span> is the policy, <span translate=no>_^_3_^_</span> is a trajectory sampled from policy,  and <span translate=no>_^_4_^_</span> is the discount factor between <span translate=no>_^_5_^_</span>.</p>\n<span translate=no>_^_6_^_</span><p>So,  <span translate=no>_^_7_^_</span></p>\n<p>Define discounted-future state distribution,  <span translate=no>_^_8_^_</span></p>\n<p>Then,</p>\n<span translate=no>_^_9_^_</span><p>Importance sampling <span translate=no>_^_10_^_</span> from <span translate=no>_^_11_^_</span>,</p>\n<span translate=no>_^_12_^_</span><p>Then we assume <span translate=no>_^_13_^_</span> and <span translate=no>_^_14_^_</span> are similar. The error we introduce to <span translate=no>_^_15_^_</span>  by this assumption is bound by the KL divergence between  <span translate=no>_^_16_^_</span> and <span translate=no>_^_17_^_</span>. <a href=\"https://papers.labml.ai/paper/1705.10528\">Constrained Policy Optimization</a>  shows the proof of this. I haven&#x27;t read it.</p>\n<span translate=no>_^_18_^_</span>": "<h2>PPO \u635f\u5931</h2>\n<p>\u4ee5\u4e0b\u662f PPO \u66f4\u65b0\u89c4\u5219\u7684\u6d3e\u751f\u65b9\u5f0f\u3002</p>\n<p>\u6211\u4eec\u5e0c\u671b\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4fdd\u5355\u5956\u52b1<span translate=no>_^_0_^_</span>\u5728\u54ea\u91cc<span translate=no>_^_1_^_</span>\uff0c<span translate=no>_^_2_^_</span>\u5956\u52b1\u5728\u54ea\u91cc\uff0c<span translate=no>_^_3_^_</span>\u662f\u4fdd\u5355\uff0c\u662f\u4ece\u4fdd\u5355\u4e2d\u62bd\u6837\u7684\u8f68\u8ff9\uff0c<span translate=no>_^_4_^_</span>\u662f\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u7684\u6298\u6263\u7cfb\u6570<span translate=no>_^_5_^_</span>\u3002</p>\n<span translate=no>_^_6_^_</span><p>\u6240\u4ee5\uff0c<span translate=no>_^_7_^_</span></p>\n<p>\u5b9a\u4e49\u6298\u6263\u672a\u6765\u72b6\u6001\u5206\u914d\uff0c<span translate=no>_^_8_^_</span></p>\n<p>\u90a3\u4e48\uff0c</p>\n<span translate=no>_^_9_^_</span><p>\u91cd\u8981\u6027\u62bd\u6837<span translate=no>_^_10_^_</span>\u6765\u81ea<span translate=no>_^_11_^_</span></p>\n<span translate=no>_^_12_^_</span><p>\u7136\u540e\u6211\u4eec\u5047\u8bbe<span translate=no>_^_13_^_</span>\u548c<span translate=no>_^_14_^_</span>\u662f\u76f8\u4f3c\u7684\u3002\u6211\u4eec<span translate=no>_^_15_^_</span>\u901a\u8fc7\u8fd9\u4e2a\u5047\u8bbe\u5f15\u5165\u7684\u8bef\u5dee\u53d7<span translate=no>_^_16_^_</span>\u548c\u4e4b\u95f4\u7684 KL \u5dee\u5f02\u7684\u7ea6\u675f<span translate=no>_^_17_^_</span>\u3002<a href=\"https://papers.labml.ai/paper/1705.10528\">\u7ea6\u675f\u7b56\u7565\u4f18\u5316</a>\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u6211\u8fd8\u6ca1\u770b\u8fc7\u3002</p>\n<span translate=no>_^_18_^_</span>",
 "<h3>Cliping the policy ratio</h3>\n<span translate=no>_^_0_^_</span><p>The ratio is clipped to be close to 1. We take the minimum so that the gradient will only pull <span translate=no>_^_1_^_</span> towards <span translate=no>_^_2_^_</span> if the ratio is not between <span translate=no>_^_3_^_</span> and <span translate=no>_^_4_^_</span>. This keeps the KL divergence between <span translate=no>_^_5_^_</span>  and <span translate=no>_^_6_^_</span> constrained. Large deviation can cause performance collapse;  where the policy performance drops and doesn&#x27;t recover because  we are sampling from a bad policy.</p>\n<p>Using the normalized advantage  <span translate=no>_^_7_^_</span>  introduces a bias to the policy gradient estimator,  but it reduces variance a lot. </p>\n": "<h3>\u524a\u51cf\u4fdd\u5355\u6bd4\u7387</h3>\n<span translate=no>_^_0_^_</span><p>\u8be5\u6bd4\u7387\u88ab\u88c1\u526a\u4e3a\u63a5\u8fd1 1\u3002\u6211\u4eec\u53d6\u6700\u5c0f\u503c\uff0c\u4ee5\u4fbf\u53ea\u6709\u5f53\u6bd4\u7387\u4e0d\u5728<span translate=no>_^_3_^_</span>\u548c\u4e4b\u95f4\u65f6\uff0c\u68af\u5ea6\u624d\u4f1a\u62c9<span translate=no>_^_1_^_</span>\u5411<span translate=no>_^_2_^_</span><span translate=no>_^_4_^_</span>\u3002\u8fd9\u4fdd\u6301\u4e86 KL \u4e4b\u95f4\u7684\u5dee\u5f02<span translate=no>_^_5_^_</span>\u548c<span translate=no>_^_6_^_</span>\u9650\u5236\u3002\u8f83\u5927\u7684\u504f\u5dee\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7b56\u7565\u6027\u80fd\u4f1a\u4e0b\u964d\u4e14\u65e0\u6cd5\u6062\u590d\uff0c\u56e0\u4e3a\u6211\u4eec\u6b63\u5728\u4ece\u4e0d\u826f\u7b56\u7565\u4e2d\u62bd\u6837\u3002</p>\n<p>\u4f7f\u7528\u5f52\u4e00\u5316\u4f18\u52bf\u4f1a\u7ed9\u653f\u7b56\u68af\u5ea6\u4f30\u8ba1\u5668<span translate=no>_^_7_^_</span>\u5e26\u6765\u504f\u5dee\uff0c\u4f46\u5b83\u5927\u5927\u51cf\u5c11\u4e86\u65b9\u5dee\u3002</p>\n",
 "<p>ratio <span translate=no>_^_0_^_</span>; <em>this is different from rewards</em> <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u6bd4\u4f8b<span translate=no>_^_0_^_</span>\uff1b<em>\u8fd9\u4e0e\u5956\u52b1\u4e0d\u540c</em><span translate=no>_^_1_^_</span>\u3002</p>\n",
 "An annotated implementation of Proximal Policy Optimization - PPO algorithm in PyTorch.": "PyTorch \u4e2d\u8fd1\u7aef\u7b56\u7565\u4f18\u5316-PPO \u7b97\u6cd5\u7684\u5e26\u6ce8\u91ca\u5b9e\u73b0\u3002",
 "Proximal Policy Optimization - PPO": "\u8fd1\u7aef\u7b56\u7565\u4f18\u5316-PPO"
}