{
 "<h1>Deep Q Networks (DQN)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of paper  <a href=\"https://papers.labml.ai/paper/1312.5602\">Playing Atari with Deep Reinforcement Learning</a>  along with <a href=\"model.html\">Dueling Network</a>, <a href=\"replay_buffer.html\">Prioritized Replay</a>  and Double Q Network.</p>\n<p>Here is the <a href=\"experiment.html\">experiment</a> and <a href=\"model.html\">model</a> implementation.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb\"><span translate=no>_^_0_^_</span></a> <a href=\"https://app.labml.ai/run/fe1ad986237511ec86e8b763a2d3f710\"><span translate=no>_^_1_^_</span></a></p>\n": "<h1>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4Q \u0da2\u0dcf\u0dbd (DQN)</h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2 <a href=\"https://papers.labml.ai/paper/1312.5602\">Atari \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc3\u0dd9\u0dbd\u0dca\u0dbd\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0dc1\u0d9a\u0dca\u0dad\u0dd2\u0db8\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8</a> \u0dc3\u0dc4 <a href=\"model.html\">\u0da9\u0dd4\u0dc0\u0dbd\u0dd2\u0d82 \u0da2\u0dcf\u0dbd\u0dba</a> , <a href=\"replay_buffer.html\">\u0db4\u0dca\u0dbb\u0db8\u0dd4\u0d9b\u0dad\u0dcf \u0db1\u0dd0\u0dc0\u0dad \u0db0\u0dcf\u0dc0\u0db1\u0dba</a> \u0dc3\u0dc4 \u0daf\u0dca\u0dc0\u0dd2\u0dad\u0dca\u0dc0 Q \u0da2\u0dcf\u0dbd\u0dba \u0dc3\u0db8\u0d9f. </p>\n<p>\u0db8\u0dd9\u0db1\u0dca\u0db1 <a href=\"experiment.html\">\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf</a> \u0db6\u0dd0\u0dbd\u0dd3\u0db8 \u0dc3\u0dc4 <a href=\"model.html\">\u0d86\u0daf\u0dbb\u0dca\u0dc1</a> \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8. </p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb\"><span translate=no>_^_0_^_</span></a> <a href=\"https://app.labml.ai/run/fe1ad986237511ec86e8b763a2d3f710\"> <span translate=no>_^_1_^_</span></a></p>\n",
 "<h2>Train the model</h2>\n<p>We want to find optimal action-value function.</p>\n<span translate=no>_^_0_^_</span><h3>Target network \ud83c\udfaf</h3>\n<p>In order to improve stability we use experience replay that randomly sample from previous experience <span translate=no>_^_1_^_</span>. We also use a Q network with a separate set of parameters <span translate=no>_^_2_^_</span> to calculate the target. <span translate=no>_^_3_^_</span> is updated periodically. This is according to paper <a href=\"https://deepmind.com/research/dqn/\">Human Level Control Through Deep Reinforcement Learning</a>.</p>\n<p>So the loss function is, <span translate=no>_^_4_^_</span></p>\n<h3>Double <span translate=no>_^_5_^_</span>-Learning</h3>\n<p>The max operator in the above calculation uses same network for both selecting the best action and for evaluating the value. That is, <span translate=no>_^_6_^_</span> We use <a href=\"https://papers.labml.ai/paper/1509.06461\">double Q-learning</a>, where the <span translate=no>_^_7_^_</span> is taken from <span translate=no>_^_8_^_</span> and the value is taken from <span translate=no>_^_9_^_</span>.</p>\n<p>And the loss function becomes,</p>\n<span translate=no>_^_10_^_</span>": "<h2>\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1</h2>\n<p>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0d9a\u0dcf\u0dbb\u0dd3 \u0d85\u0d9c\u0dba \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba \u0dc3\u0ddc\u0dba\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba\u0dba.</p>\n<span translate=no>_^_0_^_</span><h3>\u0d89\u0dbd\u0d9a\u0dca\u0d9a \u0da2\u0dcf\u0dbd\u0dba \ud83c\udfaf</h3>\n<p>\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb\u0dad\u0dca\u0dc0\u0dba \u0dc0\u0dd0\u0da9\u0dd2 \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4\u0dd2 \u0db4\u0dd9\u0dbb \u0d85\u0dad\u0dca\u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0d85\u0dc4\u0db9\u0dd4 \u0dbd\u0dd9\u0dc3 \u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd2\u0dba \u0d85\u0dad\u0dca\u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0dca \u0db1\u0dd0\u0dc0\u0dad \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4<span translate=no>_^_1_^_</span>. \u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dd9\u0db1\u0db8 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca<span translate=no>_^_2_^_</span> \u0dc3\u0db8\u0dd6\u0dc4\u0dba\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad Q \u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dca \u0daf \u0d85\u0db4\u0dd2 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4. <span translate=no>_^_3_^_</span>\u0dc0\u0dbb\u0dd2\u0db1\u0dca \u0dc0\u0dbb \u0dba\u0dcf\u0dc0\u0dad\u0dca\u0d9a\u0dcf\u0dbd\u0dd3\u0db1 \u0dc0\u0dda. \u0db8\u0dd9\u0dba \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0dc1\u0d9a\u0dca\u0dad\u0dd2\u0db8\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8 \u0dad\u0dd4\u0dc5\u0dd2\u0db1\u0dca \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 <a href=\"https://deepmind.com/research/dqn/\">\u0db8\u0dcf\u0db1\u0dc0 \u0db8\u0da7\u0dca\u0da7\u0db8\u0dca \u0db4\u0dcf\u0dbd\u0db1\u0dba\u0da7 \u0d85\u0db1\u0dd4\u0dc0</a> \u0dba.</p>\n<p>\u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0db4\u0dcf\u0da9\u0dd4 \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba \u0dc0\u0db1\u0dca\u0db1\u0dda,<span translate=no>_^_4_^_</span></p>\n<h3><span translate=no>_^_5_^_</span>\u0daf\u0dca\u0dc0\u0dd2\u0dad\u0dca\u0dc0-\u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca</h3>\n<p>\u0d89\u0dc4\u0dad \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d8b\u0db4\u0dbb\u0dd2\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0d9a\u0dbb\u0dd4 \u0dc4\u0ddc\u0db3\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dc0 \u0dad\u0ddd\u0dbb\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0dc4 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8 \u0d87\u0d9c\u0dba\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d91\u0d9a\u0db8 \u0da2\u0dcf\u0dbd\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0db1\u0db8\u0dca,<span translate=no>_^_6_^_</span> \u0d85\u0db4\u0dd2 <a href=\"https://papers.labml.ai/paper/1509.06461\">\u0daf\u0dca\u0dc0\u0dd2\u0dad\u0dca\u0dc0 Q- \u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8</a> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4,<span translate=no>_^_7_^_</span> \u0d91\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1\u0dda \u0d9a\u0ddc\u0dad\u0dd0\u0db1\u0dd2\u0db1\u0dca\u0daf<span translate=no>_^_8_^_</span> \u0dc3\u0dc4 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8 \u0dbd\u0db6\u0dcf<span translate=no>_^_9_^_</span> \u0d9c\u0db1\u0dd3.</p>\n<p>\u0db4\u0dcf\u0da9\u0dd4 \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba \u0db6\u0dc0\u0da7 \u0db4\u0dad\u0dca\u0dc0\u0dda,</p>\n<span translate=no>_^_10_^_</span>",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p>Calculate the desired Q value. We multiply by <span translate=no>_^_0_^_</span> to zero out the next state Q values if the game ended.</p>\n<p><span translate=no>_^_1_^_</span> </p>\n": "<p>\u0d85\u0db4\u0dda\u0d9a\u0dca\u0dc2\u0dd2\u0dadQ \u0d85\u0d9c\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0d9a\u0dca\u0dbb\u0dd3\u0da9\u0dcf\u0dc0 \u0d85\u0dc0\u0dc3\u0db1\u0dca \u0dc0\u0dd6\u0dba\u0dda \u0db1\u0db8\u0dca \u0d8a\u0dc5\u0d9f \u0dbb\u0dcf\u0da2\u0dca\u0dba Q \u0d85\u0d9c\u0dba\u0db1\u0dca \u0dc1\u0dd4\u0db1\u0dca\u0dba <span translate=no>_^_0_^_</span> \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0dd2 \u0d9c\u0dd4\u0dab \u0d9a\u0dbb\u0db8\u0dd4. </p>\n<p><span translate=no>_^_1_^_</span> </p>\n",
 "<p>Get the best action at state <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0dbb\u0dcf\u0da2\u0dca\u0dba\u0dba\u0dd9\u0db1\u0dca\u0dc4\u0ddc\u0db3\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dc0 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Get the q value from the target network for the best action at state <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0dbb\u0dcf\u0da2\u0dca\u0dba\u0dc4\u0ddc\u0db3\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dc0 \u0dc3\u0db3\u0dc4\u0dcf \u0d89\u0dbd\u0d9a\u0dca\u0d9a \u0da2\u0dcf\u0dbd\u0dba\u0dd9\u0db1\u0dca q \u0d85\u0d9c\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Get weighted means </p>\n": "<p>\u0db6\u0dbb\u0dad\u0dd0\u0db6\u0dd6 \u0d9a\u0dca\u0dbb\u0db8 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Gradients shouldn&#x27;t propagate gradients <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a\u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0db4\u0dca\u0dbb\u0da0\u0dcf\u0dbb\u0dba \u0db1\u0ddc\u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4\u0dba <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Temporal difference error <span translate=no>_^_0_^_</span> is used to weigh samples in replay buffer </p>\n": "<p>\u0db1\u0dd0\u0dc0\u0dad\u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0db6\u0dc6\u0dbb\u0dba\u0dda \u0dc3\u0dcf\u0db8\u0dca\u0db4\u0dbd \u0d9a\u0dd2\u0dbb\u0dcf \u0db8\u0dd0\u0db1 \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0da7 \u0dad\u0dcf\u0dc0\u0d9a\u0dcf\u0dbd\u0dd2\u0d9a \u0dc0\u0dd9\u0db1\u0dc3 \u0daf\u0ddd\u0dc2\u0dba <span translate=no>_^_0_^_</span> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2 </p>\n",
 "<p>We take <a href=\"https://en.wikipedia.org/wiki/Huber_loss\">Huber loss</a> instead of mean squared error loss because it is less sensitive to outliers </p>\n": "<p>\u0d91\u0dbaoutliers \u0d85\u0da9\u0dd4 \u0dc3\u0d82\u0dc0\u0dda\u0daf\u0dd3 \u0db1\u0dd2\u0dc3\u0dcf \u0d85\u0db4\u0dd2 \u0d92 \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba \u0dc0\u0dbb\u0dca\u0d9c \u0daf\u0ddd\u0dc2\u0dba\u0d9a\u0dca \u0d85\u0dc4\u0dd2\u0db8\u0dd2 <a href=\"https://en.wikipedia.org/wiki/Huber_loss\">Huber</a> \u0d85\u0dc4\u0dd2\u0db8\u0dd2 \u0d9c\u0dad </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> - <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> - <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> - <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> - <span translate=no>_^_7_^_</span> </li>\n<li><span translate=no>_^_8_^_</span> - whether the game ended after taking the action </li>\n<li><span translate=no>_^_9_^_</span> - <span translate=no>_^_10_^_</span> </li>\n<li><span translate=no>_^_11_^_</span> - weights of the samples from prioritized experienced replay</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> - <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> - <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> - <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> - <span translate=no>_^_7_^_</span> </li>\n<li><span translate=no>_^_8_^_</span> - \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0d9a\u0dca\u0dbb\u0dd3\u0da9\u0dcf\u0dc0 \u0d85\u0dc0\u0dc3\u0db1\u0dca \u0dc0\u0dd6\u0dc0\u0dcf\u0daf \u0dba\u0db1\u0dca\u0db1 </li>\n<li><span translate=no>_^_9_^_</span> - <span translate=no>_^_10_^_</span> </li>\n<li><span translate=no>_^_11_^_</span> - \u0db4\u0dca\u0dbb\u0db8\u0dd4\u0d9b\u0dad\u0dcf\u0dc0\u0dba \u0db4\u0dc5\u0db4\u0dd4\u0dbb\u0dd4\u0daf\u0dd4 \u0db1\u0dd0\u0dc0\u0dad \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0dc3\u0dd2\u0da7 \u0dc3\u0dcf\u0db8\u0dca\u0db4\u0dbd \u0db6\u0dbb</li></ul>\n",
 "Deep Q Networks (DQN)": "\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 Q \u0da2\u0dcf\u0dbd (DQN)",
 "This is a PyTorch implementation/tutorial of Deep Q Networks (DQN) from paper Playing Atari with Deep Reinforcement Learning. This includes dueling network architecture, a prioritized replay buffer and double-Q-network training.": "\u0db8\u0dd9\u0dba PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba\u0d9a\u0dd2 \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 Q \u0db1\u0dd9\u0da7\u0dca\u0dc0\u0dbb\u0dca\u0d9a\u0dca\u0dc3\u0dca (DQN) \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc3\u0dd9\u0dbd\u0dca\u0dbd\u0db8\u0dca \u0d85\u0da7\u0dcf\u0dbb\u0dd2 \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0dc1\u0d9a\u0dca\u0dad\u0dd2\u0db8\u0dad\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8 \u0dc3\u0db8\u0d9f. \u0da2\u0dcf\u0dbd \u0d9c\u0dd8\u0dc4 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab \u0dc1\u0dd2\u0dbd\u0dca\u0db4\u0dba \u0da9\u0db6\u0dbd\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8, \u0db4\u0dca\u0dbb\u0db8\u0dd4\u0d9b\u0dad\u0dcf \u0db1\u0dd0\u0dc0\u0dad \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0db6\u0dc6\u0dbb\u0dba\u0d9a\u0dca \u0dc3\u0dc4 \u0daf\u0dca\u0dc0\u0dd2\u0dad\u0dca\u0dc0 Q-\u0da2\u0dcf\u0dbd \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0dc0 \u0db8\u0dd9\u0dba\u0da7 \u0d87\u0dad\u0dd4\u0dc5\u0dad\u0dca \u0dba."
}