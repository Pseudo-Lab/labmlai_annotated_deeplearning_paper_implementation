{
 "<h1>Deep Q Networks (DQN)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of paper  <a href=\"https://papers.labml.ai/paper/1312.5602\">Playing Atari with Deep Reinforcement Learning</a>  along with <a href=\"model.html\">Dueling Network</a>, <a href=\"replay_buffer.html\">Prioritized Replay</a>  and Double Q Network.</p>\n<p>Here is the <a href=\"experiment.html\">experiment</a> and <a href=\"model.html\">model</a> implementation.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1>\u6df1\u5ea6 Q \u7f51\u7edc (DQN)</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0\u7684 PyTorch <a href=\"https://papers.labml.ai/paper/1312.5602\">\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u73a9\u96c5</a>\u8fbe\u5229\u4ee5\u53ca<a href=\"model.html\">\u51b3\u6597\u7f51\u7edc</a>\u3001<a href=\"replay_buffer.html\">\u4f18\u5148\u56de\u653e</a>\u548c Double Q Network\u3002</p>\n<p>\u8fd9\u662f<a href=\"experiment.html\">\u5b9e\u9a8c</a>\u548c<a href=\"model.html\">\u6a21\u578b</a>\u5b9e\u73b0\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>Train the model</h2>\n<p>We want to find optimal action-value function.</p>\n<span translate=no>_^_0_^_</span><h3>Target network \ud83c\udfaf</h3>\n<p>In order to improve stability we use experience replay that randomly sample from previous experience <span translate=no>_^_1_^_</span>. We also use a Q network with a separate set of parameters <span translate=no>_^_2_^_</span> to calculate the target. <span translate=no>_^_3_^_</span> is updated periodically. This is according to paper <a href=\"https://deepmind.com/research/dqn/\">Human Level Control Through Deep Reinforcement Learning</a>.</p>\n<p>So the loss function is, <span translate=no>_^_4_^_</span></p>\n<h3>Double <span translate=no>_^_5_^_</span>-Learning</h3>\n<p>The max operator in the above calculation uses same network for both selecting the best action and for evaluating the value. That is, <span translate=no>_^_6_^_</span> We use <a href=\"https://papers.labml.ai/paper/1509.06461\">double Q-learning</a>, where the <span translate=no>_^_7_^_</span> is taken from <span translate=no>_^_8_^_</span> and the value is taken from <span translate=no>_^_9_^_</span>.</p>\n<p>And the loss function becomes,</p>\n<span translate=no>_^_10_^_</span>": "<h2>\u8bad\u7ec3\u6a21\u578b</h2>\n<p>\u6211\u4eec\u60f3\u627e\u5230\u6700\u4f73\u7684\u52a8\u4f5c\u503c\u51fd\u6570\u3002</p>\n<span translate=no>_^_0_^_</span><h3>\u76ee\u6807\u7f51\u7edc \ud83c\udfaf</h3>\n<p>\u4e3a\u4e86\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u7ecf\u9a8c\u56de\u653e\uff0c\u4ece\u4ee5\u524d\u7684\u7ecf\u9a8c\u4e2d\u968f\u673a\u62bd\u6837<span translate=no>_^_1_^_</span>\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u5177\u6709\u4e00\u7ec4\u5355\u72ec\u53c2\u6570\u7684 Q \u7f51\u7edc<span translate=no>_^_2_^_</span>\u6765\u8ba1\u7b97\u76ee\u6807\u3002<span translate=no>_^_3_^_</span>\u5b9a\u671f\u66f4\u65b0\u3002\u8fd9\u662f\u6839\u636e\u8bba\u6587\u300a\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c<a href=\"https://deepmind.com/research/dqn/\">\u4eba\u4f53\u6c34\u5e73\u63a7\u5236</a>\u300b\u5f97\u51fa\u7684\u3002</p>\n<p>\u6240\u4ee5\u635f\u5931\u51fd\u6570\u662f\uff0c<span translate=no>_^_4_^_</span></p>\n<h3>\u53cc<span translate=no>_^_5_^_</span>\u91cd\u5b66\u4e60</h3>\n<p>\u4e0a\u8ff0\u8ba1\u7b97\u4e2d\u7684\u6700\u5927\u503c\u8fd0\u7b97\u7b26\u4f7f\u7528\u76f8\u540c\u7684\u7f51\u7edc\u6765\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u548c\u8bc4\u4f30\u503c\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c<span translate=no>_^_6_^_</span>\u6211\u4eec\u4f7f\u7528<a href=\"https://papers.labml.ai/paper/1509.06461\">\u53cc\u91cdQ-L</a><span translate=no>_^_7_^_</span> earning<span translate=no>_^_8_^_</span>\uff0c\u5176\u4e2d\u53d6\u81ea\u503c\uff0c\u53d6\u81ea\u503c<span translate=no>_^_9_^_</span>\u3002</p>\n<p>\u635f\u5931\u51fd\u6570\u53d8\u6210\uff0c</p>\n<span translate=no>_^_10_^_</span>",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the desired Q value. We multiply by <span translate=no>_^_0_^_</span> to zero out the next state Q values if the game ended.</p>\n<p><span translate=no>_^_1_^_</span> </p>\n": "<p>\u8ba1\u7b97\u6240\u9700\u7684 Q \u503c\u3002\u5982\u679c\u6e38\u620f\u7ed3\u675f\uff0c\u6211\u4eec\u5c06\u4e58<span translate=no>_^_0_^_</span>\u4ee5\u5c06\u4e0b\u4e00\u4e2a\u72b6\u6001 Q \u503c\u5f52\u96f6\u3002</p>\n<p><span translate=no>_^_1_^_</span></p>\n",
 "<p>Get the best action at state <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n": "<p>\u5728\u5dde\u5185\u91c7\u53d6\u6700\u4f73\u884c\u52a8<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span></p>\n",
 "<p>Get the q value from the target network for the best action at state <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </p>\n": "<p>\u4ece\u76ee\u6807\u7f51\u7edc\u83b7\u53d6 q \u503c\uff0c\u4ee5\u4fbf\u5728\u5dde\u5185\u91c7\u53d6\u6700\u4f73\u884c\u52a8<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span></p>\n",
 "<p>Get weighted means </p>\n": "<p>\u83b7\u53d6\u52a0\u6743\u5747\u503c</p>\n",
 "<p>Gradients shouldn&#x27;t propagate gradients <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6e10\u53d8\u4e0d\u5e94\u4f20\u64ad\u6e10\u53d8<span translate=no>_^_0_^_</span></p>\n",
 "<p>Temporal difference error <span translate=no>_^_0_^_</span> is used to weigh samples in replay buffer </p>\n": "<p>\u65f6\u5dee\u8bef\u5dee<span translate=no>_^_0_^_</span>\u7528\u4e8e\u79f0\u91cf\u91cd\u653e\u7f13\u51b2\u533a\u4e2d\u7684\u6837\u672c</p>\n",
 "<p>We take <a href=\"https://en.wikipedia.org/wiki/Huber_loss\">Huber loss</a> instead of mean squared error loss because it is less sensitive to outliers </p>\n": "<p>\u6211\u4eec\u91c7\u7528 <a href=\"https://en.wikipedia.org/wiki/Huber_loss\">Huber \u635f\u5931</a>\u800c\u4e0d\u662f\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u56e0\u4e3a\u5b83\u5bf9\u5f02\u5e38\u503c\u4e0d\u592a\u654f\u611f</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> - <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> - <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> - <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> - <span translate=no>_^_7_^_</span> </li>\n<li><span translate=no>_^_8_^_</span> - whether the game ended after taking the action </li>\n<li><span translate=no>_^_9_^_</span> - <span translate=no>_^_10_^_</span> </li>\n<li><span translate=no>_^_11_^_</span> - weights of the samples from prioritized experienced replay</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>-<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>-<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>-<span translate=no>_^_5_^_</span></li>\n<li><span translate=no>_^_6_^_</span>-<span translate=no>_^_7_^_</span></li>\n<li><span translate=no>_^_8_^_</span>-\u6e38\u620f\u5728\u91c7\u53d6\u884c\u52a8\u540e\u662f\u5426\u7ed3\u675f</li>\n<li><span translate=no>_^_9_^_</span>-<span translate=no>_^_10_^_</span></li>\n</ul><li><span translate=no>_^_11_^_</span>-\u6765\u81ea\u6709\u7ecf\u9a8c\u7684\u4f18\u5148\u91cd\u64ad\u7684\u6837\u672c\u7684\u6743\u91cd</li>\n",
 "Deep Q Networks (DQN)": "\u6df1\u5ea6\u95ee\u7b54\u7f51\u7edc (DQN)",
 "This is a PyTorch implementation/tutorial of Deep Q Networks (DQN) from paper Playing Atari with Deep Reinforcement Learning. This includes dueling network architecture, a prioritized replay buffer and double-Q-network training.": "\u8fd9\u662f Deep Q Networks\uff08DQN\uff09\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\uff0c\u6765\u81ea\u8bba\u6587\u300a\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u73a9\u96c5\u8fbe\u5229\u300b\u3002\u8fd9\u5305\u62ec\u51b3\u6597\u7f51\u7edc\u67b6\u6784\u3001\u4f18\u5148\u91cd\u64ad\u7f13\u51b2\u533a\u548c Double-Q-Network \u8bad\u7ec3\u3002"
}