{
 "<h1>Transformer XL</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1901.02860\">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Transformer has a limited attention span, equal to the length of the sequence trained in parallel. All these positions have a fixed positional encoding. Transformer XL increases this attention span by letting each of the positions pay attention to precalculated past embeddings. For instance if the context length is <span translate=no>_^_0_^_</span>, it will keep the embeddings of all layers for previous batch of length <span translate=no>_^_1_^_</span> and feed them to current step. If we use fixed-positional encodings these pre-calculated embeddings will have the same positions as the current context. They introduce relative positional encoding, where the positional encodings are introduced at the attention calculation.</p>\n<p>Annotated implementation of relative multi-headed attention is in <a href=\"relative_mha.html\"><span translate=no>_^_2_^_</span></a>.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> and a notebook for training a transformer XL model on Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a></p>\n": "<h1>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL</h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f PyTorch \u306e <a href=\"https://papers.labml.ai/paper/1901.02860\">Transformer-XL: \u56fa\u5b9a\u9577\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8d85\u3048\u305f\u6ce8\u610f\u6df1\u3044\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</a></p>\n<p>Transformer \u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30d1\u30f3\u306f\u3001\u4e26\u884c\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3068\u540c\u3058\u304f\u3089\u3044\u306e\u5236\u9650\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u4f4d\u7f6e\u306f\u3059\u3079\u3066\u56fa\u5b9a\u3055\u308c\u305f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002Transformer XL\u306f\u3001\u4e8b\u524d\u306b\u8a08\u7b97\u3055\u308c\u305f\u904e\u53bb\u306e\u57cb\u3081\u8fbc\u307f\u306b\u5404\u30dd\u30b8\u30b7\u30e7\u30f3\u306b\u6ce8\u76ee\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u3053\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30d1\u30f3\u3092\u5897\u3084\u3057\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u9577\u3055\u304c\u306e\u5834\u5408<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u524d\u306e\u30d0\u30c3\u30c1\u306e\u9577\u3055\u306e\u3059\u3079\u3066\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u57cb\u3081\u8fbc\u307f\u3092\u4fdd\u6301\u3057\u3001\u305d\u308c\u3089\u3092\u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u9001\u308a\u307e\u3059\u3002\u56fa\u5b9a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u3053\u308c\u3089\u306e\u4e8b\u524d\u306b\u8a08\u7b97\u3055\u308c\u305f\u57cb\u3081\u8fbc\u307f\u306f\u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3068\u540c\u3058\u4f4d\u7f6e\u306b\u306a\u308a\u307e\u3059\u3002\u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5c0e\u5165\u3055\u308c\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\u6642\u306b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5c0e\u5165\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u76f8\u5bfe\u7684\u591a\u9762\u7684\u6ce8\u610f\u306e\u6ce8\u91c8\u4ed8\u304d\u5b9f\u88c5\u304c\u5c0e\u5165\u3055\u308c\u307e\u3057\u305f\u3002<a href=\"relative_mha.html\"><span translate=no>_^_2_^_</span></a></p>\n<p>Tiny <a href=\"experiment.html\">Shakespeare\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3068\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u3059</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a></p>\n",
 "<h2>Transformer XL Layer</h2>\n<p>The transformer XL model comprises of a number of these layers.</p>\n": "<h2>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL \u30ec\u30a4\u30e4\u30fc</h2>\n<p>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30e2\u30c7\u30eb\u306f\u3001\u3053\u308c\u3089\u306e\u30ec\u30a4\u30e4\u30fc\u3092\u591a\u6570\u5099\u3048\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<h2>Transformer XL Model</h2>\n<p>This consists of multiple transformer XL layers</p>\n": "<h2>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL \u30e2\u30c7\u30eb</h2>\n<p>\u3053\u308c\u306f\u8907\u6570\u306e\u30c8\u30e9\u30f3\u30b9XL\u5c64\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Add the attention results </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7d50\u679c\u3092\u8ffd\u52a0</p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u7d50\u679c\u3092\u8ffd\u52a0\u3057\u76f4\u3059</p>\n",
 "<p>Add to the list of feature vectors </p>\n": "<p>\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0</p>\n",
 "<p>Attention </p>\n": "<p>\u6ce8\u610f</p>\n",
 "<p>Concatenate with <span translate=no>_^_0_^_</span> </p>\n": "<p>\u3068\u9023\u7d50 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7d42\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Finally, normalize the vectors </p>\n": "<p>\u6700\u5f8c\u306b\u3001\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u898f\u5316\u3057\u307e\u3059\u3002</p>\n",
 "<p>If there is memory </p>\n": "<p>\u30e1\u30e2\u30ea\u304c\u3042\u308c\u3070</p>\n",
 "<p>Ignore if there is no memory </p>\n": "<p>\u30e1\u30e2\u30ea\u304c\u306a\u3044\u5834\u5408\u306f\u7121\u8996\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>List to store token level feature vectors, which will become the memories for the next sequential batch. </p>\n": "<p>\u6b21\u306e\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d0\u30c3\u30c1\u306e\u30e1\u30e2\u30ea\u3068\u306a\u308b\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u683c\u7d0d\u3059\u308b\u30ea\u30b9\u30c8\u3002</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210</p>\n",
 "<p>Memory </p>\n": "<p>\u30e1\u30e2\u30ea\u30fc</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u7528\u306b\u6b63\u898f\u5316</p>\n",
 "<p>Normalize it </p>\n": "<p>\u6b63\u898f\u5316\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u884c\u3046\u524d\u306b\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u898f\u5316\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u904e</p>\n",
 "<p>Run through each transformer layer </p>\n": "<p>\u5404\u5909\u5727\u5668\u5c64\u306b\u901a\u3059</p>\n",
 "<p>Run through the transformer XL layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30ec\u30a4\u30e4\u30fc\u3092\u901a\u3059</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token embeddings vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a list of tensors of the past token level feature vectors of shape <span translate=no>_^_3_^_</span> for each layer </li>\n<li><span translate=no>_^_4_^_</span> is the masking matrix</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u904e\u53bb\u306e\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3001<span translate=no>_^_3_^_</span>\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u3067\u3059</li>\n<li><span translate=no>_^_4_^_</span>\u306f\u30de\u30b9\u30ad\u30f3\u30b0\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token level feature vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a tensor of the past token level feature vectors of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is a matrix of shape <span translate=no>_^_5_^_</span> or <span translate=no>_^_6_^_</span>. <span translate=no>_^_7_^_</span> is true if token at <span translate=no>_^_8_^_</span> can see token at <span translate=no>_^_9_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u904e\u53bb\u306e\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span><span translate=no>_^_6_^_</span>\u306f\u5f62\u72b6\u306e\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u304b<span translate=no>_^_7_^_</span>\u30c8\u30fc\u30af\u30f3 at \u304c at <span translate=no>_^_8_^_</span> \u306e\u30c8\u30fc\u30af\u30f3\u3092\u53c2\u7167\u3067\u304d\u308b\u5834\u5408\u306f true <span translate=no>_^_9_^_</span> \u306b\u306a\u308a\u307e\u3059\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the <a href=\"relative_mha.html\">self attention module</a> </li>\n<li><span translate=no>_^_2_^_</span> is the feed forward module </li>\n<li><span translate=no>_^_3_^_</span> is the probability of dropping out after self attention and FFN</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span><a href=\"relative_mha.html\">\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</a></li>\n<li><span translate=no>_^_2_^_</span>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3068FFN\u306e\u5f8c\u306b\u8131\u843d\u3059\u308b\u78ba\u7387\u3067\u3059</li></ul>\n",
 "Documented implementation with explanations of a Transformer-XL model.": "Transformer-XL \u30e2\u30c7\u30eb\u306e\u8aac\u660e\u3092\u542b\u3080\u5b9f\u88c5\u304c\u6587\u66f8\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002",
 "Transformer XL": "\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc XL"
}