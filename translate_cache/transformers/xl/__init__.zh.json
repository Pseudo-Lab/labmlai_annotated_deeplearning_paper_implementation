{
 "<h1>Transformer XL</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1901.02860\">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Transformer has a limited attention span, equal to the length of the sequence trained in parallel. All these positions have a fixed positional encoding. Transformer XL increases this attention span by letting each of the positions pay attention to precalculated past embeddings. For instance if the context length is <span translate=no>_^_0_^_</span>, it will keep the embeddings of all layers for previous batch of length <span translate=no>_^_1_^_</span> and feed them to current step. If we use fixed-positional encodings these pre-calculated embeddings will have the same positions as the current context. They introduce relative positional encoding, where the positional encodings are introduced at the attention calculation.</p>\n<p>Annotated implementation of relative multi-headed attention is in <a href=\"relative_mha.html\"><span translate=no>_^_2_^_</span></a>.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> and a notebook for training a transformer XL model on Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a></p>\n": "<h1>\u53d8\u538b\u5668 XL</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch \u4e2d Transfor</a> <a href=\"https://papers.labml.ai/paper/1901.02860\">mer-XL\uff1a\u8d85\u8d8a\u56fa\u5b9a\u957f\u5ea6\u4e0a\u4e0b\u6587\u7684\u4e13\u5fc3\u8bed\u8a00\u6a21\u578b</a>\u7684\u5b9e\u73b0\u3002</p>\n<p>Transformer \u7684\u6ce8\u610f\u529b\u8de8\u5ea6\u6709\u9650\uff0c\u7b49\u4e8e\u5e76\u884c\u8bad\u7ec3\u5e8f\u5217\u7684\u957f\u5ea6\u3002\u6240\u6709\u8fd9\u4e9b\u4f4d\u7f6e\u90fd\u6709\u56fa\u5b9a\u7684\u4f4d\u7f6e\u7f16\u7801\u3002Transformer XL \u901a\u8fc7\u8ba9\u6bcf\u4e2a\u4f4d\u7f6e\u5173\u6ce8\u8fc7\u53bb\u9884\u5148\u8ba1\u7b97\u7684\u5d4c\u5165\u6b21\u6570\uff0c\u4ece\u800c\u5ef6\u957f\u4e86\u8fd9\u79cd\u6ce8\u610f\u529b\u8de8\u5ea6\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e3a<span translate=no>_^_0_^_</span>\uff0c\u5b83\u5c06\u4fdd\u7559\u524d\u4e00\u6279\u957f\u5ea6\u7684\u6240\u6709\u5c42\u7684\u5d4c\u5165<span translate=no>_^_1_^_</span>\u5e76\u5c06\u5176\u9988\u9001\u5230\u5f53\u524d\u6b65\u9aa4\u3002\u5982\u679c\u6211\u4eec\u4f7f\u7528\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801\uff0c\u8fd9\u4e9b\u9884\u5148\u8ba1\u7b97\u7684\u5d4c\u5165\u5c06\u4e0e\u5f53\u524d\u4e0a\u4e0b\u6587\u5177\u6709\u76f8\u540c\u7684\u4f4d\u7f6e\u3002\u5b83\u4eec\u5f15\u5165\u4e86\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u5176\u4e2d\u4f4d\u7f6e\u7f16\u7801\u662f\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u65f6\u5f15\u5165\u7684\u3002</p>\n<p>\u76f8\u5bf9\u591a\u5934\u6ce8\u610f\u529b\u7684\u5e26\u6ce8\u91ca\u7684\u5b9e\u73b0\u5df2\u7ecf\u5f00\u59cb<a href=\"relative_mha.html\"><span translate=no>_^_2_^_</span></a>\u4e86\u3002</p>\n<p>\u8fd9\u662f\u7528\u4e8e<a href=\"experiment.html\">\u5728 Tiny Shakespeare \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 transformer XL \u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\u548c\u7b14\u8bb0\u672c\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/xl/experiment.ipynb\"><span translate=no>_^_3_^_</span></a></p>\n",
 "<h2>Transformer XL Layer</h2>\n<p>The transformer XL model comprises of a number of these layers.</p>\n": "<h2>\u53d8\u538b\u5668 XL \u5c42</h2>\n<p>\u53d8\u538b\u5668 XL \u6a21\u578b\u7531\u8bb8\u591a\u8fd9\u6837\u7684\u5c42\u7ec4\u6210\u3002</p>\n",
 "<h2>Transformer XL Model</h2>\n<p>This consists of multiple transformer XL layers</p>\n": "<h2>\u53d8\u538b\u5668 XL \u578b\u53f7</h2>\n<p>\u5b83\u7531\u591a\u4e2a\u53d8\u538b\u5668 XL \u5c42\u7ec4\u6210</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Add the attention results </p>\n": "<p>\u6dfb\u52a0\u5173\u6ce8\u7ed3\u679c</p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u5c06\u524d\u9988\u7ed3\u679c\u6dfb\u52a0\u56de\u6765</p>\n",
 "<p>Add to the list of feature vectors </p>\n": "<p>\u6dfb\u52a0\u5230\u7279\u5f81\u5411\u91cf\u5217\u8868\u4e2d</p>\n",
 "<p>Attention </p>\n": "<p>\u6ce8\u610f</p>\n",
 "<p>Concatenate with <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8fde\u63a5\u4e0e<span translate=no>_^_0_^_</span></p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7ec8\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>Finally, normalize the vectors </p>\n": "<p>\u6700\u540e\uff0c\u5bf9\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>If there is memory </p>\n": "<p>\u5982\u679c\u6709\u8bb0\u5fc6</p>\n",
 "<p>Ignore if there is no memory </p>\n": "<p>\u5982\u679c\u6ca1\u6709\u5185\u5b58\uff0c\u5219\u5ffd\u7565</p>\n",
 "<p>List to store token level feature vectors, which will become the memories for the next sequential batch. </p>\n": "<p>\u7528\u4e8e\u5b58\u50a8\u4ee4\u724c\u7ea7\u7279\u5f81\u5411\u91cf\u7684\u5217\u8868\uff0c\u8fd9\u4e9b\u5411\u91cf\u5c06\u6210\u4e3a\u4e0b\u4e00\u4e2a\u8fde\u7eed\u6279\u6b21\u7684\u8bb0\u5fc6\u3002</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u5236\u4f5c\u53d8\u538b\u5668\u5c42\u7684\u526f\u672c</p>\n",
 "<p>Memory </p>\n": "<p>\u8bb0\u5fc6</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u6807\u51c6\u5316\u4ee5\u8fdb\u884c\u524d\u9988</p>\n",
 "<p>Normalize it </p>\n": "<p>\u89c4\u8303\u5316\u5b83</p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u5728\u8fdb\u884c\u81ea\u6211\u6ce8\u610f\u4e4b\u524d\u5bf9\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u901a\u8fc7\u524d\u9988\u7f51\u7edc</p>\n",
 "<p>Run through each transformer layer </p>\n": "<p>\u7a7f\u8fc7\u6bcf\u4e2a\u53d8\u538b\u5668\u5c42</p>\n",
 "<p>Run through the transformer XL layer </p>\n": "<p>\u7a7f\u8fc7\u53d8\u538b\u5668 XL \u5c42</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token embeddings vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a list of tensors of the past token level feature vectors of shape <span translate=no>_^_3_^_</span> for each layer </li>\n<li><span translate=no>_^_4_^_</span> is the masking matrix</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5d4c\u5165\u5f62\u72b6\u5411\u91cf\u7684\u4ee4\u724c\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8fc7\u53bb\u4ee4\u724c\u7ea7\u522b\u7684\u5f20\u91cf\u5217\u8868\uff0c\u6bcf\u4e2a\u5c42\u7684\u5f62\u72b6<span translate=no>_^_3_^_</span>\u5411\u91cf\u7279\u5f81</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u63a9\u7801\u77e9\u9635</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token level feature vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a tensor of the past token level feature vectors of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is a matrix of shape <span translate=no>_^_5_^_</span> or <span translate=no>_^_6_^_</span>. <span translate=no>_^_7_^_</span> is true if token at <span translate=no>_^_8_^_</span> can see token at <span translate=no>_^_9_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4ee4\u724c\u7ea7\u7279\u5f81\u5f62\u72b6\u5411\u91cf\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8fc7\u53bb\u4ee4\u724c\u7ea7\u522b\u7279\u5f81\u5f62\u72b6\u5411\u91cf\u7684\u5f20\u91cf<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u662f\u5f62\u72b6\u7684\u77e9\u9635<span translate=no>_^_5_^_</span>\u6216<span translate=no>_^_6_^_</span>\u3002<span translate=no>_^_7_^_</span>\u5982\u679c token<span translate=no>_^_8_^_</span> \u53ef\u4ee5\u5728\u5904\u770b\u5230\u4ee4\u724c\uff0c\u5219\u4e3a true<span translate=no>_^_9_^_</span>\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the <a href=\"relative_mha.html\">self attention module</a> </li>\n<li><span translate=no>_^_2_^_</span> is the feed forward module </li>\n<li><span translate=no>_^_3_^_</span> is the probability of dropping out after self attention and FFN</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4ee4\u724c\u5d4c\u5165\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_1_^_</span>\u662f<a href=\"relative_mha.html\">\u81ea\u6211\u5173\u6ce8\u6a21\u5757</a></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u524d\u9988\u6a21\u5757</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u81ea\u6211\u5173\u6ce8\u548c FFN \u540e\u9000\u5b66\u7684\u6982\u7387</li></ul>\n",
 "Documented implementation with explanations of a Transformer-XL model.": "\u8bb0\u5f55\u4e86\u5b9e\u73b0\uff0c\u5e76\u89e3\u91ca\u4e86 Transformer-XL \u6a21\u578b\u3002",
 "Transformer XL": "\u53d8\u538b\u5668 XL"
}