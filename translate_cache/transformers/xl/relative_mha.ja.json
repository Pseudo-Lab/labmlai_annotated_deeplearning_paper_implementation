{
 "<h1>Relative Multi-Headed Attention</h1>\n<p>This is an implementation of relative multi-headed attention from paper <a href=\"https://papers.labml.ai/paper/1901.02860\">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n": "<h1>\u76f8\u5bfe\u7684\u591a\u9762\u7684\u6ce8\u610f</h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://papers.labml.ai/paper/1901.02860\">Transformer-XL: PyTorch \u306b\u304a\u3051\u308b\u56fa\u5b9a\u9577\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8d85\u3048\u305f\u6ce8\u610f\u306e\u884c\u304d\u5c4a\u3044\u305f\u8a00\u8a9e\u30e2\u30c7\u30eb\u300d\u306e\u6bd4\u8f03\u7684\u591a\u9762\u7684\u306a\u6ce8\u610f\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</a></p>\n",
 "<h2>Relative Multi-Head Attention Module</h2>\n<p>We override <a href=\"mha.html\">Multi-Head Attention</a> module so we only need to write the <span translate=no>_^_0_^_</span> method.</p>\n": "<h2>\u76f8\u5bfe\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb</h2>\n<p><a href=\"mha.html\">\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3059\u308b\u306e\u3067</a>\u3001<span translate=no>_^_0_^_</span>\u30e1\u30bd\u30c3\u30c9\u3092\u8a18\u8ff0\u3059\u308b\u3060\u3051\u3067\u6e08\u307f\u307e\u3059\u3002</p>\n",
 "<h3>Get relative attention scores</h3>\n<p>With absolute attention</p>\n<span translate=no>_^_0_^_</span><p>where <span translate=no>_^_1_^_</span>, are linear transformations of  original embeddings <span translate=no>_^_2_^_</span>  and <span translate=no>_^_3_^_</span> are linear transformations of  absolute positional encodings <span translate=no>_^_4_^_</span>.</p>\n<p>They reason out that the attention to a given key should be the same regardless of the position of query. Hence replace <span translate=no>_^_5_^_</span> with a constant <span translate=no>_^_6_^_</span>.</p>\n<p>For the second and third terms relative positional encodings are introduced. So <span translate=no>_^_7_^_</span> is replaced with <span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> with <span translate=no>_^_10_^_</span>.</p>\n<span translate=no>_^_11_^_</span>": "<h3>\u76f8\u5bfe\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30b3\u30a2\u3092\u53d6\u5f97</h3>\n<p>\u7d76\u5bfe\u7684\u306a\u6ce8\u610f\u3092\u6255\u3063\u3066</p>\n<span translate=no>_^_0_^_</span><p>\u3053\u3053\u3067<span translate=no>_^_1_^_</span>\u3001<span translate=no>_^_2_^_</span>\u306f\u5143\u306e\u57cb\u3081\u8fbc\u307f\u306e\u7dda\u5f62\u5909\u63db\u3067\u3001<span translate=no>_^_3_^_</span>\u306f\u7d76\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u7dda\u5f62\u5909\u63db\u3067\u3059\u3002<span translate=no>_^_4_^_</span></p>\n<p>\u5f7c\u3089\u306f\u3001\u7279\u5b9a\u306e\u30ad\u30fc\u3078\u306e\u6ce8\u610f\u306f\u3001\u30af\u30a8\u30ea\u306e\u4f4d\u7f6e\u306b\u95a2\u4fc2\u306a\u304f\u540c\u3058\u3067\u3042\u308b\u3079\u304d\u3060\u3068\u63a8\u8ad6\u3057\u3066\u3044\u307e\u3059\u3002\u3057\u305f\u304c\u3063\u3066\u3001<span translate=no>_^_5_^_</span>\u5b9a\u6570\u306b\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044<span translate=no>_^_6_^_</span>\u3002</p>\n<p>\u7b2c2\u7528\u8a9e\u3068\u7b2c3\u7528\u8a9e\u3067\u306f\u3001\u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304c\u5c0e\u5165\u3055\u308c\u3066\u3044\u307e\u3059\u3002<span translate=no>_^_7_^_</span>So \u306f\u3001<span translate=no>_^_8_^_</span>\u3068\u3001<span translate=no>_^_9_^_</span>\u306b\u7f6e\u304d\u63db\u3048\u3089\u308c\u307e\u3059<span translate=no>_^_10_^_</span>\u3002</p>\n<span translate=no>_^_11_^_</span>",
 "<p> </p>\n": "<p></p>\n",
 "<p> This method shifts <span translate=no>_^_0_^_</span> row of a matrix by <span translate=no>_^_1_^_</span> columns.</p>\n<p>If the input is <span translate=no>_^_2_^_</span>, the shifted result would be <span translate=no>_^_3_^_</span>. <em>Ideally we should mask out the lower triangle but it&#x27;s ok for our purpose</em>.</p>\n": "<p>\u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u884c\u5217\u306e\u884c\u3092\u5217\u3054\u3068\u306b\u30b7\u30d5\u30c8\u3057\u307e\u3059\u3002</p>\n<p>\u5165\u529b\u304c\u306e\u5834\u5408<span translate=no>_^_2_^_</span>\u3001\u30b7\u30d5\u30c8\u3055\u308c\u305f\u7d50\u679c\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002<span translate=no>_^_3_^_</span><em>\u4e0b\u306e\u4e09\u89d2\u5f62\u3092\u30de\u30b9\u30af\u3059\u308b\u306e\u304c\u7406\u60f3\u7684\u3067\u3059\u304c\u3001\u3053\u306e\u76ee\u7684\u306b\u306f\u554f\u984c\u3042\u308a\u307e\u305b\u3093</em>\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Concatenate a column of zeros </p>\n": "<p>0 \u306e\u5217\u3092\u9023\u7d50\u3059\u308b</p>\n",
 "<p>Number of relative positions </p>\n": "<p>\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6570</p>\n",
 "<p>Positional embeddings for the query is independent of the position of the query </p>\n": "<p>\u30af\u30a8\u30ea\u306e\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u306f\u30af\u30a8\u30ea\u306e\u4f4d\u7f6e\u3068\u306f\u7121\u95a2\u4fc2\u3067\u3059</p>\n",
 "<p>Relative positional embedding bias for key relative to the query. </p>\n": "<p>\u30af\u30a8\u30ea\u306b\u5bfe\u3059\u308b\u30ad\u30fc\u306e\u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u30d0\u30a4\u30a2\u30b9\u3002</p>\n",
 "<p>Relative positional embeddings for key relative to the query. We need <span translate=no>_^_0_^_</span> embeddings because the keys can be before or after the query. </p>\n": "<p>\u30af\u30a8\u30ea\u3092\u57fa\u6e96\u3068\u3057\u305f\u30ad\u30fc\u306e\u76f8\u5bfe\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3002\u30ad\u30fc\u306f\u30af\u30a8\u30ea\u306e\u524d\u3067\u3082\u5f8c\u3067\u3082\u69cb\u308f\u306a\u3044\u306e\u3067\u3001<span translate=no>_^_0_^_</span>\u57cb\u3081\u8fbc\u307f\u304c\u5fc5\u8981\u3067\u3059</p>\u3002\n",
 "<p>Remove extra positions </p>\n": "<p>\u4f59\u5206\u306a\u30dd\u30b8\u30b7\u30e7\u30f3\u3092\u524a\u9664</p>\n",
 "<p>Reshape and remove excess elements from the end </p>\n": "<p>\u5f62\u3092\u5909\u3048\u3066\u7aef\u304b\u3089\u4f59\u5206\u306a\u8981\u7d20\u3092\u53d6\u308a\u9664\u304f</p>\n",
 "<p>Return the sum <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5408\u8a08\u3092\u8fd4\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Shift the rows of <span translate=no>_^_0_^_</span> to get <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u884c\u3092\u30b7\u30d5\u30c8\u3059\u308b\u3068 <span translate=no>_^_1_^_</span></p>\n",
 "<p>The linear transformations do not need a bias since we explicitly include it when calculating scores. However having a bias for <span translate=no>_^_0_^_</span> might make sense. </p>\n": "<p>\u7dda\u5f62\u5909\u63db\u306f\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\u6642\u306b\u660e\u793a\u7684\u306b\u542b\u3081\u308b\u306e\u3067\u3001\u30d0\u30a4\u30a2\u30b9\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002\u305f\u3060\u3057\u3001<span translate=no>_^_0_^_</span>\u504f\u898b\u3092\u6301\u3064\u3053\u3068\u306f\u7406\u306b\u304b\u306a\u3063\u3066\u3044\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093.</p>\n",
 "Documented implementation with explanations of Relative Multi-Headed Attention from paper Transformer-XL.": "\u30da\u30fc\u30d1\u30fc\u30fb\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u306e\u76f8\u5bfe\u7684\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30fb\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u8aac\u660e\u3092\u542b\u3080\u5b9f\u88c5\u304c\u6587\u66f8\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002",
 "Relative Multi-Headed Attention": "\u76f8\u5bfe\u7684\u591a\u9762\u7684\u6ce8\u610f"
}