{
 "<h1>BERT Embeddings of chunks of text</h1>\n<p>This is the code to get BERT embeddings of chunks for <a href=\"index.html\">RETRO model</a>.</p>\n": "<h1>BERT \u30c6\u30ad\u30b9\u30c8\u306e\u584a\u306e\u57cb\u3081\u8fbc\u307f</h1>\n<p><a href=\"index.html\">\u3053\u308c\u306f\u3001RETRO\u30e2\u30c7\u30eb\u7528\u306e\u30c1\u30e3\u30f3\u30af\u306eBERT\u57cb\u3081\u8fbc\u307f\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002</a></p>\n",
 "<h2>BERT Embeddings</h2>\n<p>For a given chunk of text <span translate=no>_^_0_^_</span> this class generates BERT embeddings <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is the average of BERT embeddings of all the tokens in <span translate=no>_^_3_^_</span>.</p>\n": "<h2>BERT \u30a8\u30f3\u30d9\u30c7\u30a3\u30f3\u30b0</h2>\n<p><span translate=no>_^_0_^_</span>\u3053\u306e\u30af\u30e9\u30b9\u306f\u3001\u7279\u5b9a\u306e\u30c6\u30ad\u30b9\u30c8\u30c1\u30e3\u30f3\u30af\u306b\u5bfe\u3057\u3066 BERT <span translate=no>_^_1_^_</span> \u57cb\u3081\u8fbc\u307f\u3092\u751f\u6210\u3057\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u306f\u3001\u5185\u306e\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u306eBERT\u57cb\u3081\u8fbc\u307f\u306e\u5e73\u5747\u3067\u3059\u3002<span translate=no>_^_3_^_</span></p>\n",
 "<h3>Code to test BERT embeddings</h3>\n": "<h3>BERT \u57cb\u3081\u8fbc\u307f\u3092\u30c6\u30b9\u30c8\u3059\u308b\u30b3\u30fc\u30c9</h3>\n",
 "<h3>Get <span translate=no>_^_0_^_</span> for a list of chunks.</h3>\n": "<h3><span translate=no>_^_0_^_</span>\u30c1\u30e3\u30f3\u30af\u306e\u30ea\u30b9\u30c8\u3092\u5165\u624b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> In this implementation, we do not make chunks with a fixed number of tokens. One of the reasons is that this implementation uses character-level tokens and BERT uses its sub-word tokenizer.</p>\n<p>So this method will truncate the text to make sure there are no partial tokens.</p>\n<p>For instance, a chunk could be like <span translate=no>_^_0_^_</span>, with partial words (partial sub-word tokens) on the ends. We strip them off to get better BERT embeddings. As mentioned earlier this is not necessary if we broke chunks after tokenizing.</p>\n": "<p>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u3001\u56fa\u5b9a\u6570\u306e\u30c8\u30fc\u30af\u30f3\u3067\u30c1\u30e3\u30f3\u30af\u3092\u4f5c\u6210\u3057\u307e\u305b\u3093\u3002\u7406\u7531\u306e1\u3064\u306f\u3001\u3053\u306e\u5b9f\u88c5\u3067\u306f\u6587\u5b57\u30ec\u30d9\u30eb\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4f7f\u7528\u3057\u3001BERT\u306f\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3053\u3068\u3067\u3059</p>\u3002\n<p>\u305d\u306e\u305f\u3081\u3001\u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30c6\u30ad\u30b9\u30c8\u3092\u5207\u308a\u6368\u3066\u3066\u3001\u90e8\u5206\u7684\u306a\u30c8\u30fc\u30af\u30f3\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002</p>\n<p>\u305f\u3068\u3048\u3070\u3001\u30c1\u30e3\u30f3\u30af\u306f\u3001\u672b\u5c3e\u306b\u5358\u8a9e\u306e\u4e00\u90e8\uff08\u90e8\u5206\u7684\u306a\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\uff09<span translate=no>_^_0_^_</span>\u304c\u3042\u308b\u3088\u3046\u306a\u3082\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002BERT \u57cb\u3081\u8fbc\u307f\u306e\u7cbe\u5ea6\u3092\u9ad8\u3081\u308b\u305f\u3081\u3001\u3053\u308c\u3089\u3092\u524a\u9664\u3057\u307e\u3057\u305f\u3002\u5148\u306b\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u30c8\u30fc\u30af\u30f3\u5316\u5f8c\u306b\u30c1\u30e3\u30f3\u30af\u3092\u5206\u5272\u3057\u305f\u5834\u5408\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093</p>\u3002\n",
 "<p>Break words </p>\n": "<p>\u30d6\u30ec\u30fc\u30af\u30fb\u30ef\u30fc\u30c9</p>\n",
 "<p>Calculate the average token embeddings. Note that the attention mask is <span translate=no>_^_0_^_</span> if the token is empty padded. We get empty tokens because the chunks are of different lengths. </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u306e\u5e73\u5747\u56de\u6570\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u6ce8\u610f\u30de\u30b9\u30af\u306f\u3001<span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u304c\u7a7a\u3067\u30d1\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3067\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30c1\u30e3\u30f3\u30af\u306e\u9577\u3055\u304c\u7570\u306a\u308b\u305f\u3081\u3001\u7a7a\u306e\u30c8\u30fc\u30af\u30f3\u304c\u8fd4\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<p>Check BERT model outputs </p>\n": "<p>BERT \u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u30c1\u30a7\u30c3\u30af</p>\n",
 "<p>Check BERT tokenizer </p>\n": "<p>BERT \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u30c1\u30a7\u30c3\u30af</p>\n",
 "<p>Check recreating text from token ids </p>\n": "<p>\u30c8\u30fc\u30af\u30f3 ID \u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3092\u518d\u4f5c\u6210\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Evaluate the model </p>\n": "<p>\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1</p>\n",
 "<p>Get chunk embeddings </p>\n": "<p>\u30c1\u30e3\u30f3\u30af\u57cb\u3081\u8fbc\u307f\u3092\u53d6\u5f97</p>\n",
 "<p>Get the token embeddings </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u5165\u624b</p>\n",
 "<p>If empty return original string </p>\n": "<p>\u7a7a\u306e\u5834\u5408\u3001\u5143\u306e\u6587\u5b57\u5217\u3092\u8fd4\u3059</p>\n",
 "<p>Initialize </p>\n": "<p>[\u521d\u671f\u5316]</p>\n",
 "<p>Load the BERT model from <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFace</a> </p>\n": "<p><a href=\"https://huggingface.co/bert-base-uncased\">\u30cf\u30ae\u30f3\u30b0\u30d5\u30a7\u30a4\u30b9\u304b\u3089</a> BERT \u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9</p>\n",
 "<p>Load the BERT tokenizer from <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFace</a> </p>\n": "<p><a href=\"https://huggingface.co/bert-base-uncased\">HuggingFace \u304b\u3089 BERT \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3059</a></p>\n",
 "<p>Move the model to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30e2\u30c7\u30eb\u3092\u6b21\u306e\u5834\u6240\u306b\u79fb\u52d5 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Move token ids, attention mask and token types to the device </p>\n": "<p>\u30c8\u30fc\u30af\u30f3 ID\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u3001\u30c8\u30fc\u30af\u30f3\u30bf\u30a4\u30d7\u3092\u30c7\u30d0\u30a4\u30b9\u306b\u79fb\u52d5</p>\n",
 "<p>Otherwise, return the stripped string </p>\n": "<p>\u305d\u308c\u4ee5\u5916\u306e\u5834\u5408\u306f\u3001\u53d6\u308a\u9664\u3044\u305f\u6587\u5b57\u5217\u3092\u8fd4\u3059</p>\n",
 "<p>Remove first and last pieces </p>\n": "<p>\u6700\u521d\u3068\u6700\u5f8c\u306e\u30d4\u30fc\u30b9\u3092\u524a\u9664\u3059\u308b</p>\n",
 "<p>Remove whitespace </p>\n": "<p>\u7a7a\u767d\u3092\u524a\u9664</p>\n",
 "<p>Sample </p>\n": "<p>[\u30b5\u30f3\u30d7\u30eb]</p>\n",
 "<p>Strip whitespace </p>\n": "<p>\u7a7a\u767d\u3092\u524a\u9664</p>\n",
 "<p>Tokenize the chunks with BERT tokenizer </p>\n": "<p>BERT \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3067\u30c1\u30e3\u30f3\u30af\u3092\u30c8\u30fc\u30af\u30f3\u5316\u3059\u308b</p>\n",
 "<p>Trim the chunks </p>\n": "<p>\u30c1\u30e3\u30f3\u30af\u3092\u30c8\u30ea\u30df\u30f3\u30b0</p>\n",
 "<p>We don&#x27;t need to compute gradients </p>\n": "<p>\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093</p>\n",
 "BERT Embeddings of chunks of text": "BERT \u30c6\u30ad\u30b9\u30c8\u306e\u584a\u306e\u57cb\u3081\u8fbc\u307f",
 "Generate BERT embeddings for chunks using a frozen BERT model": "\u30d5\u30ea\u30fc\u30ba\u3057\u305f BERT \u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066\u30c1\u30e3\u30f3\u30af\u306e BERT \u57cb\u3081\u8fbc\u307f\u3092\u751f\u6210"
}