{
 "<h1>BERT Embeddings of chunks of text</h1>\n<p>This is the code to get BERT embeddings of chunks for <a href=\"index.html\">RETRO model</a>.</p>\n": "<h1>BERT \u6587\u672c\u5757\u7684\u5d4c\u5165</h1>\n<p>\u8fd9\u662f\u83b7\u53d6 <a href=\"index.html\">RETRO \u6a21\u578b</a>\u5757\u7684 BERT \u5d4c\u5165\u7684\u4ee3\u7801\u3002</p>\n",
 "<h2>BERT Embeddings</h2>\n<p>For a given chunk of text <span translate=no>_^_0_^_</span> this class generates BERT embeddings <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is the average of BERT embeddings of all the tokens in <span translate=no>_^_3_^_</span>.</p>\n": "<h2>BERT \u5d4c\u5165\u5f0f</h2>\n<p>\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u6587\u672c\u5757\uff0c<span translate=no>_^_0_^_</span>\u8fd9\u4e2a\u7c7b\u4f1a\u751f\u6210 BERT \u5d4c\u5165<span translate=no>_^_1_^_</span>\u3002<span translate=no>_^_2_^_</span>\u662f\u6240\u6709\u4ee4\u724c\u7684 BERT \u5d4c\u5165\u7684\u5e73\u5747\u503c<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<h3>Code to test BERT embeddings</h3>\n": "<h3>\u7528\u4e8e\u6d4b\u8bd5 BERT \u5d4c\u5165\u7684\u4ee3\u7801</h3>\n",
 "<h3>Get <span translate=no>_^_0_^_</span> for a list of chunks.</h3>\n": "<h3><span translate=no>_^_0_^_</span>\u83b7\u53d6\u533a\u5757\u5217\u8868\u3002</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> In this implementation, we do not make chunks with a fixed number of tokens. One of the reasons is that this implementation uses character-level tokens and BERT uses its sub-word tokenizer.</p>\n<p>So this method will truncate the text to make sure there are no partial tokens.</p>\n<p>For instance, a chunk could be like <span translate=no>_^_0_^_</span>, with partial words (partial sub-word tokens) on the ends. We strip them off to get better BERT embeddings. As mentioned earlier this is not necessary if we broke chunks after tokenizing.</p>\n": "<p>\u5728\u6b64\u5b9e\u73b0\u4e2d\uff0c\u6211\u4eec\u4e0d\u4f1a\u4f7f\u7528\u56fa\u5b9a\u6570\u91cf\u7684\u4ee4\u724c\u5236\u4f5c\u533a\u5757\u3002\u539f\u56e0\u4e4b\u4e00\u662f\u6b64\u5b9e\u73b0\u4f7f\u7528\u5b57\u7b26\u7ea7\u4ee4\u724c\uff0c\u800c BERT \u4f7f\u7528\u5176\u5b50\u8bcd\u5206\u8bcd\u5668\u3002</p>\n<p>\u56e0\u6b64\uff0c\u6b64\u65b9\u6cd5\u5c06\u622a\u65ad\u6587\u672c\u4ee5\u786e\u4fdd\u6ca1\u6709\u90e8\u5206\u6807\u8bb0\u3002</p>\n<p>\u4f8b\u5982\uff0c\u4e00\u4e2a\u5757\u53ef\u80fd\u50cf<span translate=no>_^_0_^_</span>\uff0c\u672b\u5c3e\u5e26\u6709\u90e8\u5206\u5355\u8bcd\uff08\u90e8\u5206\u5b50\u8bcd\u6807\u8bb0\uff09\u3002\u6211\u4eec\u5265\u79bb\u5b83\u4eec\u4ee5\u83b7\u5f97\u66f4\u597d\u7684 BERT \u5d4c\u5165\u3002\u5982\u524d\u6240\u8ff0\uff0c\u5982\u679c\u6211\u4eec\u5728\u6807\u8bb0\u5316\u540e\u7834\u574f\u4e86\u533a\u5757\uff0c\u5219\u6ca1\u6709\u5fc5\u8981\u8fd9\u6837\u505a\u3002</p>\n",
 "<p>Break words </p>\n": "<p>\u65ad\u8bcd</p>\n",
 "<p>Calculate the average token embeddings. Note that the attention mask is <span translate=no>_^_0_^_</span> if the token is empty padded. We get empty tokens because the chunks are of different lengths. </p>\n": "<p>\u8ba1\u7b97\u5e73\u5747\u4ee3\u5e01\u5d4c\u5165\u91cf\u3002\u8bf7\u6ce8\u610f\uff0c<span translate=no>_^_0_^_</span>\u5982\u679c\u4ee4\u724c\u662f\u7a7a\u586b\u5145\u7684\uff0c\u5219\u6ce8\u610f\u63a9\u7801\u4e3a\u3002\u6211\u4eec\u5f97\u5230\u7a7a\u4ee4\u724c\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5757\u7684\u957f\u5ea6\u4e0d\u540c\u3002</p>\n",
 "<p>Check BERT model outputs </p>\n": "<p>\u68c0\u67e5 BERT \u6a21\u578b\u8f93\u51fa</p>\n",
 "<p>Check BERT tokenizer </p>\n": "<p>\u67e5\u770b BERT \u5206\u8bcd\u5668</p>\n",
 "<p>Check recreating text from token ids </p>\n": "<p>\u68c0\u67e5\u4ece\u4ee4\u724c ID \u4e2d\u91cd\u65b0\u521b\u5efa\u6587\u672c</p>\n",
 "<p>Evaluate the model </p>\n": "<p>\u8bc4\u4f30\u6a21\u578b</p>\n",
 "<p>Get chunk embeddings </p>\n": "<p>\u83b7\u53d6\u533a\u5757\u5d4c\u5165</p>\n",
 "<p>Get the token embeddings </p>\n": "<p>\u83b7\u53d6\u4ee4\u724c\u5d4c\u5165</p>\n",
 "<p>If empty return original string </p>\n": "<p>\u5982\u679c\u4e3a\u7a7a\u5219\u8fd4\u56de\u539f\u59cb\u5b57\u7b26\u4e32</p>\n",
 "<p>Initialize </p>\n": "<p>\u521d\u59cb\u5316</p>\n",
 "<p>Load the BERT model from <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFace</a> </p>\n": "<p>\u4ece <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFac</a> e \u52a0\u8f7d BERT \u6a21\u578b</p>\n",
 "<p>Load the BERT tokenizer from <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFace</a> </p>\n": "<p>\u4ece <a href=\"https://huggingface.co/bert-base-uncased\">HuggingFac</a> e \u52a0\u8f7d BERT \u5206\u8bcd\u5668</p>\n",
 "<p>Move the model to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5c06\u6a21\u578b\u79fb\u5230<span translate=no>_^_0_^_</span></p>\n",
 "<p>Move token ids, attention mask and token types to the device </p>\n": "<p>\u5c06\u4ee4\u724c ID\u3001\u6ce8\u610f\u63a9\u7801\u548c\u4ee4\u724c\u7c7b\u578b\u79fb\u52a8\u5230\u8bbe\u5907</p>\n",
 "<p>Otherwise, return the stripped string </p>\n": "<p>\u5426\u5219\uff0c\u8fd4\u56de\u88ab\u5265\u79bb\u7684\u5b57\u7b26\u4e32</p>\n",
 "<p>Remove first and last pieces </p>\n": "<p>\u79fb\u9664\u7b2c\u4e00\u5757\u548c\u6700\u540e\u4e00\u5757\u788e\u7247</p>\n",
 "<p>Remove whitespace </p>\n": "<p>\u79fb\u9664\u7a7a\u683c</p>\n",
 "<p>Sample </p>\n": "<p>\u6837\u672c</p>\n",
 "<p>Strip whitespace </p>\n": "<p>\u53bb\u6389\u7a7a\u767d</p>\n",
 "<p>Tokenize the chunks with BERT tokenizer </p>\n": "<p>\u4f7f\u7528 BERT \u5206\u8bcd\u5668\u5bf9\u533a\u5757\u8fdb\u884c\u6807\u8bb0\u5316</p>\n",
 "<p>Trim the chunks </p>\n": "<p>\u4fee\u526a\u5757</p>\n",
 "<p>We don&#x27;t need to compute gradients </p>\n": "<p>\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6</p>\n",
 "BERT Embeddings of chunks of text": "BERT \u6587\u672c\u5757\u7684\u5d4c\u5165",
 "Generate BERT embeddings for chunks using a frozen BERT model": "\u4f7f\u7528\u51bb\u7ed3 BERT \u6a21\u578b\u4e3a\u533a\u5757\u751f\u6210 BERT \u5d4c\u5165"
}