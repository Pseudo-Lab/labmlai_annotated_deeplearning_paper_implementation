{
 "<h1>RETRO training dataset</h1>\n<p>We pre-retrieve nearest neighbors from the <a href=\"database.html\">key-value database</a>  and create the dataset to train the <a href=\"index.html\">RETRO</a>  <a href=\"model.html\">model</a>.</p>\n": "<h1>RETRO \u8bad\u7ec3\u6570\u636e\u96c6</h1>\n<p>\u6211\u4eec\u4ece<a href=\"database.html\">\u952e\u503c\u6570\u636e\u5e93</a>\u4e2d\u9884\u5148\u68c0\u7d22\u6700\u8fd1\u7684\u90bb\u57df\uff0c\u5e76\u521b\u5efa\u6570\u636e\u96c6\u6765\u8bad\u7ec3 <a href=\"index.html\">RETRO</a> <a href=\"model.html\">\u6a21\u578b</a>\u3002</p>\n",
 "<h2>Build the dataset</h2>\n<ul><li><span translate=no>_^_0_^_</span> is the chunk length </li>\n<li><span translate=no>_^_1_^_</span> is the number of chunks per training sample </li>\n<li><span translate=no>_^_2_^_</span> is the maximum number of characters to skip between two samples.  We skip a few characters between samples to make sure the samples  aren&#x27;t aligned perfectly with the chunks in the <a href=\"database.html\">database</a></li></ul>\n": "<h2>\u6784\u5efa\u6570\u636e\u96c6</h2>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u533a\u5757\u957f\u5ea6</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u5757\u6570</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u5728\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u8df3\u8fc7\u7684\u6700\u5927\u5b57\u7b26\u6570\u3002\u6211\u4eec\u5728\u6837\u672c\u4e4b\u95f4\u8df3\u8fc7\u51e0\u4e2a\u5b57\u7b26\uff0c\u4ee5\u786e\u4fdd\u6837\u672c\u4e0e<a href=\"database.html\">\u6570\u636e\u5e93</a>\u4e2d\u7684\u5757\u4e0d\u5b8c\u5168\u5bf9\u9f50</li></ul>\n",
 "<h2>Dataset</h2>\n<p>This is the PyTorch dataset that loads the dataset created by <span translate=no>_^_0_^_</span>.</p>\n": "<h2>\u6570\u636e\u96c6</h2>\n<p>\u8fd9\u662f PyTorch \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u52a0\u8f7d\u7531\u521b\u5efa\u7684\u6570\u636e\u96c6<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Get a sample</p>\n": "<p>\u83b7\u53d6\u6837\u54c1</p>\n",
 "<p> Number of samples</p>\n": "<p>\u6837\u672c\u6570\u91cf</p>\n",
 "<p>Add to list of samples </p>\n": "<p>\u6dfb\u52a0\u5230\u6837\u54c1\u6e05\u5355</p>\n",
 "<p>Break it into chunks </p>\n": "<p>\u628a\u5b83\u5206\u6210\u5927\u5757</p>\n",
 "<p>Collect the offset </p>\n": "<p>\u6536\u96c6\u504f\u79fb\u91cf</p>\n",
 "<p>Cursor for the text </p>\n": "<p>\u5149\u6807\u6307\u5411\u6587\u672c</p>\n",
 "<p>For samples </p>\n": "<p>\u5bf9\u4e8e\u6837\u54c1</p>\n",
 "<p>Get neighbor texts. The neighbor length is twice the <span translate=no>_^_0_^_</span> </p>\n": "<p>\u83b7\u53d6\u90bb\u5c45\u77ed\u4fe1\u3002\u90bb\u5c45\u957f\u5ea6\u662f\u4e24\u500d<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get the sample </p>\n": "<p>\u83b7\u53d6\u6837\u54c1</p>\n",
 "<p>Get the sample including an extra character (for prediction) </p>\n": "<p>\u83b7\u53d6\u5305\u542b\u989d\u5916\u5b57\u7b26\u7684\u6837\u672c\uff08\u7528\u4e8e\u9884\u6d4b\uff09</p>\n",
 "<p>Increment the cursor </p>\n": "<p>\u589e\u52a0\u5149\u6807</p>\n",
 "<p>Iterate through sample offsets </p>\n": "<p>\u904d\u5386\u6837\u672c\u504f\u79fb\u91cf</p>\n",
 "<p>Load the index for retrieving neighbors </p>\n": "<p>\u52a0\u8f7d\u7d22\u5f15\u4ee5\u68c0\u7d22\u90bb\u5c45</p>\n",
 "<p>Load the samples </p>\n": "<p>\u52a0\u8f7d\u6837\u54c1</p>\n",
 "<p>Load the text file </p>\n": "<p>\u52a0\u8f7d\u6587\u672c\u6587\u4ef6</p>\n",
 "<p>Retrieve nearest neighbors </p>\n": "<p>\u68c0\u7d22\u6700\u8fd1\u7684\u90bb\u5c45</p>\n",
 "<p>Save the samples in JSON. We don&#x27;t need to use complex dataset storage mechanisms or pre-tokenize since our dataset is small. </p>\n": "<p>\u4ee5 JSON \u683c\u5f0f\u4fdd\u5b58\u793a\u4f8b\u3002\u6211\u4eec\u4e0d\u9700\u8981\u4f7f\u7528\u590d\u6742\u7684\u6570\u636e\u96c6\u5b58\u50a8\u673a\u5236\u6216\u9884\u6807\u8bb0\u5316\uff0c\u56e0\u4e3a\u6211\u4eec\u7684\u6570\u636e\u96c6\u5f88\u5c0f\u3002</p>\n",
 "<p>Skip a few characters to make sure it&#x27;s not aligned with the neighbors </p>\n": "<p>\u8df3\u8fc7\u51e0\u4e2a\u89d2\u8272\u4ee5\u786e\u4fdd\u5b83\u4e0d\u4e0e\u90bb\u5c45\u5bf9\u9f50</p>\n",
 "<p>Stop if we&#x27;ve reached the end of the text </p>\n": "<p>\u5982\u679c\u6211\u4eec\u5230\u8fbe\u4e86\u6587\u5b57\u7684\u672b\u5c3e\uff0c\u5c31\u505c\u4e0b\u6765</p>\n",
 "<p>The chunk offsets </p>\n": "<p>\u533a\u5757\u504f\u79fb\u91cf</p>\n",
 "<p>The input </p>\n": "<p>\u8f93\u5165</p>\n",
 "<p>The input sample offsets </p>\n": "<p>\u8f93\u5165\u6837\u672c\u504f\u79fb</p>\n",
 "<p>Tokenize </p>\n": "<p>Tokenize</p>\n",
 "<p>Training portion of it </p>\n": "<p>\u5176\u4e2d\u7684\u4e00\u90e8\u5206\u8bad\u7ec3</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the path of the saved JSON file </li>\n<li><span translate=no>_^_1_^_</span> is the <span translate=no>_^_2_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4fdd\u5b58\u7684 JSON \u6587\u4ef6\u7684\u8def\u5f84</li>\n</ul><li><span translate=no>_^_1_^_</span>\u662f<span translate=no>_^_2_^_</span></li>\n",
 "Create a dataset for RETRO model training": "\u521b\u5efa\u7528\u4e8e RETRO \u6a21\u578b\u8bad\u7ec3\u7684\u6570\u636e\u96c6",
 "Training dataset for RETRO": "RETRO \u7684\u8bad\u7ec3\u6570\u636e\u96c6"
}