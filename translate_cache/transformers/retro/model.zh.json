{
 "<h1>RETRO model</h1>\n<p>This is the model definition for  <a href=\"index.html\">RETRO</a>.</p>\n": "<h1>\u590d\u53e4\u6a21\u578b</h1>\n<p>\u8fd9\u662f RETRO \u7684\u6a21\u578b<a href=\"index.html\">\u5b9a\u4e49</a>\u3002</p>\n",
 "<h2><a href=\"../rope/index.html\">RoPE embeddings</a></h2>\n<p><em>We use rotary position embeddings in self-attention layers. We assume the positional information gets embedded in embeddings and therefore not use them in causal attention. <a href=\"https://papers.labml.ai/paper/3999902edc8511eba3db37f65e372566\">Non-causal self-attention needs explicit positional information  because it cannot infer it</a>.</em></p>\n": "<h2><a href=\"../rope/index.html\">\u7ef3\u7d22\u5d4c\u5165</a></h2>\n<p><em>\u6211\u4eec\u5728\u81ea\u6211\u6ce8\u610f\u529b\u5c42\u4e2d\u4f7f\u7528\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u3002\u6211\u4eec\u5047\u8bbe\u4f4d\u7f6e\u4fe1\u606f\u88ab\u5d4c\u5165\u5230\u5d4c\u5165\u4e2d\uff0c\u56e0\u6b64\u4e0d\u4f1a\u5728\u56e0\u679c\u5173\u6ce8\u4e2d\u4f7f\u7528\u5b83\u4eec\u3002<a href=\"https://papers.labml.ai/paper/3999902edc8511eba3db37f65e372566\">\u975e\u56e0\u679c\u7684\u81ea\u6211\u6ce8\u610f\u529b\u9700\u8981\u660e\u786e\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u56e0\u4e3a\u5b83\u65e0\u6cd5\u63a8\u65ad\u51fa\u6765</a>\u3002</em></p>\n",
 "<h2>Chunked Cross-Attention Layer <span translate=no>_^_0_^_</span></h2>\n<p>This is similar to the cross-attention layer defined above.</p>\n<p>This is used in the decoder to pay attention to the retrieved neighbor chunks.</p>\n<p><em>We do not use any explicit positional embeddings here. We assume that the model can represent positional information in the embeddings implicitly.</em></p>\n": "<h2>\u5206\u5757\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42<span translate=no>_^_0_^_</span></h2>\n<p>\u8fd9\u4e0e\u4e0a\u9762\u5b9a\u4e49\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u7c7b\u4f3c\u3002</p>\n<p>\u8fd9\u5728\u89e3\u7801\u5668\u4e2d\u7528\u4e8e\u5173\u6ce8\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u5757\u3002</p>\n<p><em>\u6211\u4eec\u5728\u6b64\u5904\u4e0d\u4f7f\u7528\u4efb\u4f55\u663e\u5f0f\u7684\u4f4d\u7f6e\u5d4c\u5165\u3002\u6211\u4eec\u5047\u8bbe\u6a21\u578b\u53ef\u4ee5\u5728\u5d4c\u5165\u4e2d\u9690\u5f0f\u8868\u793a\u4f4d\u7f6e\u4fe1\u606f\u3002</em></p>\n",
 "<h2>Cross-Attention Layer <span translate=no>_^_0_^_</span></h2>\n<p>This is similar to the self-attention layer defined above, except that it gets keys and values from a different set of embeddings than the queries.</p>\n<p>This is used in the encoder to encode the retrieved chunks based on the input chunks.</p>\n<p><em>We do not use any explicit positional embeddings here. We assume that the model can represent positional information in the embeddings implicitly.</em></p>\n": "<h2>\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42<span translate=no>_^_0_^_</span></h2>\n<p>\u8fd9\u4e0e\u4e0a\u9762\u5b9a\u4e49\u7684\u81ea\u6211\u6ce8\u610f\u5c42\u7c7b\u4f3c\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u5b83\u4ece\u4e0e\u67e5\u8be2\u4e0d\u540c\u7684\u5d4c\u5165\u96c6\u83b7\u53d6\u952e\u548c\u503c\u3002</p>\n<p>\u8fd9\u5728\u7f16\u7801\u5668\u4e2d\u7528\u4e8e\u6839\u636e\u8f93\u5165\u533a\u5757\u5bf9\u68c0\u7d22\u5230\u7684\u533a\u5757\u8fdb\u884c\u7f16\u7801\u3002</p>\n<p><em>\u6211\u4eec\u5728\u6b64\u5904\u4e0d\u4f7f\u7528\u4efb\u4f55\u663e\u5f0f\u7684\u4f4d\u7f6e\u5d4c\u5165\u3002\u6211\u4eec\u5047\u8bbe\u6a21\u578b\u53ef\u4ee5\u5728\u5d4c\u5165\u4e2d\u9690\u5f0f\u8868\u793a\u4f4d\u7f6e\u4fe1\u606f\u3002</em></p>\n",
 "<h2>Nearest Neighbor Encoder <span translate=no>_^_0_^_</span></h2>\n<p>This module encodes the retrieved nearest neighbors</p>\n": "<h2>\u6700\u8fd1\u90bb\u7f16\u7801\u5668<span translate=no>_^_0_^_</span></h2>\n<p>\u6b64\u6a21\u5757\u5bf9\u68c0\u7d22\u5230\u7684\u6700\u8fd1\u90bb\u8fdb\u884c\u7f16\u7801</p>\n",
 "<h2>Retro Model</h2>\n<p>This is the Retro decoder</p>\n": "<h2>\u590d\u53e4\u6a21\u7279</h2>\n<p>\u8fd9\u662f\u590d\u53e4\u89e3\u7801\u5668</p>\n",
 "<h2>Self-Attention Layer <span translate=no>_^_0_^_</span></h2>\n<p>This applies causal and non-causal <a href=\"../mha.html\">multi-headed self-attention</a>.</p>\n": "<h2>\u81ea\u6211\u6ce8\u610f\u5c42<span translate=no>_^_0_^_</span></h2>\n<p>\u8fd9\u9002\u7528\u4e8e\u56e0\u679c\u548c\u975e\u56e0\u679c\u7684<a href=\"../mha.html\">\u591a\u5934\u81ea\u6211\u5173\u6ce8</a>\u3002</p>\n",
 "<h3>Mask the attention layer for causal attention</h3>\n<ul><li><span translate=no>_^_0_^_</span> is the attention matrix of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<h3>\u906e\u4f4f\u6ce8\u610f\u5c42\u4ee5\u83b7\u5f97\u56e0\u679c\u5173\u6ce8</h3>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u7684\u6ce8\u610f\u529b\u77e9\u9635<span translate=no>_^_1_^_</span></li></ul>\n",
 "<h3>Position-wise Feed Forward Layer <span translate=no>_^_0_^_</span></h3>\n<p>This consists of two linear layers and an activation in the middle.</p>\n": "<h3>\u4f4d\u7f6e\u524d\u9988\u5c42<span translate=no>_^_0_^_</span></h3>\n<p>\u5b83\u7531\u4e24\u4e2a\u7ebf\u6027\u5c42\u548c\u4e2d\u95f4\u7684\u6fc0\u6d3b\u5c42\u7ec4\u6210\u3002</p>\n",
 "<h3>Test the model with fake data</h3>\n": "<h3>\u4f7f\u7528\u865a\u5047\u6570\u636e\u6d4b\u8bd5\u6a21\u578b</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> are the embeddings of shape <span translate=no>_^_1_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u7684\u5d4c\u5165<span translate=no>_^_1_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> are the input embeddings of shape <span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span> are the retrieved nearest neighbors of shape <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>shape \u7684\u8f93\u5165\u5d4c\u5165<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u662f\u68c0\u7d22\u5230\u7684 shape \u7684\u6700\u8fd1\u90bb\u503c<span translate=no>_^_3_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span></p>\n<p>We passed the embeddings of <span translate=no>_^_1_^_</span> to encoder. </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n<p>\u6211\u4eec\u5c06\u7684\u5d4c\u5165\u4f20\u9012<span translate=no>_^_1_^_</span>\u7ed9\u7f16\u7801\u5668\u3002</p>\n",
 "<p>Activation </p>\n": "<p>\u6fc0\u6d3b</p>\n",
 "<p>Add residual connection </p>\n": "<p>\u6dfb\u52a0\u5269\u4f59\u8fde\u63a5</p>\n",
 "<p>Add the residual connection </p>\n": "<p>\u6dfb\u52a0\u5269\u4f59\u8fde\u63a5</p>\n",
 "<p>Append <span translate=no>_^_0_^_</span> zero embedding to the left; i.e. right shift it back </p>\n": "<p>\u5411\u5de6\u8ffd\u52a0<span translate=no>_^_0_^_</span>\u96f6\u5d4c\u5165\uff1b\u5373\u53f3\u79fb\u56de\u53bb</p>\n",
 "<p>Append empty embeddings to the end to be able to split the input into chunks </p>\n": "<p>\u5728\u672b\u5c3e\u8ffd\u52a0\u7a7a\u5d4c\u5165\uff0c\u4ee5\u4fbf\u80fd\u591f\u5c06\u8f93\u5165\u62c6\u5206\u4e3a\u5757</p>\n",
 "<p>Apply final linear layer. The result will have shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5e94\u7528\u6700\u540e\u7684\u7ebf\u6027\u56fe\u5c42\u3002\u7ed3\u679c\u5c06\u6709\u5f62\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Apply masks if it&#x27;s causal attention </p>\n": "<p>\u5982\u679c\u662f\u56e0\u679c\u5173\u7cfb\uff0c\u8bf7\u6234\u53e3\u7f69</p>\n",
 "<p>Apply rotary positional embeddings </p>\n": "<p>\u5e94\u7528\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165</p>\n",
 "<p>Apply softmax over the last two dimensions <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5728\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u5e94\u7528 softmax<span translate=no>_^_0_^_</span></p>\n",
 "<p>Attention layers <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6ce8\u610f\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Bi-directional self attention <span translate=no>_^_0_^_</span> </p>\n": "<p>\u53cc\u5411\u81ea\u6211\u5173\u6ce8<span translate=no>_^_0_^_</span></p>\n",
 "<p>Bi-directional self attention layers </p>\n": "<p>\u53cc\u5411\u81ea\u6211\u5173\u6ce8\u5c42</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate attention probabilities </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b\u6982\u7387</p>\n",
 "<p>Calculate attention scores for all chunks. Each retrieved neighbor will pay attention to the original chunk that retrieved it. This will have shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u6240\u6709\u533a\u5757\u7684\u6ce8\u610f\u529b\u5206\u6570\u3002\u6bcf\u4e2a\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u90fd\u5c06\u6ce8\u610f\u68c0\u7d22\u5230\u5b83\u7684\u539f\u59cb\u533a\u5757\u3002\u8fd9\u5c06\u6709\u5f62\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate attention scores for input chunks. Each chunk will pay attention to neighbors retrieved by the previous chunk. This will have shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u8f93\u5165\u533a\u5757\u7684\u6ce8\u610f\u529b\u5206\u6570\u3002\u6bcf\u4e2a\u533a\u5757\u90fd\u5c06\u5173\u6ce8\u524d\u4e00\u4e2a\u533a\u5757\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u3002\u8fd9\u5c06\u6709\u5f62\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate attentions </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b</p>\n",
 "<p>Calculate softmax across the last dimension </p>\n": "<p>\u8ba1\u7b97\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u7684 softmax</p>\n",
 "<p>Calculate the product of position index and <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u6301\u4ed3\u6307\u6570\u7684\u4e58\u79ef\u548c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate</p>\n<span translate=no>_^_0_^_</span><p>for <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8ba1\u7b97</p>\n<span translate=no>_^_0_^_</span><p>\u5bf9\u4e8e<span translate=no>_^_1_^_</span></p>\n",
 "<p>Causal self attention <span translate=no>_^_0_^_</span> </p>\n": "<p>\u56e0\u679c\u81ea\u6211\u5173\u6ce8<span translate=no>_^_0_^_</span></p>\n",
 "<p>Change from shape <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p>\u4ece\u5f62\u72b6\u6539<span translate=no>_^_0_^_</span>\u4e3a<span translate=no>_^_1_^_</span></p>\n",
 "<p>Chunked cross attention layers <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5206\u5757\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Chunked-cross attention if <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5927\u5757\u4ea4\u53c9\u6ce8\u610f\u5982\u679c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Concatenate so that for row <span translate=no>_^_0_^_</span> we have <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8fde\u63a5\u8fd9\u6837<span translate=no>_^_0_^_</span>\u6211\u4eec\u5c31\u6709 row<span translate=no>_^_1_^_</span></p>\n",
 "<p>Create a triangular mask </p>\n": "<p>\u521b\u5efa\u4e09\u89d2\u5f62\u8499\u7248</p>\n",
 "<p>Create position indexes <span translate=no>_^_0_^_</span> </p>\n": "<p>\u521b\u5efa\u5934\u5bf8\u6307\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Cross attention if <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4ea4\u53c9\u6ce8\u610f\u5982\u679c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Cross-attention layers </p>\n": "<p>\u4ea4\u53c9\u6ce8\u610f\u5c42</p>\n",
 "<p>Embeddings of the retrieved neighbors <span translate=no>_^_0_^_</span>.</p>\n<p>We use same embeddings for both input and neighbors </p>\n": "<p>\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u7684\u5d4c\u5165<span translate=no>_^_0_^_</span>\u3002</p>\n<p>\u6211\u4eec\u5bf9\u8f93\u5165\u548c\u90bb\u5c45\u4f7f\u7528\u76f8\u540c\u7684\u5d4c\u5165</p>\n",
 "<p>Extract the shape </p>\n": "<p>\u63d0\u53d6\u5f62\u72b6</p>\n",
 "<p>Feed forward layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u524d\u9988\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Feed forward layers </p>\n": "<p>\u524d\u9988\u56fe\u5c42</p>\n",
 "<p>Feed forward layers <span translate=no>_^_0_^_</span> </p>\n": "<p>\u524d\u9988\u56fe\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Filter by the mask </p>\n": "<p>\u6309\u53e3\u7f69\u8fc7\u6ee4</p>\n",
 "<p>Final linear layer </p>\n": "<p>\u6700\u540e\u7684\u7ebf\u6027\u5c42</p>\n",
 "<p>First linear layer </p>\n": "<p>\u7b2c\u4e00\u4e2a\u7ebf\u6027\u5c42</p>\n",
 "<p>For all layers <span translate=no>_^_0_^_</span> </p>\n": "<p>\u9002\u7528\u4e8e\u6240\u6709\u56fe\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Gather values </p>\n": "<p>\u6536\u96c6\u503c</p>\n",
 "<p>Get encoder embeddings before the first <span translate=no>_^_0_^_</span> layer, when <span translate=no>_^_1_^_</span> </p>\n": "<p>\u5728\u7b2c\u4e00<span translate=no>_^_0_^_</span>\u5c42\u4e4b\u524d\u83b7\u53d6\u7f16\u7801\u5668\u5d4c\u5165<span translate=no>_^_1_^_</span></p>\n",
 "<p>Get input embeddings <span translate=no>_^_0_^_</span> </p>\n": "<p>\u83b7\u53d6\u8f93\u5165\u5d4c\u5165<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get keys and values from the input chunks </p>\n": "<p>\u4ece\u8f93\u5165\u5757\u4e2d\u83b7\u53d6\u952e\u548c\u503c</p>\n",
 "<p>Get keys and values from the retrieved neighbors </p>\n": "<p>\u4ece\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u83b7\u53d6\u952e\u548c\u503c</p>\n",
 "<p>Get query from the input </p>\n": "<p>\u4ece\u8f93\u5165\u4e2d\u83b7\u53d6\u67e5\u8be2</p>\n",
 "<p>Get query from the retrieved chunks </p>\n": "<p>\u4ece\u68c0\u7d22\u5230\u7684\u533a\u5757\u4e2d\u83b7\u53d6\u67e5\u8be2</p>\n",
 "<p>Get query, key, and values and split them in to heads. These will have shapes <span translate=no>_^_0_^_</span> </p>\n": "<p>\u83b7\u53d6\u67e5\u8be2\u3001\u952e\u548c\u503c\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u6210\u5934\u90e8\u3002\u8fd9\u4e9b\u4f1a\u6709\u5f62\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get shape </p>\n": "<p>\u5851\u9020\u8eab\u6750</p>\n",
 "<p>Get values </p>\n": "<p>\u83b7\u53d6\u503c</p>\n",
 "<p>Increment chunked cross-attention index </p>\n": "<p>\u9012\u589e\u5206\u5757\u4ea4\u53c9\u6ce8\u610f\u529b\u6307\u6570</p>\n",
 "<p>Incremnt the cross attention index </p>\n": "<p>\u589e\u52a0\u4ea4\u53c9\u6ce8\u610f\u529b\u6307\u6570</p>\n",
 "<p>Keep index of the chunked cross attention layer </p>\n": "<p>\u4fdd\u7559\u5206\u5757\u4ea4\u53c9\u6ce8\u610f\u5c42\u7684\u7d22\u5f15</p>\n",
 "<p>Keep the index of the cross attention layer </p>\n": "<p>\u4fdd\u7559\u4ea4\u53c9\u5173\u6ce8\u5c42\u7684\u7d22\u5f15</p>\n",
 "<p>Linear layers for query, key and value heads. </p>\n": "<p>\u7528\u4e8e\u67e5\u8be2\u3001\u952e\u548c\u503c\u6807\u5934\u7684\u7ebf\u6027\u56fe\u5c42\u3002</p>\n",
 "<p>No attention if there are no chunks (for short inputs when sampling) </p>\n": "<p>\u5982\u679c\u6ca1\u6709\u533a\u5757\uff0c\u5219\u4e0d\u6ce8\u610f\uff08\u91c7\u6837\u65f6\u7528\u4e8e\u77ed\u8f93\u5165\uff09</p>\n",
 "<p>No masking for non-causal attention </p>\n": "<p>\u975e\u56e0\u679c\u6ce8\u610f\u6ca1\u6709\u906e\u7f69</p>\n",
 "<p>Normalize encoder embeddings </p>\n": "<p>\u89c4\u8303\u5316\u7f16\u7801\u5668\u5d4c\u5165</p>\n",
 "<p>Normalize retrieved chunks </p>\n": "<p>\u89c4\u8303\u5316\u68c0\u7d22\u5230\u7684\u533a\u5757</p>\n",
 "<p>Pre-norm </p>\n": "<p>\u89c4\u8303\u524d</p>\n",
 "<p>Pre-norm layer </p>\n": "<p>\u89c4\u8303\u524d\u5c42</p>\n",
 "<p>Pre-norm layer for the query embeddings. The paper uses RMSNorm instead. </p>\n": "<p>\u67e5\u8be2\u5d4c\u5165\u7684\u9884\u89c4\u8303\u5c42\u3002\u672c\u6587\u6539\u4e3a\u4f7f\u7528 rmsNorm\u3002</p>\n",
 "<p>Pre-norm layer. The paper uses RMSNorm instead. </p>\n": "<p>\u9884\u5148\u89c4\u8303\u5c42\u3002\u672c\u6587\u6539\u4e3a\u4f7f\u7528 rmsNorm\u3002</p>\n",
 "<p>Pre-normalization </p>\n": "<p>\u89c4\u8303\u5316\u524d</p>\n",
 "<p>Pre-normalization layer for <span translate=no>_^_0_^_</span> </p>\n": "<p>\u9884\u5f52\u4e00\u5316\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Pre-normalization layer for nearest neighbor embeddings from <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u8fd1\u90bb\u5d4c\u5165\u7684\u9884\u5f52\u4e00\u5316\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>ReLU Activation </p>\n": "<p>ReLU \u6fc0\u6d3b</p>\n",
 "<p>Readout layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8bfb\u51fa\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Remove the first <span translate=no>_^_0_^_</span> embeddings. The input pays attention to neighbors retrieved and encoded using the past tokens only; so that there is no information leakage. That is the retrieved neighbors from the first chunks will have information from the first chunk. So by shifting the sequence to the left by <span translate=no>_^_1_^_</span> we make sure that information only flows to the right. </p>\n": "<p>\u79fb\u9664\u7b2c\u4e00\u4e2a<span translate=no>_^_0_^_</span>\u5d4c\u5165\u3002\u8f93\u5165\u53ea\u5173\u6ce8\u4f7f\u7528\u8fc7\u53bb\u7684\u4ee4\u724c\u68c0\u7d22\u548c\u7f16\u7801\u7684\u90bb\u5c45\uff1b\u8fd9\u6837\u5c31\u4e0d\u4f1a\u6cc4\u9732\u4fe1\u606f\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u4ece\u7b2c\u4e00\u4e2a\u533a\u5757\u4e2d\u68c0\u7d22\u5230\u7684\u90bb\u5c45\u5c06\u83b7\u5f97\u6765\u81ea\u7b2c\u4e00\u4e2a\u533a\u5757\u7684\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u901a\u8fc7\u5c06\u5e8f\u5217\u5411\u5de6\u79fb\u52a8\uff0c<span translate=no>_^_1_^_</span>\u6211\u4eec\u53ef\u4ee5\u786e\u4fdd\u4fe1\u606f\u53ea\u5411\u53f3\u6d41\u52a8\u3002</p>\n",
 "<p>Reshape the input into chunks. </p>\n": "<p>\u5c06\u8f93\u5165\u91cd\u5851\u4e3a\u5757\u3002</p>\n",
 "<p>Residual </p>\n": "<p>\u5269\u4f59</p>\n",
 "<p>Residual connection </p>\n": "<p>\u5269\u4f59\u8fde\u63a5</p>\n",
 "<p>Rotary positional embeddings </p>\n": "<p>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165</p>\n",
 "<p>Scale attention scores </p>\n": "<p>\u7f29\u653e\u6ce8\u610f\u529b\u5206\u6570</p>\n",
 "<p>Scale it by <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6309\u6bd4\u4f8b\u7f29\u653e<span translate=no>_^_0_^_</span></p>\n",
 "<p>Second linear layer </p>\n": "<p>\u7b2c\u4e8c\u4e2a\u7ebf\u6027\u5c42</p>\n",
 "<p>Softmax for attention probabilities </p>\n": "<p>Softmax \u8868\u793a\u6ce8\u610f\u529b\u6982\u7387</p>\n",
 "<p>The two linear layers </p>\n": "<p>\u4e24\u4e2a\u7ebf\u6027\u5c42</p>\n",
 "<p>To scale attentions before softmax by <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5728 softmax \u4e4b\u524d\u6269\u5927\u6ce8\u610f\u529b<span translate=no>_^_0_^_</span></p>\n",
 "<p>Token embedding layer </p>\n": "<p>\u4ee4\u724c\u5d4c\u5165\u5c42</p>\n",
 "<p>Truncate and add the residual connection </p>\n": "<p>\u622a\u65ad\u5e76\u6dfb\u52a0\u5269\u4f59\u8fde\u63a5</p>\n",
 "<p>return <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8fd4\u56de<span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> are the retrieved nearest neighbor chunk embeddings with shape  <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> are the input chunks from which the nearest neighbors were retrieved with shape  <span translate=no>_^_3_^_</span>. This is already normalized.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5e26\u6709 shape \u7684\u68c0\u7d22\u7684\u6700\u8fd1\u90bb\u533a\u5757\u5d4c\u5165<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u4f7f\u7528 shape \u4ece\u4e2d\u68c0\u7d22\u6700\u8fd1\u90bb\u57df\u7684\u8f93\u5165\u5757<span translate=no>_^_3_^_</span>\u3002\u8fd9\u5df2\u7ecf\u89c4\u8303\u5316\u4e86\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> are token embeddings of the retrieved nearest neighbors,  <span translate=no>_^_1_^_</span>  of shape <span translate=no>_^_2_^_</span></li></ul>\n<ul><li><span translate=no>_^_3_^_</span> is are the input token embeddings, <span translate=no>_^_4_^_</span>  of shape <span translate=no>_^_5_^_</span></li></ul>\n<p><em>The chunks <span translate=no>_^_6_^_</span> and neighbors <span translate=no>_^_7_^_</span> are processed in parallel.</em></p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u68c0\u7d22\u5230\u7684\u6700\u8fd1\u90bb\u7684\u4ee4\u724c\u5d4c\u5165\uff0c\u5f62<span translate=no>_^_1_^_</span>\u72b6\u4e3a<span translate=no>_^_2_^_</span></li></ul>\n<ul><li><span translate=no>_^_3_^_</span>is \u662f\u5f62\u72b6\u7684\u8f93\u5165\u4ee4\u724c\u5d4c\u5165<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span></li></ul>\n<p><em>\u533a\u5757<span translate=no>_^_6_^_</span>\u548c\u90bb\u5c45<span translate=no>_^_7_^_</span>\u662f\u5e76\u884c\u5904\u7406\u7684\u3002</em></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the Tensor at the head of a key or a query with shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4f4d\u4e8e\u952e\u6216\u5e26\u6709\u5f62\u72b6\u7684\u67e5\u8be2\u5f00\u5934\u7684 Tensor<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input sequence, <span translate=no>_^_1_^_</span> of shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> are the retrieved neighbors  <span translate=no>_^_4_^_</span>  of shape <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6<span translate=no>_^_1_^_</span>\u7684\u8f93\u5165\u5e8f\u5217<span translate=no>_^_2_^_</span></li>\n</ul><li><span translate=no>_^_3_^_</span>\u662f\u68c0\u7d22\u5230<span translate=no>_^_4_^_</span>\u7684\u5f62\u72b6\u90bb\u57df<span translate=no>_^_5_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the length of a chunk </li>\n<li><span translate=no>_^_1_^_</span> is the number of layers in the encoder <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> are the layers with cross attention <span translate=no>_^_4_^_</span> </li>\n<li><span translate=no>_^_5_^_</span> is the number of features in embeddings </li>\n<li><span translate=no>_^_6_^_</span> is the number of heads in attention layers </li>\n<li><span translate=no>_^_7_^_</span> is the size of attention heads </li>\n<li><span translate=no>_^_8_^_</span> is the size of the feed-forward networks hidden layers</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u533a\u5757\u7684\u957f\u5ea6</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u7f16\u7801\u5668\u4e2d\u7684\u5c42\u6570<span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span>\u662f\u4ea4\u53c9\u5173\u6ce8\u7684\u5c42\u6b21\u5417<span translate=no>_^_4_^_</span></li>\n<li><span translate=no>_^_5_^_</span>\u662f\u5d4c\u5165\u4e2d\u8981\u7d20\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_6_^_</span>\u662f\u6ce8\u610f\u5c42\u4e2d\u7684\u5934\u90e8\u6570\u91cf</li>\n<li><span translate=no>_^_7_^_</span>\u662f\u6ce8\u610f\u5934\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_8_^_</span>\u662f\u524d\u9988\u7f51\u7edc\u9690\u85cf\u5c42\u7684\u5927\u5c0f</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the constant used for calculating <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8981\u7d20\u7684\u6570\u91cf<span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u662f\u7528\u4e8e\u8ba1\u7b97\u7684\u5e38\u6570<span translate=no>_^_3_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in transformer embeddings </li>\n<li><span translate=no>_^_1_^_</span> is the number features in the hidden layer</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u9690\u85cf\u56fe\u5c42\u4e2d\u7684\u6570\u5b57\u8981\u7d20</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in transformer embeddings </li>\n<li><span translate=no>_^_1_^_</span> is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span> is the number of features per head </li>\n<li><span translate=no>_^_3_^_</span> indicates whether this is causal attention (masked)</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6bcf\u5934\u7279\u5f81\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_3_^_</span>\u8868\u793a\u8fd9\u662f\u5426\u662f\u56e0\u679c\u5173\u6ce8\uff08\u5c4f\u853d\uff09</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in transformer embeddings </li>\n<li><span translate=no>_^_1_^_</span> is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span> is the number of features per head </li>\n<li><span translate=no>_^_3_^_</span> is the length of a chunk</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6bcf\u5934\u7279\u5f81\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u533a\u5757\u7684\u957f\u5ea6</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features in transformer embeddings </li>\n<li><span translate=no>_^_1_^_</span> is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span> is the number of features per head</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u53d8\u538b\u5668\u5d4c\u5165\u4e2d\u7684\u7279\u5f81\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u6ce8\u610f\u5934\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6bcf\u5934\u7279\u5f81\u7684\u6570\u91cf</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of tokens in the vocabulary </li>\n<li><span translate=no>_^_1_^_</span> is the number of features in embeddings </li>\n<li><span translate=no>_^_2_^_</span> is the number of layers in the decoder <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> are the layers with cross attention <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is the length of a chunk </li>\n<li><span translate=no>_^_7_^_</span> is the number of heads in attention layers </li>\n<li><span translate=no>_^_8_^_</span> is the size of attention heads </li>\n<li><span translate=no>_^_9_^_</span> is the size of the feed-forward networks hidden layers </li>\n<li><span translate=no>_^_10_^_</span> is the nearest neighbor encoder</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8bcd\u6c47\u8868\u4e2d\u4ee3\u5e01\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u5d4c\u5165\u4e2d\u8981\u7d20\u7684\u6570\u91cf</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u89e3\u7801\u5668\u4e2d\u7684\u5c42\u6570<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u662f\u4ea4\u53c9\u5173\u6ce8\u7684\u5c42\u6b21\u5417<span translate=no>_^_5_^_</span></li>\n<li><span translate=no>_^_6_^_</span>\u662f\u533a\u5757\u7684\u957f\u5ea6</li>\n<li><span translate=no>_^_7_^_</span>\u662f\u6ce8\u610f\u5c42\u4e2d\u7684\u5934\u90e8\u6570\u91cf</li>\n<li><span translate=no>_^_8_^_</span>\u662f\u6ce8\u610f\u5934\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_9_^_</span>\u662f\u524d\u9988\u7f51\u7edc\u9690\u85cf\u5c42\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_10_^_</span>\u662f\u6700\u8fd1\u90bb\u7f16\u7801\u5668</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer embeddings of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u53d8\u538b\u5668\u5d4c\u5165\u7684\u5f62\u72b6\u662f\u591a\u5c11<span translate=no>_^_1_^_</span></li></ul>\n",
 "RETRO model": "\u590d\u53e4\u578b\u53f7",
 "RETRO model with encoder for neighbors and autoregressive decoder": "\u5e26\u6709\u90bb\u5c45\u7f16\u7801\u5668\u548c\u81ea\u56de\u5f52\u89e3\u7801\u5668\u7684RETRO\u6a21\u578b"
}