{
 "<h1>GPT</h1>\n<p>This is a tutorial/implementation of <a href=\"https://openai.com/blog/better-language-models/\">OpenAI GPT architecture</a> in <a href=\"https://pytorch.org\">PyTorch</a>. We got a bunch of implementation details from <a href=\"https://github.com/karpathy/minGPT\">minGPT</a> by <a href=\"https://twitter.com/karpathy\">@karpathy</a>. This implementation also uses character tiny shakespeare dataset.</p>\n<p>GPT model is essentially a standard transformer with a few tweaks. GPT-2 and especially GPT-3 models are quite large and won&#x27;t fit on a single GPU and will need model parallelism. This implementation doesn&#x27;t even use data parallelism and is intended to be more of a tutorial.</p>\n<p>Main differences of this compared to a simple autoregressive transformer are the parameter initialization, weight decay, and learning rate schedule. For the transformer we reuse the <a href=\"../transformers/index.html\">existing labml/nn transformer implementation</a>.</p>\n<p>Here&#x27;s a notebook for training a GPT model on Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/gpt/experiment.ipynb\"><span translate=no>_^_0_^_</span></a> <a href=\"https://app.labml.ai/run/0324c6d0562111eba65d0242ac1c0002\"><span translate=no>_^_1_^_</span></a></p>\n": "<h1>\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3</h1>\n</a> <p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0dc4\u0dd2 <a href=\"https://openai.com/blog/better-language-models/\">OpenAI GPT \u0d9c\u0dd8\u0dc4 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab \u0dc1\u0dd2\u0dbd\u0dca\u0db4\u0dba \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba\u0d9a\u0dca/\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2. <a href=\"https://twitter.com/karpathy\">@karpathy</a> \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dca <a href=\"https://github.com/karpathy/minGPT\">MingPT</a> \u0dc0\u0dd9\u0dad\u0dd2\u0db1\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc0\u0dd2\u0dc3\u0dca\u0dad\u0dbb \u0dbb\u0dcf\u0dc1\u0dd2\u0dba\u0d9a\u0dca \u0d85\u0db4\u0da7 \u0dbd\u0dd0\u0db6\u0dd4\u0dab\u0dd2. \u0db8\u0dd9\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d85\u0d9a\u0dca\u0dc2\u0dbb \u0d9a\u0dd4\u0da9\u0dcf \u0dc2\u0dda\u0d9a\u0dca\u0dc3\u0dca\u0db4\u0dd2\u0dba\u0dbb\u0dca \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba \u0daf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>GPT\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0d85\u0dad\u0dca\u0dba\u0dc0\u0dc1\u0dca\u0dba\u0dba\u0dd9\u0db1\u0dca\u0db8 tweaks \u0d9a\u0dd2\u0dc4\u0dd2\u0db4\u0dba\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0dc3\u0db8\u0dca\u0db8\u0dad \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba\u0d9a\u0dca \u0dc0\u0dda. GPT-2 \u0dc3\u0dc4 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dba\u0dd9\u0db1\u0dca GPT-3 \u0db8\u0dcf\u0daf\u0dd2\u0dbd\u0dd2 \u0dad\u0dbb\u0db8\u0d9a\u0dca \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0dad\u0db1\u0dd2 GPU \u0d91\u0d9a\u0d9a\u0da7 \u0db1\u0ddc\u0d9c\u0dd0\u0dbd\u0db4\u0dd9\u0db1 \u0d85\u0dad\u0dbb \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0dc3\u0db8\u0dcf\u0db1\u0dca\u0dad\u0dbb\u0d9a\u0dbb\u0dab\u0dba \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0dda. \u0db8\u0dd9\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0daf\u0dad\u0dca\u0dad \u0dc3\u0db8\u0dcf\u0db1\u0dca\u0dad\u0dbb\u0d9a\u0dbb\u0dab\u0dba \u0db4\u0dc0\u0dcf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0db1\u0ddc\u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba\u0d9a\u0dca \u0dc0\u0dd0\u0da9\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d85\u0daf\u0dc4\u0dc3\u0dca \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\u0dc3\u0dbb\u0dbd\u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0d9c\u0dcf\u0db8\u0dd3 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba\u0d9a\u0da7 \u0dc3\u0dcf\u0db4\u0dda\u0d9a\u0dca\u0dc2\u0dc0 \u0db8\u0dd9\u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 \u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0d9a\u0db8\u0dca \u0dc0\u0db1\u0dca\u0db1\u0dda \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d86\u0dbb\u0db8\u0dca\u0db7\u0dba, \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0dc4 \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad \u0d9a\u0dcf\u0dbd\u0dc3\u0da7\u0dc4\u0db1\u0dba\u0dba\u0dd2. \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db4\u0dd2 <a href=\"../transformers/index.html\">\u0daf\u0dd0\u0db1\u0da7 \u0db4\u0dc0\u0dad\u0dd2\u0db1 labml/nn \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8</a>\u0db1\u0dd0\u0dc0\u0dad \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4. </p>\n<p>\u0d9a\u0dd4\u0da9\u0dcf\u0dc2\u0dda\u0d9a\u0dca\u0dc3\u0dca\u0db4\u0dd2\u0dba\u0dbb\u0dca \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3 \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0da7\u0dc4\u0db1\u0dca \u0db4\u0ddc\u0dad\u0d9a\u0dca \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/gpt/experiment.ipynb\"><span translate=no>_^_0_^_</span></a> <a href=\"https://app.labml.ai/run/0324c6d0562111eba65d0242ac1c0002\"> <span translate=no>_^_1_^_</span></a></p>\n",
 "<h2>Configurations</h2>\n<p>This inherits from <a href=\"../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs\"><span translate=no>_^_0_^_</span></a></p>\n": "<h2>\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca</h2>\n<p>\u0db8\u0dd9\u0dba\u0d8b\u0dbb\u0dd4\u0db8 \u0dc0\u0db1\u0dca\u0db1\u0dda <a href=\"../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>GPT model</h2>\n<p>This consists of a token embedding layer, transformer encoder, and a final linear layer that gives token logits.</p>\n": "<h2>GPT\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba</h2>\n<p>\u0db8\u0dd9\u0dba\u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca, \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d91\u0db1\u0dca\u0d9a\u0ddd\u0da9\u0dbb\u0dba\u0d9a\u0dca \u0dc3\u0dc4 \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dd2\u0db1\u0dca \u0dc3\u0db8\u0db1\u0dca\u0dc0\u0dd2\u0dad \u0dc0\u0dda. </p>\n",
 "<h3>Create custom optimizer with weight decay</h3>\n<p>This code is taken from <a href=\"https://github.com/karpathy/minGPT\">minGPT</a>. This applies weight decay only to weights of linear layers.</p>\n": "<h3>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8\u0dad\u0dca \u0dc3\u0db8\u0d9f \u0d85\u0db7\u0dd2\u0dbb\u0dd4\u0da0\u0dd2 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1</h3>\n<p>\u0db8\u0dd9\u0db8\u0d9a\u0dda\u0dad\u0dba <a href=\"https://github.com/karpathy/minGPT\">MingPT</a>\u0dc0\u0dd9\u0dad\u0dd2\u0db1\u0dca \u0d9c\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda. \u0db8\u0dd9\u0dba \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dc0\u0dbd \u0db6\u0dbb\u0da7 \u0db4\u0db8\u0dab\u0d9a\u0dca \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0d85\u0daf\u0dcf\u0dc5 \u0dc0\u0dda. </p>\n",
 "<h3>Initialize weights</h3>\n<p>Weights of linear layers and embedding layers are initialized to <span translate=no>_^_0_^_</span> instead of the default Xavier initialzation.</p>\n": "<h3>\u0db6\u0dbb\u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n<p>\u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2\u0dc3\u0dda\u0dc0\u0dd2\u0dba\u0dbb\u0dca \u0d86\u0dbb\u0db8\u0dca\u0db7\u0dba <span translate=no>_^_0_^_</span> \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dc4 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0dc0\u0dbd \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb \u0d87\u0dad. </p>\n",
 "<h3>Transformer configurations</h3>\n": "<h3>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba\u0db1\u0dca</h3>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> Create GPT model and initialize weights</p>\n": "<p> GPT\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 \u0dc3\u0dc4 \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p>Apply custom weight initialization </p>\n": "<p>\u0d85\u0db7\u0dd2\u0dbb\u0dd4\u0da0\u0dd2\u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Batch size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca\u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Collect names of parameters to apply weight decay </p>\n": "<p>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dba\u0dd9\u0daf\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca\u0d9c\u0dda \u0db1\u0db8\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create a <a href=\"../optimizers/configs.html#OptimizerConfigs\">configurable optimizer</a>, so that we can change these simply by passing a config dictionary. </p>\n": "<p><a href=\"../optimizers/configs.html#OptimizerConfigs\">\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0d9c\u0dad \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba\u0d9a\u0dca</a>\u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1, \u0d91\u0dc0\u0dd2\u0da7 \u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3 \u0dc1\u0db6\u0dca\u0daf\u0d9a\u0ddd\u0dc2\u0dba\u0d9a\u0dca \u0dc3\u0db8\u0dca\u0db8\u0dad \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0d85\u0db4\u0da7 \u0db8\u0dda\u0dc0\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n",
 "<p>Create configs </p>\n": "<p>\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create experiment </p>\n": "<p>\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf\u0db6\u0dd0\u0dbd\u0dd3\u0db8 \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create subsequent mask if mask is not initialized or if the size of the mask is different </p>\n": "<p>\u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0dc4\u0ddd \u0dc0\u0dd9\u0dc3\u0dca \u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0dd9\u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db1\u0db8\u0dca \u0db4\u0dc3\u0dd4\u0d9a\u0dcf\u0dbd\u0dd3\u0db1 \u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Custom optimizer </p>\n": "<p>\u0d85\u0db7\u0dd2\u0dbb\u0dd4\u0da0\u0dd2\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>GPT model </p>\n": "<p>GPT\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba </p>\n",
 "<p>GPT uses GELU activation for position wise feedforward </p>\n": "<p>GPT\u0dc3\u0dca\u0dae\u0dcf\u0db1\u0d9c\u0dad \u0db1\u0dd0\u0dab\u0dc0\u0dad\u0dca \u0db4\u0ddd\u0dc2\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf GELU \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2 </p>\n",
 "<p>GPT uses a maximum learning rate of <span translate=no>_^_0_^_</span>. </p>\n": "<p>GPT\u0d8b\u0db4\u0dbb\u0dd2\u0db8 \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_0_^_</span>. </p>\n",
 "<p>Get all the parameters </p>\n": "<p>\u0dc3\u0dd2\u0dba\u0dbd\u0dd4\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get logits </p>\n": "<p>\u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca\u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the token embeddings with positional encodings </p>\n": "<p>\u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dd3\u0dba\u0d9a\u0dda\u0dad\u0db1 \u0d9a\u0dca\u0dbb\u0db8 \u0dc3\u0db8\u0d9f \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Initialize biases to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0\u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Number of tokens for wamup </p>\n": "<p>\u0dc0\u0db8\u0dca\u0d85\u0db4\u0dca\u0dc3\u0db3\u0dc4\u0dcf \u0da7\u0ddd\u0d9a\u0db1 \u0d9c\u0dab\u0db1 </p>\n",
 "<p>Number of warmup optimization steps </p>\n": "<p>\u0d8b\u0db1\u0dd4\u0dc3\u0dd4\u0db8\u0dca\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0dab\u0db1 </p>\n",
 "<p>Override configurations </p>\n": "<p>\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba\u0db1\u0dca\u0d85\u0db7\u0dd2\u0db6\u0dc0\u0dcf \u0dba\u0db1\u0dca\u0db1 </p>\n",
 "<p>Parameters that are not decayed </p>\n": "<p>\u0daf\u0dd2\u0dbb\u0dcf\u0db4\u0dad\u0dca\u0db1\u0ddc\u0dc0\u0db1 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca </p>\n",
 "<p>Prompt separator is blank </p>\n": "<p>\u0d9a\u0da9\u0dd2\u0db1\u0db8\u0dca\u0db6\u0dd9\u0daf\u0dd4\u0db8\u0dca\u0d9a\u0dbb\u0dd4 \u0dc4\u0dd2\u0dc3\u0dca \u0dba </p>\n",
 "<p>Return results (second value is for state, since our trainer is used with RNNs also) </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dbd\u0dcf\u0db7\u0db4\u0dca\u0dbb\u0dad\u0dd2 results \u0dbd (\u0daf\u0dd9\u0dc0\u0db1 \u0d85\u0d9c\u0dba \u0dbb\u0dcf\u0da2\u0dca\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dda, \u0db8\u0db1\u0dca\u0daf \u0d85\u0db4\u0d9c\u0dda \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0d9a\u0dbb\u0dd4 RNs \u0dc3\u0db8\u0d9f \u0daf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2) </p>\n",
 "<p>Run training </p>\n": "<p>\u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0db0\u0dcf\u0dc0\u0db1\u0dba </p>\n",
 "<p>Set default weight decay. This is not required since we set the weight decay in the parameter groups. </p>\n": "<p>\u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2\u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1. \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca\u0dc0\u0dbd \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0db1\u0dd2\u0dc3\u0dcf \u0db8\u0dd9\u0dba \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0db1\u0ddc\u0dc0\u0dda. </p>\n",
 "<p>Set model embedding size, required if we use <a href=\"../optimizers/noam.html\">Noam optimizer</a> which has an exponential decay. </p>\n": "<p>Set\u0d86\u0daf\u0dbb\u0dca\u0dc1 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba, \u0d85\u0db4\u0dd2 \u0d9d\u0dcf\u0dad\u0dd3\u0dba \u0d9a\u0dca\u0dc2\u0dba \u0d87\u0dad\u0dd2 <a href=\"../optimizers/noam.html\">Noam optimizer</a> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0db8\u0dca \u0d85\u0dc0\u0dc1\u0dca\u0dba. </p>\n",
 "<p>Set models for saving and loading </p>\n": "<p>\u0d89\u0dad\u0dd2\u0dbb\u0dd2\u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0db4\u0dd0\u0da7\u0dc0\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1 </p>\n",
 "<p>Set parameter groups for optimization. </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba\u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1. </p>\n",
 "<p>Set the vocabulary sizes for embeddings and generating logits </p>\n": "<p>\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca\u0dc3\u0dc4 \u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0d8b\u0dad\u0dca\u0db4\u0dcf\u0daf\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0da0\u0db1 \u0db8\u0dcf\u0dbd\u0dcf\u0dc0 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1 </p>\n",
 "<p>Start the experiment </p>\n": "<p>\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf\u0db6\u0dd0\u0dbd\u0dd3\u0db8 \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Starting prompt for sampling </p>\n": "<p>\u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd3\u0db8\u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0d9a\u0dca \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>Subsequent mask, will mask out tokens from seeing future tokens </p>\n": "<p>\u0db4\u0dc3\u0dd4\u0d9a\u0dcf\u0dbd\u0dd3\u0db1\u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab, \u0d85\u0db1\u0dcf\u0d9c\u0dad \u0da7\u0ddd\u0d9a\u0db1 \u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0da7\u0ddd\u0d9a\u0db1 \u0dc0\u0dc3\u0d82 \u0d9a\u0dbb\u0db1\u0dd4 \u0d87\u0dad </p>\n",
 "<p>Switch between training and validation for <span translate=no>_^_0_^_</span> times per epoch </p>\n": "<p>\u0d91\u0d9a\u0dca <span translate=no>_^_0_^_</span> \u0dba\u0dd4\u0d9c\u0dba\u0d9a\u0da7 \u0dc0\u0dbb\u0d9a\u0dca \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0dc0 \u0dc3\u0dc4 \u0dc0\u0dbd\u0d82\u0d9c\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d85\u0dad\u0dbb \u0db8\u0dcf\u0dbb\u0dd4 \u0dc0\u0db1\u0dca\u0db1 </p>\n",
 "<p>The mask will be initialized on the first call </p>\n": "<p>\u0db4\u0dc5\u0db8\u0dd4\u0d87\u0db8\u0dad\u0dd4\u0db8\u0dd9\u0db1\u0dca \u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dd4 \u0d87\u0dad </p>\n",
 "<p>Total number of optimization steps for learning rate cosine decay </p>\n": "<p>\u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8\u0dda\u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba \u0d9a\u0ddc\u0dc3\u0dba\u0dd2\u0db1\u0dca \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0dab\u0db1 </p>\n",
 "<p>Train for <span translate=no>_^_0_^_</span> epochs </p>\n": "<p><span translate=no>_^_0_^_</span> Epochs \u0dc3\u0db3\u0dc4\u0dcf \u0daf\u0dd4\u0db8\u0dca\u0dbb\u0dd2\u0dba </p>\n",
 "<p>Transformer </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca </p>\n",
 "<p>Transformer configurations </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba\u0db1\u0dca </p>\n",
 "<p>Transformer encoder </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca\u0d91\u0db1\u0dca\u0d9a\u0ddd\u0da9\u0dbb\u0dba </p>\n",
 "<p>Use <a href=\"../optimizers/adam_warmup_cosine_decay.html\">cosine decay optimizer</a>. This is what GPT uses. </p>\n": "<p><a href=\"../optimizers/adam_warmup_cosine_decay.html\">\u0d9a\u0ddc\u0dc3\u0dba\u0dd2\u0db1\u0dca \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dcf\u0dbb\u0d9a\u0dba</a>\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1. GPT \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db8\u0dd9\u0dba\u0dba\u0dd2. </p>\n",
 "<p>Use Tiny Shakespeare dataset </p>\n": "<p>\u0d9a\u0dd4\u0da9\u0dcf\u0dc2\u0dda\u0d9a\u0dca\u0dc3\u0dca\u0db4\u0dd2\u0dba\u0dbb\u0dca \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Use a context size of <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d9a\u0dc3\u0db1\u0dca\u0daf\u0dbb\u0dca\u0db7\u0dba \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Use character level tokenizer </p>\n": "<p>\u0d85\u0d9a\u0dca\u0dc2\u0dbb\u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>We use our <a href=\"../configs.html#TransformerConfigs\">configurable transformer implementation</a> </p>\n": "<p>\u0d85\u0db4\u0d9c\u0dda <a href=\"../configs.html#TransformerConfigs\">\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0d9c\u0dad \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8</a> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4 </p>\n",
 "<p>Weight decay </p>\n": "<p>\u0dc3\u0dd2\u0dbb\u0dd4\u0dbb\u0dda\u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba </p>\n",
 "<p>Weight decay is decoupled from gradients </p>\n": "<p>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0dc3\u0dd2\u0da7 decoupled \u0d87\u0dad </p>\n",
 "<p>create the pytorch optimizer object </p>\n": "<p>\u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0dc0\u0dc3\u0dca\u0dad\u0dd4\u0dc0 \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer <a href=\"../models.html#Encoder\">Encoder</a> </li>\n<li><span translate=no>_^_1_^_</span> is the token <a href=\"../models.html#EmbeddingsWithLearnedPositionalEncoding\">embedding module (with positional encodings)</a> </li>\n<li><span translate=no>_^_2_^_</span> is the <a href=\"../models.html#Generator\">final fully connected layer</a> that gives the logits.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca <a href=\"../models.html#Encoder\">\u0d91\u0db1\u0dca\u0d9a\u0ddd\u0da9\u0dbb\u0dba</a> </li>\n<li><span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 \u0da7\u0ddd\u0d9a\u0db1\u0dca <a href=\"../models.html#EmbeddingsWithLearnedPositionalEncoding\">\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba (\u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dd3\u0dba \u0d9a\u0dda\u0dad\u0dd3\u0d9a\u0dbb\u0dab \u0dc3\u0db8\u0d9f)</a> </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 <a href=\"../models.html#Generator\">\u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0db4\u0dd6\u0dbb\u0dca\u0dab \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dd2\u0dad \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dba\u0dd2</a> . </li></ul>\n",
 "GPT": "GPT",
 "Implementation/tutorial of GPT model and training code.": "GPT \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc3\u0dc4 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dda\u0dad\u0dba \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba."
}