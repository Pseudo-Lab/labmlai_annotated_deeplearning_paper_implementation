{
 "<h1>k-Nearest Neighbor Language Models</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper  <a href=\"https://papers.labml.ai/paper/1911.00172\">Generalization through Memorization: Nearest Neighbor Language Models</a>. It uses k-nearest neighbors to improve perplexity of autoregressive transformer models.</p>\n<p>An autoregressive language model estimates <span translate=no>_^_0_^_</span>,  where <span translate=no>_^_1_^_</span> is the token at step <span translate=no>_^_2_^_</span>  and <span translate=no>_^_3_^_</span> is the context, <span translate=no>_^_4_^_</span>.</p>\n<p>This paper, improves <span translate=no>_^_5_^_</span> using a k-nearest neighbor search  on key-value pairs <span translate=no>_^_6_^_</span>, with search key <span translate=no>_^_7_^_</span>.  Here <span translate=no>_^_8_^_</span> is an embedding of the context <span translate=no>_^_9_^_</span>.  The paper (and this implementation) uses the <strong>input to the feed-forward layer of the  final layer of the transformer</strong> as <span translate=no>_^_10_^_</span>.</p>\n<p>We use <a href=\"https://github.com/facebookresearch/faiss\">FAISS</a> to index <span translate=no>_^_11_^_</span>.</p>\n<h3>Implementation</h3>\n<p>So to run <span translate=no>_^_12_^_</span>NN-LM we need to:</p>\n<ul><li><a href=\"train_model.html\">Train a transformer model</a> </li>\n<li><a href=\"build_index.html\">Build an index</a> of <span translate=no>_^_13_^_</span> </li>\n<li><a href=\"eval_knn.html\">Evaluate kNN-ML</a> using <span translate=no>_^_14_^_</span>NN seach on <span translate=no>_^_15_^_</span> with <span translate=no>_^_16_^_</span></li></ul>\n<p>This experiment uses a small dataset so that we can run this without using up a few hundred giga-bytes of disk space for the index.</p>\n<p>The official implementation of <span translate=no>_^_17_^_</span>NN-LM can be found <a href=\"https://github.com/urvashik/knnlm\">here</a>.</p>\n": "<h1>K-\u6700\u8fd1\u508d\u8a00\u8a9e\u30e2\u30c7\u30eb</h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://papers.labml.ai/paper/1911.00172\">\u8a18\u61b6\u306b\u3088\u308b\u4e00\u822c\u5316\uff1a</a>\u6700\u8fd1\u508d\u8a00\u8a9e\u30e2\u30c7\u30eb\u300d<a href=\"https://pytorch.org\">\u3068\u3044\u3046\u8ad6\u6587\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002\u81ea\u5df1\u56de\u5e30\u5909\u63db\u30e2\u30c7\u30eb\u306e\u8907\u96d1\u3055\u3092\u6539\u5584\u3059\u308b\u305f\u3081\u306b\u3001k-\u6700\u8fd1\u508d\u3092\u4f7f\u7528\u3057\u307e\u3059</p>\u3002\n<p>\u81ea\u5df1\u56de\u5e30\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u306f<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u30b9\u30c6\u30c3\u30d7\u4e2d\u306e\u30c8\u30fc\u30af\u30f3\u304c\u3069\u3053\u3067\u3001<span translate=no>_^_3_^_</span>\u304c\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306a\u306e\u304b\u3092\u63a8\u5b9a\u3057\u307e\u3059\u3002<span translate=no>_^_4_^_</span></p>\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001<span translate=no>_^_6_^_</span>\u691c\u7d22\u30ad\u30fc\u4ed8\u304d\u306e\u30ad\u30fc\u3068\u5024\u306e\u30da\u30a2\u3067\u306e k <span translate=no>_^_5_^_</span> \u6700\u8fd1\u508d\u691c\u7d22\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u6539\u826f\u3057\u307e\u3057\u305f\u3002<span translate=no>_^_7_^_</span><span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span>\u3053\u308c\u304c\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u57cb\u3081\u8fbc\u307f\u3067\u3059\u3002\u3053\u306e\u8ad6\u6587\uff08\u304a\u3088\u3073\u3053\u306e\u5b9f\u88c5\uff09\u3067\u306f\u3001<strong>\u5909\u5727\u5668\u306e\u6700\u7d42\u5c64\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u5c64\u3078\u306e\u5165\u529b\u3092\u6b21\u306e\u3088\u3046\u306b\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</strong></p>\u3002<span translate=no>_^_10_^_</span>\n<p><a href=\"https://github.com/facebookresearch/faiss\"><span translate=no>_^_11_^_</span>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u306fFAISS\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</a>\u3002</p>\n<h3>\u5b9f\u88c5</h3>\n<p>\u3057\u305f\u304c\u3063\u3066\u3001<span translate=no>_^_12_^_</span> NN-LM \u3092\u5b9f\u884c\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p>\n<ul><li><a href=\"train_model.html\">\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0</a></li>\n<li><a href=\"build_index.html\">\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4f5c\u6210</a> <span translate=no>_^_13_^_</span></li>\n<li>\u4ee5\u4e0b\u306e NN \u691c\u7d22\u3092\u4f7f\u7528\u3057\u3066 <a href=\"eval_knn.html\"><span translate=no>_^_14_^_</span>knn-ML \u3092\u8a55\u4fa1\u3059\u308b</a> <span translate=no>_^_15_^_</span> <span translate=no>_^_16_^_</span></li></ul>\n<p>\u3053\u306e\u5b9f\u9a13\u3067\u306f\u5c0f\u3055\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u7528\u306b\u6570\u767e\u30ae\u30ac\u30d0\u30a4\u30c8\u306e\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u3092\u6d88\u8cbb\u305b\u305a\u306b\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002</p>\n<p><span translate=no>_^_17_^_</span><a href=\"https://github.com/urvashik/knnlm\">NN-LM \u306e\u6b63\u5f0f\u306a\u5b9f\u88c5\u306f\u3053\u3053\u306b\u3042\u308a\u307e\u3059\u3002</a></p>\n",
 "This is a simple PyTorch implementation/tutorial of the paper Generalization through Memorization: Nearest Neighbor Language Models using FAISS. It runs a kNN model on the final transformer layer embeddings to improve the loss of transformer based language models. It's also great for domain adaptation without pre-training.": "\u3053\u308c\u306f\u3001\u300c\u8a18\u61b6\u306b\u3088\u308b\u4e00\u822c\u5316\uff1aFAISS\u3092\u4f7f\u7528\u3057\u305f\u6700\u8fd1\u508d\u8a00\u8a9e\u30e2\u30c7\u30eb\u300d\u3068\u3044\u3046\u8ad6\u6587\u306e\u7c21\u5358\u306aPyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30d9\u30fc\u30b9\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30ed\u30b9\u3092\u6539\u5584\u3059\u308b\u305f\u3081\u306b\u3001\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5c64\u306e\u6700\u5f8c\u306e\u57cb\u3081\u8fbc\u307f\u3067kNN\u30e2\u30c7\u30eb\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u4e8b\u524d\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306a\u3057\u3067\u30c9\u30e1\u30a4\u30f3\u3092\u9069\u5fdc\u3055\u305b\u308b\u306e\u306b\u3082\u6700\u9069\u3067\u3059\u3002",
 "k-Nearest Neighbor Language Models": "K-\u6700\u8fd1\u508d\u8a00\u8a9e\u30e2\u30c7\u30eb"
}