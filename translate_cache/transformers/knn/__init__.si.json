{
 "<h1>k-Nearest Neighbor Language Models</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper  <a href=\"https://papers.labml.ai/paper/1911.00172\">Generalization through Memorization: Nearest Neighbor Language Models</a>. It uses k-nearest neighbors to improve perplexity of autoregressive transformer models.</p>\n<p>An autoregressive language model estimates <span translate=no>_^_0_^_</span>,  where <span translate=no>_^_1_^_</span> is the token at step <span translate=no>_^_2_^_</span>  and <span translate=no>_^_3_^_</span> is the context, <span translate=no>_^_4_^_</span>.</p>\n<p>This paper, improves <span translate=no>_^_5_^_</span> using a k-nearest neighbor search  on key-value pairs <span translate=no>_^_6_^_</span>, with search key <span translate=no>_^_7_^_</span>.  Here <span translate=no>_^_8_^_</span> is an embedding of the context <span translate=no>_^_9_^_</span>.  The paper (and this implementation) uses the <strong>input to the feed-forward layer of the  final layer of the transformer</strong> as <span translate=no>_^_10_^_</span>.</p>\n<p>We use <a href=\"https://github.com/facebookresearch/faiss\">FAISS</a> to index <span translate=no>_^_11_^_</span>.</p>\n<h3>Implementation</h3>\n<p>So to run <span translate=no>_^_12_^_</span>NN-LM we need to:</p>\n<ul><li><a href=\"train_model.html\">Train a transformer model</a> </li>\n<li><a href=\"build_index.html\">Build an index</a> of <span translate=no>_^_13_^_</span> </li>\n<li><a href=\"eval_knn.html\">Evaluate kNN-ML</a> using <span translate=no>_^_14_^_</span>NN seach on <span translate=no>_^_15_^_</span> with <span translate=no>_^_16_^_</span></li></ul>\n<p>This experiment uses a small dataset so that we can run this without using up a few hundred giga-bytes of disk space for the index.</p>\n<p>The official implementation of <span translate=no>_^_17_^_</span>NN-LM can be found <a href=\"https://github.com/urvashik/knnlm\">here</a>.</p>\n": "<h1>K-\u0dc5\u0d9f\u0db8\u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2\u0dba\u0dcf\u0d9c\u0dda \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2</h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2 <a href=\"https://papers.labml.ai/paper/1911.00172\">\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0da7\u0db4\u0dcf\u0da9\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8: \u0dc5\u0d9f\u0db8 \u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2 \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2</a> . \u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0d9c\u0dcf\u0db8\u0dd3 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dc0\u0dbd \u0d85\u0dc0\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf\u0dc0 \u0dc0\u0dd0\u0da9\u0dd2 \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d91\u0dba k-\u0dc5\u0d9f\u0db8 \u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2\u0dba\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\u0dc3\u0dca\u0dc0\u0dba\u0d82\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0d9c\u0dcf\u0db8\u0dd3 \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0d87\u0dc3\u0dca\u0dad\u0db8\u0dda\u0db1\u0dca\u0dad\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_0_^_</span>, \u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0dda\u0daf\u0dd3 \u0da7\u0ddd\u0d9a\u0db1\u0dba <span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span> \u0d9a\u0ddc\u0dad\u0dd0\u0db1\u0daf \u0dc3\u0dc4 \u0dc3\u0db1\u0dca\u0daf\u0dbb\u0dca\u0db7\u0dba <span translate=no>_^_3_^_</span> \u0dc0\u0dda, <span translate=no>_^_4_^_</span>. </p>\n<p>\u0db8\u0dd9\u0db8\u0dbd\u0dd2\u0db4\u0dd2\u0dba, \u0dc3\u0dd9\u0dc0\u0dd4\u0db8\u0dca \u0dba\u0dad\u0dd4\u0dbb \u0dc3\u0db8\u0d9f \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0dca \u0dba\u0dd4\u0d9c\u0dbd <span translate=no>_^_6_^_</span>\u0db8\u0dad k-\u0dc5\u0d9f\u0db8 \u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2\u0dba\u0dcf \u0dc3\u0dd9\u0dc0\u0dd4\u0db8\u0d9a\u0dca <span translate=no>_^_5_^_</span> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca \u0dc0\u0dd0\u0da9\u0dd2 \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_7_^_</span>. <span translate=no>_^_8_^_</span> \u0db8\u0dd9\u0db1\u0dca\u0db1 \u0dc3\u0db1\u0dca\u0daf\u0dbb\u0dca\u0db7\u0dba \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 <span translate=no>_^_9_^_</span>. \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 (\u0dc3\u0dc4 \u0db8\u0dd9\u0db8 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8) \u0dbd\u0dd9\u0dc3 <strong>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba\u0dda \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0ddd\u0dc2\u0d9a-\u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0da7 \u0d86\u0daf\u0dcf\u0db1\u0dba</strong> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2 <span translate=no>_^_10_^_</span>. </p>\n<p>\u0d85\u0db4\u0dd2\u0daf\u0dbb\u0dca\u0dc1\u0d9a\u0dba <a href=\"https://github.com/facebookresearch/faiss\">FAISS</a> \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_11_^_</span>. </p>\n<h3>\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a\u0d9a\u0dd2\u0dbb\u0dd3\u0db8</h3>\n<p>\u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca <span translate=no>_^_12_^_</span>NN-LM \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0db1\u0dca\u0db1\u0dda:</p>\n<ul><li><a href=\"train_model.html\">\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1</a> </li>\n<li><a href=\"build_index.html\">\u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0daf\u0dbb\u0dca\u0dc1\u0d9a\u0dba\u0d9a\u0dca \u0d9c\u0ddc\u0da9\u0db1\u0d9f\u0db1\u0dca\u0db1</a> <span translate=no>_^_13_^_</span> </li>\n<li><a href=\"eval_knn.html\"><span translate=no>_^_14_^_</span>NN seach \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca KN-\u0db8\u0dd2\u0dbd\u0dd2 \u0dad\u0d9a\u0dca\u0dc3\u0dda\u0dbb\u0dd4</a> \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_15_^_</span> <span translate=no>_^_16_^_</span></li></ul>\n<p>\u0db8\u0dd9\u0db8\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8 \u0d9a\u0dd4\u0da9\u0dcf \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0d91\u0db8\u0d9f\u0dd2\u0db1\u0dca \u0daf\u0dbb\u0dca\u0dc1\u0d9a\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d9c\u0dd2\u0d9c\u0dcf \u0db6\u0dba\u0dd2\u0da7\u0dca \u0dc3\u0dd2\u0dba \u0d9c\u0dab\u0db1\u0d9a\u0dca \u0dad\u0dd0\u0da7\u0dd2\u0dba\u0dda \u0d89\u0da9 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0d9a\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0db1\u0ddc\u0d9a\u0dbb \u0db8\u0dd9\u0dba \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n<p><span translate=no>_^_17_^_</span>NN-LM \u0dc4\u0dd2 \u0db1\u0dd2\u0dbd \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 <a href=\"https://github.com/urvashik/knnlm\">\u0db8\u0dd9\u0dc4\u0dd2</a>\u0dc3\u0ddc\u0dba\u0dcf\u0d9c\u0dad \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n",
 "This is a simple PyTorch implementation/tutorial of the paper Generalization through Memorization: Nearest Neighbor Language Models using FAISS. It runs a kNN model on the final transformer layer embeddings to improve the loss of transformer based language models. It's also great for domain adaptation without pre-training.": "\u0db8\u0dd9\u0dba \u0dc3\u0dbb\u0dbd PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0db8\u0dad\u0d9a \u0dad\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc4\u0dbb\u0dc4\u0dcf \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba: FAISS \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d86\u0dc3\u0db1\u0dca\u0db1\u0dad\u0db8 \u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2 \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2. \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0db4\u0daf\u0db1\u0db8\u0dca \u0d9a\u0dbb\u0d9c\u0dad\u0dca \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0db1\u0dd0\u0dad\u0dd2\u0dc0\u0dd3\u0db8 \u0dc0\u0dd0\u0da9\u0dd2 \u0daf\u0dd2\u0dba\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0db8\u0dad \u0d91\u0dba kN \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0db4\u0dd9\u0dbb \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0dc0\u0d9a\u0dd2\u0db1\u0dca \u0dad\u0ddc\u0dbb\u0dc0 \u0da9\u0ddc\u0db8\u0dda\u0db1\u0dca \u0d85\u0db1\u0dd4\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0daf \u0d91\u0dba \u0dc0\u0dd2\u0dc1\u0dd2\u0dc2\u0dca\u0da7\u0dba\u0dd2.",
 "k-Nearest Neighbor Language Models": "K-\u0dc5\u0d9f\u0db8 \u0d85\u0dc3\u0dbd\u0dca\u0dc0\u0dd0\u0dc3\u0dd2\u0dba\u0dcf\u0d9c\u0dda \u0db7\u0dcf\u0dc2\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2"
}