{
 "<h1>Vision Transformer (ViT)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2010.11929\">An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</a>.</p>\n<p>Vision transformer applies a pure transformer to images without any convolution layers. They split the image into patches and apply a transformer on patch embeddings. <a href=\"#PathEmbeddings\">Patch embeddings</a> are generated by applying a simple linear transformation to the flattened pixel values of the patch. Then a standard transformer encoder is fed with the patch embeddings, along with a classification token <span translate=no>_^_0_^_</span>. The encoding on the <span translate=no>_^_1_^_</span> token is used to classify the image with an MLP.</p>\n<p>When feeding the transformer with the patches, learned positional embeddings are added to the patch embeddings, because the patch embeddings do not have any information about where that patch is from. The positional embeddings are a set of vectors for each patch location that get trained with gradient descent along with other parameters.</p>\n<p>ViTs perform well when they are pre-trained on large datasets. The paper suggests pre-training them with an MLP classification head and then using a single linear layer when fine-tuning. The paper beats SOTA with a ViT pre-trained on a 300 million image dataset. They also use higher resolution images during inference while keeping the patch size the same. The positional embeddings for new patch locations are calculated by interpolating learning positional embeddings.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">an experiment</a> that trains ViT on CIFAR-10. This doesn&#x27;t do very well because it&#x27;s trained on a small dataset. It&#x27;s a simple experiment that anyone can run and play with ViTs.</p>\n": "<h1>\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (ViT)</h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://papers.labml.ai/paper/2010.11929\">\u753b\u50cf\u306f16x16\u306e\u8a00\u8449\u306b\u5024\u3059\u308b\u300d\u3068\u3044\u3046\u8ad6\u6587\u300c\u5927\u898f\u6a21\u753b\u50cf\u8a8d\u8b58\u306e\u305f\u3081\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</a></p>\n<p>\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3001\u7573\u307f\u8fbc\u307f\u5c64\u306e\u306a\u3044\u753b\u50cf\u306b\u7d14\u7c8b\u306a\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u9069\u7528\u3057\u307e\u3059\u3002\u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3001\u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f\u306b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u9069\u7528\u3057\u307e\u3059\u3002<a href=\"#PathEmbeddings\">\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u306f</a>\u3001\u30d1\u30c3\u30c1\u306e\u5e73\u5766\u5316\u3055\u308c\u305f\u30d4\u30af\u30bb\u30eb\u5024\u306b\u5358\u7d14\u306a\u7dda\u5f62\u5909\u63db\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u307e\u3059\u3002\u6b21\u306b\u3001\u6a19\u6e96\u306e\u30c8\u30e9\u30f3\u30b9\u30a8\u30f3\u30b3\u30fc\u30c0\u306b\u3001\u5206\u985e\u30c8\u30fc\u30af\u30f3\u3068\u3068\u3082\u306b\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u304c\u4f9b\u7d66\u3055\u308c\u307e\u3059\u3002<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306f\u3001\u753b\u50cf\u3092MLP\u3067\u5206\u985e\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u30c8\u30e9\u30f3\u30b9\u306b\u30d1\u30c3\u30c1\u3092\u4f9b\u7d66\u3059\u308b\u969b\u3001\u5b66\u7fd2\u3057\u305f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u304c\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u306b\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u306b\u306f\u305d\u306e\u30d1\u30c3\u30c1\u304c\u3069\u3053\u304b\u3089\u6765\u305f\u304b\u306b\u3064\u3044\u3066\u306e\u60c5\u5831\u304c\u306a\u3044\u305f\u3081\u3067\u3059\u3002\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u306f\u3001\u5404\u30d1\u30c3\u30c1\u4f4d\u7f6e\u306e\u30d9\u30af\u30c8\u30eb\u306e\u30bb\u30c3\u30c8\u3067\u3001\u4ed6\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u3068\u3082\u306b\u52fe\u914d\u964d\u4e0b\u6cd5\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u307e\u3059</p>\u3002\n<p>VIT\u306f\u3001\u5927\u898f\u6a21\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3066\u304a\u304f\u3068\u3046\u307e\u304f\u6a5f\u80fd\u3057\u307e\u3059\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001MLP\u5206\u985e\u30d8\u30c3\u30c9\u3067\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3001\u5fae\u8abf\u6574\u306e\u969b\u306b\u306f\u5358\u4e00\u306e\u7dda\u5f62\u5c64\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u306e\u8ad6\u6587\u306f\u30013\u5104\u306e\u753b\u50cf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305fVIT\u3067SOTA\u3092\u4e0a\u56de\u3063\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u30d1\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u540c\u3058\u306b\u4fdd\u3061\u306a\u304c\u3089\u3001\u63a8\u8ad6\u6642\u306b\u306f\u9ad8\u89e3\u50cf\u5ea6\u306e\u753b\u50cf\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u65b0\u3057\u3044\u30d1\u30c3\u30c1\u4f4d\u7f6e\u306e\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u306f\u3001\u5b66\u7fd2\u3057\u305f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3092\u88dc\u9593\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u3053\u308c\u306f<a href=\"experiment.html\">\u3001CIFAR-10 \u3067 VIT \u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5b9f\u9a13\u3067\u3059</a>\u3002\u3053\u308c\u306f\u5c0f\u3055\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u3042\u307e\u308a\u3046\u307e\u304f\u3044\u304d\u307e\u305b\u3093\u3002\u8ab0\u3067\u3082\u8d70\u3063\u3066VIT\u3067\u904a\u3079\u308b\u7c21\u5358\u306a\u5b9f\u9a13\u3067\u3059</p>\u3002\n",
 "<h2>Vision Transformer</h2>\n<p>This combines the <a href=\"#PatchEmbeddings\">patch embeddings</a>, <a href=\"#LearnedPositionalEmbeddings\">positional embeddings</a>, transformer and the <a href=\"#ClassificationHead\">classification head</a>.</p>\n": "<h2>\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc</h2>\n<p><a href=\"#ClassificationHead\">\u3053\u308c\u306b\u3088\u308a\u3001<a href=\"#PatchEmbeddings\">\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u3001\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f</a><a href=\"#LearnedPositionalEmbeddings\">\u3001\u5909\u5727\u5668\u3001\u5206\u985e\u30d8\u30c3\u30c9\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u307e\u3059\u3002</a></a></p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <a id=\"ClassificationHead\"></a></p>\n<h2>MLP Classification Head</h2>\n<p>This is the two layer MLP head to classify the image based on <span translate=no>_^_0_^_</span> token embedding.</p>\n": "<p><a id=\"ClassificationHead\"></a></p>\n<h2>MLP \u30af\u30e9\u30b9\u5206\u3051\u30d8\u30c3\u30c9</h2>\n<p>\u3053\u308c\u306f\u3001<span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u306b\u57fa\u3065\u3044\u3066\u753b\u50cf\u3092\u5206\u985e\u3059\u308b\u305f\u3081\u306e2\u5c64\u306eMLP\u30d8\u30c3\u30c9\u3067\u3059\u3002</p>\n",
 "<p> <a id=\"LearnedPositionalEmbeddings\"></a></p>\n<h2>Add parameterized positional encodings</h2>\n<p>This adds learned positional embeddings to patch embeddings.</p>\n": "<p><a id=\"LearnedPositionalEmbeddings\"></a></p>\n<h2>\u30d1\u30e9\u30e1\u30fc\u30bf\u5316\u3055\u308c\u305f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u8ffd\u52a0</h2>\n<p>\u3053\u308c\u306b\u3088\u308a\u3001\u5b66\u7fd2\u3057\u305f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u304c\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u306b\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<p> <a id=\"PatchEmbeddings\"></a></p>\n<h2>Get patch embeddings</h2>\n<p>The paper splits the image into patches of equal size and do a linear transformation on the flattened pixels for each patch.</p>\n<p>We implement the same thing through a convolution layer, because it&#x27;s simpler to implement.</p>\n": "<p><a id=\"PatchEmbeddings\"></a></p>\n<h2>\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u3092\u5165\u624b</h2>\n<p>\u7528\u7d19\u306f\u753b\u50cf\u3092\u540c\u3058\u30b5\u30a4\u30ba\u306e\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3001\u30d1\u30c3\u30c1\u3054\u3068\u306b\u5e73\u5766\u5316\u3055\u308c\u305f\u30d4\u30af\u30bb\u30eb\u3092\u7dda\u5f62\u5909\u63db\u3057\u307e\u3059\u3002</p>\n<p>\u5b9f\u88c5\u304c\u7c21\u5358\u306a\u305f\u3081\u3001\u7573\u307f\u8fbc\u307f\u5c64\u3067\u3082\u540c\u3058\u3053\u3068\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> token embedding </p>\n": "<p><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f</p>\n",
 "<p>Activation </p>\n": "<p>\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Add positional embeddings </p>\n": "<p>\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3092\u8ffd\u52a0</p>\n",
 "<p>Add to patch embeddings and return </p>\n": "<p>\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u306b\u8ffd\u52a0\u3057\u3066\u8fd4\u3059</p>\n",
 "<p>Apply convolution layer </p>\n": "<p>\u7573\u307f\u8fbc\u307f\u5c64\u3092\u9069\u7528</p>\n",
 "<p>Classification head </p>\n": "<p>\u5206\u985e\u30d8\u30c3\u30c9</p>\n",
 "<p>Classification head, to get logits </p>\n": "<p>\u30ed\u30b8\u30c3\u30c8\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u5206\u985e\u30d8\u30c3\u30c9</p>\n",
 "<p>Concatenate the <span translate=no>_^_0_^_</span> token embeddings before feeding the transformer </p>\n": "<p><span translate=no>_^_0_^_</span>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306b\u7d66\u96fb\u3059\u308b\u524d\u306b\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u9023\u7d50\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7d42\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>First layer </p>\n": "<p>\u7b2c 1 \u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>First layer and activation </p>\n": "<p>\u7b2c1\u5c64\u3068\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Get patch embeddings. This gives a tensor of shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f\u3092\u5165\u624b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3053\u308c\u306b\u3088\u308a\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u304c\u5f97\u3089\u308c\u307e\u3059\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get the positional embeddings for the given patches </p>\n": "<p>\u4e0e\u3048\u3089\u308c\u305f\u30d1\u30c3\u30c1\u306e\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3092\u53d6\u5f97</p>\n",
 "<p>Get the shape. </p>\n": "<p>\u5f62\u3092\u624b\u306b\u5165\u308c\u3088\u3046\u3002</p>\n",
 "<p>Get the transformer output of the <span translate=no>_^_0_^_</span> token (which is the first in the sequence). </p>\n": "<p><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3 (\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u6700\u521d\u306e\u3082\u306e) \u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u51fa\u529b\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p>\n",
 "<p>Layer normalization </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210</p>\n",
 "<p>Pass through transformer layers with no attention masking </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30de\u30b9\u30af\u306a\u3057\u3067\u5909\u5727\u5668\u5c64\u3092\u901a\u904e</p>\n",
 "<p>Patch embeddings </p>\n": "<p>\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f</p>\n",
 "<p>Positional embeddings for each location </p>\n": "<p>\u5404\u30ed\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f</p>\n",
 "<p>Rearrange to shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u56f3\u5f62\u306b\u518d\u914d\u7f6e <span translate=no>_^_0_^_</span></p>\n",
 "<p>Return the patch embeddings </p>\n": "<p>\u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f\u3092\u8fd4\u3059</p>\n",
 "<p>Second layer </p>\n": "<p>\u7b2c 2 \u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>We create a convolution layer with a kernel size and and stride length equal to patch size. This is equivalent to splitting the image into patches and doing a linear transformation on each patch. </p>\n": "<p>\u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u3067\u30b9\u30c8\u30e9\u30a4\u30c9\u306e\u9577\u3055\u304c\u30d1\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u540c\u3058\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3001\u5404\u30d1\u30c3\u30c1\u3067\u7dda\u5f62\u5909\u63db\u3092\u884c\u3046\u306e\u3068\u540c\u3058\u3067\u3059</p>\u3002\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a copy of a single <a href=\"../models.html#TransformerLayer\">transformer layer</a>.  We make copies of it to make the transformer with <span translate=no>_^_1_^_</span>. </li>\n<li><span translate=no>_^_2_^_</span> is the number of <a href=\"../models.html#TransformerLayer\">transformer layers</a>. </li>\n<li><span translate=no>_^_3_^_</span> is the <a href=\"#PatchEmbeddings\">patch embeddings layer</a>. </li>\n<li><span translate=no>_^_4_^_</span> is the <a href=\"#LearnedPositionalEmbeddings\">positional embeddings layer</a>. </li>\n<li><span translate=no>_^_5_^_</span> is the <a href=\"#ClassificationHead\">classification head</a>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>1 <a href=\"../models.html#TransformerLayer\">\u3064\u306e\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30b3\u30d4\u30fc\u3067\u3059</a>\u3002<span translate=no>_^_1_^_</span>\u305d\u308c\u3092\u30b3\u30d4\u30fc\u3057\u3066\u5909\u5727\u5668\u3092\u4f5c\u308a\u307e\u3059</li>\u3002\n<li><span translate=no>_^_2_^_</span><a href=\"../models.html#TransformerLayer\">\u5909\u5727\u5668\u5c64\u306e\u6570\u3067\u3059</a>\u3002</li>\n<li><span translate=no>_^_3_^_</span><a href=\"#PatchEmbeddings\">\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc\u3067\u3059</a>\u3002</li>\n<li><span translate=no>_^_4_^_</span><a href=\"#LearnedPositionalEmbeddings\">\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc\u3067\u3059</a>\u3002</li>\n<li><span translate=no>_^_5_^_</span><a href=\"#ClassificationHead\">\u5206\u985e\u8cac\u4efb\u8005\u3067\u3059</a>\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input image of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u306e\u5165\u529b\u753b\u50cf\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the patch embeddings of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u306e\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the size of the hidden layer </li>\n<li><span translate=no>_^_2_^_</span> is the number of classes in the classification task</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5909\u5727\u5668\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u96a0\u308c\u30ec\u30a4\u30e4\u30fc\u306e\u30b5\u30a4\u30ba</li>\n<li><span translate=no>_^_2_^_</span>\u5206\u985e\u30bf\u30b9\u30af\u5185\u306e\u30af\u30e9\u30b9\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer embeddings size </li>\n<li><span translate=no>_^_1_^_</span> is the maximum number of patches</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5909\u5727\u5668\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u30d1\u30c3\u30c1\u306e\u6700\u5927\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer embeddings size </li>\n<li><span translate=no>_^_1_^_</span> is the size of the patch </li>\n<li><span translate=no>_^_2_^_</span> is the number of channels in the input image (3 for rgb)</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5909\u5727\u5668\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u30d1\u30c3\u30c1\u306e\u30b5\u30a4\u30ba</li>\n<li><span translate=no>_^_2_^_</span>\u306f\u5165\u529b\u753b\u50cf\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 (RGB \u306e\u5834\u5408\u306f 3)</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer encoding for <span translate=no>_^_1_^_</span> token</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "A PyTorch implementation/tutorial of the paper \"An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale\"": "\u8ad6\u6587\u300c\u753b\u50cf\u306f16x16\u306e\u8a00\u8449\u306b\u5024\u3059\u308b\uff1a\u5927\u898f\u6a21\u306a\u753b\u50cf\u8a8d\u8b58\u306e\u305f\u3081\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u300d\u306ePyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb",
 "Vision Transformer (ViT)": "\u30d3\u30b8\u30e7\u30f3\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc (ViT)"
}