{
 "<h1><a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP-Mixer: An all-MLP Architecture for Vision</a></h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.01601\">MLP-Mixer: An all-MLP Architecture for Vision</a>.</p>\n<p>This paper applies the model on vision tasks. The model is similar to a transformer with attention layer being replaced by a MLP that is applied across the patches (or tokens in case of a NLP task).</p>\n<p>Our implementation of MLP Mixer is a drop in replacement for the <a href=\"https://nn.labml.ai/transformers/mha.html\">self-attention layer</a> in <a href=\"https://nn.labml.ai/transformers/models.html\">our transformer implementation</a>. So it&#x27;s just a couple of lines of code, transposing the tensor to apply the MLP across the sequence dimension.</p>\n<p>Although the paper applied MLP Mixer on vision tasks, we tried it on a <a href=\"https://nn.labml.ai/transformers/mlm/index.html\">masked language model</a>. <a href=\"https://nn.labml.ai/transformers/mlp_mixer/experiment.html\">Here is the experiment code</a>. </p>\n": "<h1><a href=\"https://nn.labml.ai/transformers/mlp_mixer/index.html\">MLP\u30df\u30ad\u30b5\u30fc:\u30d3\u30b8\u30e7\u30f3\u7528\u306e\u30aa\u30fc\u30ebMLP\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</a></h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://papers.labml.ai/paper/2105.01601\">MLP\u30df\u30ad\u30b5\u30fc\uff1a\u30d3\u30b8\u30e7\u30f3\u7528\u306e\u30aa\u30fc\u30ebMLP\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</p>\n<p>\u672c\u7a3f\u3067\u306f\u3001\u3053\u306e\u30e2\u30c7\u30eb\u3092\u30d3\u30b8\u30e7\u30f3\u30bf\u30b9\u30af\u306b\u9069\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u304c\u30d1\u30c3\u30c1\uff08NLP\u30bf\u30b9\u30af\u306e\u5834\u5408\u306f\u30c8\u30fc\u30af\u30f3\uff09\u5168\u4f53\u306b\u9069\u7528\u3055\u308c\u308bMLP\u306b\u7f6e\u304d\u63db\u3048\u3089\u308c\u308b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306b\u4f3c\u3066\u3044\u307e\u3059</p>\u3002\n<p>MLP Mixer\u306e\u5b9f\u88c5\u306f\u3001<a href=\"https://nn.labml.ai/transformers/mha.html\"><a href=\"https://nn.labml.ai/transformers/models.html\">\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5b9f\u88c5\u306e\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u306b\u4ee3\u308f\u308b\u3082\u306e\u3067\u3059</a></a>\u3002\u3064\u307e\u308a\u3001\u30c6\u30f3\u30bd\u30eb\u3092\u8ee2\u7f6e\u3057\u3066\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u6b21\u5143\u5168\u4f53\u306b MLP \u3092\u9069\u7528\u3059\u308b\u3060\u3051\u306e\u30b3\u30fc\u30c9\u3067\u3059</p>\u3002\n<p>\u3053\u306e\u8ad6\u6587\u3067\u306f\u8996\u899a\u30bf\u30b9\u30af\u306bMLP Mixer\u3092\u9069\u7528\u3057\u307e\u3057\u305f\u304c\u3001<a href=\"https://nn.labml.ai/transformers/mlm/index.html\">\u30de\u30b9\u30af\u3055\u308c\u305f\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u8a66\u3057\u3066\u307f\u307e\u3057\u305f</a>\u3002<a href=\"https://nn.labml.ai/transformers/mlp_mixer/experiment.html\">\u3053\u308c\u304c\u5b9f\u9a13\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n",
 "MLP-Mixer: An all-MLP Architecture for Vision": "MLP\u30df\u30ad\u30b5\u30fc:\u30d3\u30b8\u30e7\u30f3\u7528\u306e\u30aa\u30fc\u30ebMLP\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3"
}