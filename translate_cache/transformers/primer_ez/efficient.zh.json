{
 "<h2>Multi-DConv-Head Attention (MDHA)</h2>\n<p>We extend our original implementation of <a href=\"../mha.html#MHA\">Multi-Head Attention</a> and add the spatial depth-wise convolution to query, key and value projections.</p>\n": "<h2>\u591a dconv-Head \u6ce8\u610f\u529b (MDHA)</h2>\n<p>\u6211\u4eec\u6269\u5c55\u4e86\u6700\u521d\u7684 M <a href=\"../mha.html#MHA\">ulti-Head</a> Attention \u5b9e\u73b0\uff0c\u5e76\u5c06\u7a7a\u95f4\u6df1\u5ea6\u5377\u79ef\u6dfb\u52a0\u5230\u67e5\u8be2\u3001\u952e\u548c\u503c\u6295\u5f71\u4e2d\u3002</p>\n",
 "<h2>Spatial Depth Wise Convolution</h2>\n<p>This is actually slower</p>\n": "<h2>\u7a7a\u95f4\u6df1\u5ea6\u660e\u667a\u5377\u79ef</h2>\n<p>\u8fd9\u5176\u5b9e\u6bd4\u8f83\u6162</p>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_1_^_</span></p>\n",
 "<p><a href=\"../mha.html#MHA\">Multi-Head Attention</a> will create query, key and value projection modules <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, and <span translate=no>_^_2_^_</span>.</p>\n<p>We combine a spatial depth-wise convolution layer to each of them and replace <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span>, and <span translate=no>_^_5_^_</span>. </p>\n": "<p><a href=\"../mha.html#MHA\">Multi-Head</a> Attention \u5c06\u521b\u5efa\u67e5\u8be2\u3001\u952e\u548c\u4ef7\u503c\u6295\u5f71\u6a21\u5757<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u3001\u548c<span translate=no>_^_2_^_</span>\u3002</p>\n<p>\u6211\u4eec\u5c06\u7a7a\u95f4\u6df1\u5ea6\u5377\u79ef\u5c42\u7ec4\u5408\u5230\u6bcf\u4e2a\u5c42\u4e0a\uff0c\u5e76\u66ff\u6362<span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u3001\u548c<span translate=no>_^_5_^_</span>\u3002</p>\n",
 "<p>We use PyTorch&#x27;s <span translate=no>_^_0_^_</span> module. We set the number of groups to be equal to the number of channels so that it does a separate convolution (with different kernels) for each channel. We add padding to both sides and later crop the right most <span translate=no>_^_1_^_</span> results </p>\n": "<p>\u6211\u4eec\u4f7f\u7528 PyTorch \u7684<span translate=no>_^_0_^_</span>\u6a21\u5757\u3002\u6211\u4eec\u5c06\u7ec4\u7684\u6570\u91cf\u8bbe\u7f6e\u4e3a\u7b49\u4e8e\u901a\u9053\u6570\uff0c\u4ee5\u4fbf\u5b83\u5bf9\u6bcf\u4e2a\u901a\u9053\u8fdb\u884c\u5355\u72ec\u7684\u5377\u79ef\uff08\u4f7f\u7528\u4e0d\u540c\u7684\u5185\u6838\uff09\u3002\u6211\u4eec\u5728\u4e24\u8fb9\u6dfb\u52a0\u586b\u5145\uff0c\u7136\u540e\u88c1\u526a\u6700\u53f3\u8fb9\u7684<span translate=no>_^_1_^_</span>\u7ed3\u679c</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of channels in each head</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u6bcf\u4e2a head \u4e2d\u7684\u901a\u9053\u6570</li></ul>\n",
 "efficient.py": "efficient.py"
}