{
 "<h1>Compressive Transformer</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1911.05507\">Compressive Transformers for Long-Range Sequence Modelling</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>This is an extension of <a href=\"../xl/index.html\">Transformer XL</a> where past memories are compressed to give a longer attention range. That is, the furthest <span translate=no>_^_0_^_</span> memories are compressed into <span translate=no>_^_1_^_</span> memories, where <span translate=no>_^_2_^_</span> is the compression rate.</p>\n<h2>Compression operation</h2>\n<p>The compression operation is defined as <span translate=no>_^_3_^_</span>. The paper introduces multiple choices for <span translate=no>_^_4_^_</span> and we have only implemented 1D convolution which seems to give the best results. Each layer has a separate compression operation <span translate=no>_^_5_^_</span> where <span translate=no>_^_6_^_</span> is the layer number.</p>\n<h2>Training compression operation</h2>\n<p>Since training compression with BPTT requires maintaining a very large computational graph (many time steps), the paper proposes an <em>auto-encoding loss</em> and an <em>attention reconstruction loss</em>. The auto-encoding loss decodes the original memories from the compressed memories and calculates the loss. Attention reconstruction loss computes the multi-headed attention results on the compressed memory and on uncompressed memory and gets a mean squared error between them. We have implemented the latter here since it gives better results.</p>\n<p>This implementation uses pre-layer normalization while the paper uses post-layer normalization. Pre-layer norm does the layer norm before <a href=\"../feedforward.html\">FFN</a> and self-attention, and the pass-through in the residual connection is not normalized. This is supposed to be more stable in standard transformer setups.</p>\n<p>Here are <a href=\"experiment.html\">the training code</a> and a notebook for training a compressive transformer model on the Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a> <a href=\"https://app.labml.ai/run/0d9b5338726c11ebb7c80242ac1c0002\"><span translate=no>_^_8_^_</span></a></p>\n": "<h1>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca</h1>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://pytorch.org\">PyTorch</a> \u0dc4\u0dd2 <a href=\"https://papers.labml.ai/paper/1911.05507\">\u0daf\u0dd2\u0d9c\u0dd4 \u0db4\u0dbb\u0dcf\u0dc3 \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0d86\u0d9a\u0dd8\u0dad\u0dd2 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca</a> \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2. </p>\n<p>\u0db8\u0dd9\u0dba <a href=\"../xl/index.html\">\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d91\u0d9a\u0dca\u0dc3\u0dca\u0d91\u0dbd\u0dca</a> \u0dc4\u0dd2 \u0daf\u0dd2\u0d9c\u0dd4\u0dc0\u0d9a\u0dd2, \u0d91\u0dc4\u0dd2\u0daf\u0dd3 \u0d85\u0dad\u0dd3\u0dad \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0daf\u0dd2\u0d9c\u0dd4 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0dc0\u0dda. \u0d91\u0db1\u0db8\u0dca, furthest <span translate=no>_^_0_^_</span> \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca <span translate=no>_^_1_^_</span> \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0db6\u0dc0\u0da7 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda, \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1 \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba \u0d9a\u0ddc\u0dc4\u0dd9\u0daf <span translate=no>_^_2_^_</span> . </p>\n<h2>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1\u0db8\u0dd9\u0dc4\u0dd9\u0dba\u0dd4\u0db8</h2>\n<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1\u0db8\u0dd9\u0dc4\u0dd9\u0dba\u0dd4\u0db8 \u0dbd\u0dd9\u0dc3 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dda <span translate=no>_^_3_^_</span>. \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dc4\u0dd4 \u0dad\u0dda\u0dbb\u0dd3\u0db8\u0dca \u0dc4\u0db3\u0dd4\u0db1\u0dca\u0dc0\u0dcf \u0daf\u0dd9\u0db1 <span translate=no>_^_4_^_</span> \u0d85\u0dad\u0dbb \u0d85\u0db4 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb \u0d87\u0dad\u0dca\u0dad\u0dda 1D \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dca\u0db8 \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0d91\u0dba \u0dc4\u0ddc\u0db3\u0db8 \u0db4\u0dca\u0dbb\u0dad\u0dd2. \u0dbd \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 \u0db6\u0dc0 \u0db4\u0dd9\u0db1\u0dda. \u0dc3\u0dd1\u0db8 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0db8 \u0dc0\u0dd9\u0db1\u0db8 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1 \u0db8\u0dd9\u0dc4\u0dd9\u0dba\u0dd4\u0db8\u0d9a\u0dca <span translate=no>_^_5_^_</span> \u0d87\u0dad. <span translate=no>_^_6_^_</span> </p>\n<h2>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1\u0db8\u0dd9\u0dc4\u0dd9\u0dba\u0dd4\u0db8 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8</h2>\n<p>\u0db6\u0dd3\u0db4\u0dd3\u0da7\u0dd3\u0da7\u0dd3\u0dc3\u0db8\u0d9f \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d89\u0dad\u0dcf \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0db4\u0dbb\u0dd2\u0d9c\u0dab\u0d9a\u0db8\u0dba \u0db4\u0dca\u0dbb\u0dc3\u0dca\u0dae\u0dcf\u0dbb\u0dba\u0d9a\u0dca (\u0db6\u0ddc\u0dc4\u0ddd \u0d9a\u0dcf\u0dbd \u0db4\u0dd2\u0dba\u0dc0\u0dbb) \u0db4\u0dc0\u0dad\u0dca\u0dc0\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0db1 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca, \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 <em>\u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d9a\u0dca\u0dbb\u0dd3\u0dba\u0dc0 \u0d9a\u0dda\u0dad\u0dd3\u0d9a\u0dbb\u0dab \u0d85\u0dbd\u0dcf\u0db7\u0dba\u0d9a\u0dca</em> \u0dc3\u0dc4 <em>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d85\u0dbd\u0dcf\u0db7\u0dba\u0d9a\u0dca</em> \u0dba\u0ddd\u0da2\u0db1\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0d9a\u0dda\u0dad\u0dd3\u0d9a\u0dbb\u0dab \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0db8\u0dd4\u0dbd\u0dca \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0dc0\u0dd2\u0d9a\u0dda\u0dad\u0db1\u0dba \u0d9a\u0dbb \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d85\u0dc4\u0dd2\u0db8\u0dd2 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba \u0db8\u0dad \u0dc3\u0dc4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db1\u0ddc\u0dc0\u0db1 \u0db8\u0dad\u0d9a\u0dba \u0db8\u0dad \u0db6\u0dc4\u0dd4-\u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dad\u0dca\u0dc0\u0dba\u0dd9\u0db1\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db5\u0dbd \u0d9c\u0dab\u0db1\u0dba \u0dc4\u0dcf \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0d85\u0dad\u0dbb \u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba \u0dc0\u0dbb\u0dca\u0d9c \u0daf\u0ddd\u0dc2\u0dba\u0d9a\u0dca \u0dbd\u0dd0\u0db6\u0dd9\u0db1. \u0dc0\u0da9\u0dcf \u0dc4\u0ddc\u0db3 \u0db4\u0dca\u0dbb\u0dad\u0dd2. \u0dbd \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0d85\u0db4\u0dd2 \u0db8\u0dd9\u0dc4\u0dd2 \u0daf\u0dd9\u0dc0\u0dd0\u0db1\u0dca\u0db1 \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb \u0d87\u0dad\u0dca\u0dad\u0dd9\u0db8\u0dd4. </p>\n<p>\u0db8\u0dd9\u0db8\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0db4\u0dd6\u0dbb\u0dca\u0dc0 \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0db4\u0dc1\u0dca\u0da0\u0dcf\u0dad\u0dca-\u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0db4\u0dd6\u0dbb\u0dca\u0dc0 \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba <a href=\"../feedforward.html\">FFN</a> \u0dc3\u0dc4 \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0da7 \u0db4\u0dd9\u0dbb \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2, \u0dc3\u0dc4 \u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba\u0dda \u0d9c\u0db8\u0db1\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba \u0dad\u0dad\u0dca\u0dc0\u0dba\u0da7 \u0db4\u0dad\u0dca \u0db1\u0ddc\u0dc0\u0dda. \u0dc3\u0db8\u0dca\u0db8\u0dad \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dd0\u0d9a\u0dc3\u0dd4\u0db8\u0dca\u0dc0\u0dbd\u0daf\u0dd3 \u0db8\u0dd9\u0dba \u0dc0\u0da9\u0dcf\u0dad\u0dca \u0dc3\u0dca\u0dae\u0dcf\u0dba\u0dd3 \u0dc0\u0dd2\u0dba \u0dba\u0dd4\u0dad\u0dd4\u0dba. </p>\n<p>\u0d9a\u0dd4\u0da9\u0dcf\u0dc2\u0dda\u0d9a\u0dca\u0dc3\u0dca\u0db4\u0dd2\u0dba\u0dbb\u0dca \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba\u0dda \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 <a href=\"experiment.html\">\u0d9a\u0dda\u0dad\u0dba</a> \u0dc3\u0dc4 \u0dc3\u0da7\u0dc4\u0db1\u0dca \u0db4\u0ddc\u0dad\u0d9a\u0dca \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a> <a href=\"https://app.labml.ai/run/0d9b5338726c11ebb7c80242ac1c0002\"> <span translate=no>_^_8_^_</span></a></p>\n",
 "<h2>1D Convolution Compression <span translate=no>_^_0_^_</span></h2>\n<p>This is a simple wrapper around <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a> with some tensor dimension permutations.</p>\n": "<h2>1D\u0dc3\u0db8\u0dca\u0db8\u0dd4\u0dad\u0dd2 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1\u0dba <span translate=no>_^_0_^_</span></h2>\n<p>\u0db8\u0dd9\u0dba\u0dc3\u0db8\u0dc4\u0dbb tensor \u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca permutations <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a> \u0dc3\u0db8\u0d9c \u0db4\u0db8\u0dab \u0dc3\u0dbb\u0dbd \u0daf\u0dc0\u0da7\u0db1\u0dba \u0dc0\u0dda. </p>\n",
 "<h2>Attention Reconstruction Loss</h2>\n<p>Attention reconstruction loss recreates the self-attention output with uncompressed memory and with compressed memory and calculates the mean squared error between the two. It does this without positional encoding.</p>\n<p>When calculating and training the compression function <span translate=no>_^_0_^_</span> with attention reconstruction loss, all parameters but <span translate=no>_^_1_^_</span> are frozen. This includes key/value projections and bias/scaling after normalization.</p>\n<p>Since this loss can be computed independently of the cross-entropy-loss of the model you can have a separate optimizer that only updates <span translate=no>_^_2_^_</span>. However, we use the same optimizer to update <span translate=no>_^_3_^_</span> so when calculating attention reconstruction loss, we detach all other parameters except <span translate=no>_^_4_^_</span> from the gradient computation.</p>\n": "<h2>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0d82\u0dc3\u0dca\u0d9a\u0dbb\u0dab \u0d85\u0dbd\u0dcf\u0db7\u0dba</h2>\n<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db1\u0ddc\u0dc0\u0db1 \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0dc3\u0dc4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dbb\u0db1 \u0d85\u0dad\u0dbb \u0db8\u0dda \u0daf\u0dd9\u0d9a \u0d85\u0dad\u0dbb \u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba \u0d9a\u0ddc\u0da7\u0dd4 \u0daf\u0ddd\u0dc2\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dd3\u0dba \u0d9a\u0dda\u0dad\u0dd3\u0d9a\u0dbb\u0dab\u0dba\u0d9a\u0dd2\u0db1\u0dca \u0dad\u0ddc\u0dbb\u0dc0 \u0db8\u0dd9\u0dba \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d85\u0dc4\u0dd2\u0db8\u0dd2 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1 \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba <span translate=no>_^_0_^_</span> \u0d9c\u0dab\u0db1\u0dba \u0dc4\u0dcf \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dbb\u0db1 \u0dc0\u0dd2\u0da7, \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 <span translate=no>_^_1_^_</span> \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0db1\u0db8\u0dd4\u0dad\u0dca \u0dc1\u0dd3\u0dad \u0d9a\u0dc5 \u0d87\u0dad. \u0db8\u0dd9\u0dba\u0da7 \u0dba\u0dad\u0dd4\u0dbb/\u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0dca \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab \u0dc3\u0dc4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0db4\u0d9a\u0dca\u0dc2\u0d9c\u0dca\u0dbb\u0dcf\u0dc4\u0dd3/\u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d87\u0dad\u0dd4\u0dc5\u0dad\u0dca \u0dc0\u0dda. </p>\n<p>\u0db8\u0dd9\u0db8\u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0dda \u0dc4\u0dbb\u0dc3\u0dca \u0d91\u0db1\u0dca\u0da7\u0dca\u0dbb\u0ddc\u0db4\u0dd2\u0dba-\u0d85\u0dbd\u0dcf\u0db7\u0dba\u0dd9\u0db1\u0dca \u0dc3\u0dca\u0dc0\u0dcf\u0db0\u0dd3\u0db1\u0dc0 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0d94\u0db6\u0da7 \u0dba\u0dcf\u0dc0\u0dad\u0dca\u0d9a\u0dcf\u0dbd\u0dd3\u0db1 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dc0\u0dd9\u0db1\u0db8 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2\u0dba <span translate=no>_^_2_^_</span>. \u0d9a\u0dd9\u0dc3\u0dda \u0dc0\u0dd9\u0dad\u0dad\u0dca, \u0d85\u0db4\u0dd2 \u0dba\u0dcf\u0dc0\u0dad\u0dca\u0d9a\u0dcf\u0dbd\u0dd3\u0db1 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d91\u0db8 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0d82\u0dc3\u0dca\u0d9a\u0dbb\u0dab\u0dba \u0d85\u0dc4\u0dd2\u0db8\u0dd2 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda\u0daf\u0dd3 <span translate=no>_^_3_^_</span> \u0d91\u0dc3\u0dda, \u0d85\u0db4\u0dd2 \u0db5\u0dbd\u0dba \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0d9c\u0dab\u0db1\u0dba <span translate=no>_^_4_^_</span> \u0dc3\u0dd2\u0da7 \u0dc4\u0dd0\u0dbb \u0d85\u0db1\u0dd9\u0d9a\u0dd4\u0dad\u0dca \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dca. </p>\n",
 "<h2>Compressive Transformer Layer</h2>\n<p>This is the implementation of a single compressive transformer layer</p>\n": "<h2>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba</h2>\n<p>\u0db8\u0dd9\u0dba\u0dad\u0db1\u0dd2 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dad\u0da7\u0dca\u0da7\u0dd4\u0dc0\u0d9a\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2</p>\n",
 "<h2>Compressive Transformer Model</h2>\n<p>This consists of multiple compressive transformer layers</p>\n": "<h2>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba</h2>\n<p>\u0db8\u0dd9\u0dba\u0db6\u0dc4\u0dd4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0dc3\u0db8\u0db1\u0dca\u0dc0\u0dd2\u0dad \u0dc0\u0dda</p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span></p>\n": "<p> <span translate=no>_^_0_^_</span> \u0dc4\u0dd0\u0da9\u0dba \u0d87\u0dad <span translate=no>_^_1_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is the list of Compressive Transformer layers</p>\n": "<p> <span translate=no>_^_0_^_</span> \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0dba\u0dd2</p>\n",
 "<p> Concatenate the normalized token embeddings with memory and compressed memory.</p>\n<ul><li><span translate=no>_^_0_^_</span> is layer normalized token embeddings. </li>\n<li><span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are memory and compressed memory (not normalized).</li></ul>\n": "<p> \u0db8\u0dad\u0d9a\u0dba\u0dc3\u0dc4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dc5 \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n<ul><li><span translate=no>_^_0_^_</span> \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dda. </li>\n<li><span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span> \u0dc3\u0dc4 \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0dc4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba (\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0db1\u0ddc\u0dc0\u0dda). </li></ul>\n",
 "<p> Perform layer normalization with shift and scale parameters detached.</p>\n": "<p> \u0dc0\u0dd9\u0db1\u0dca\u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0db8\u0dcf\u0dbb\u0dd4\u0dc0 \u0dc3\u0dc4 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n",
 "<p> This calculates the loss for a layer</p>\n": "<p> \u0db8\u0dd9\u0dba\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2</p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#MHA\">&#x27;Multi-Head Attention&#x27;</a> which calls <span translate=no>_^_0_^_</span> instead of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> to detach projection parameters.</p>\n": "<p> \u0db8\u0dd9\u0dba\u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf <a href=\"../mha.html#MHA\"><a href=\"../mha.html#PrepareMHA\">'\u0dc3\u0dd6\u0daf\u0dcf\u0db1\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dd6\u0dad\u0dca\u0dbb\u0dba\u0dc4\u0dd9\u0da9\u0dca\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba <span translate=no>_^_0_^_</span> \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d9a\u0dd0\u0db3\u0dc0\u0db1 '\u0db6\u0dc4\u0dd4-\u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba'</a> </a> \u0db1\u0dd0\u0dc0\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2. </p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> where the projections are done with the parameters detached from gradient computation.</p>\n<ul><li><span translate=no>_^_0_^_</span> is the <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> module </li>\n<li><span translate=no>_^_1_^_</span> is tensor with the token embeddings</li></ul>\n": "<p> \u0db8\u0dd9\u0dba\u0db1\u0dd0\u0dc0\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2 <a href=\"../mha.html#PrepareMHA\">'\u0dc3\u0dd6\u0daf\u0dcf\u0db1\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dd6\u0dad\u0dca\u0dbb\u0dba\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8'</a> \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0dda \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb \u0d87\u0dad\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda. </p>\n<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 <a href=\"../mha.html#PrepareMHA\">'\u0dc3\u0dd6\u0daf\u0dcf\u0db1\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dd6\u0dad\u0dca\u0dbb\u0dba\u0dc4\u0dd9\u0da9\u0dca\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8'</a> \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba </li>\n<li><span translate=no>_^_1_^_</span> \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc3\u0db8\u0d9f \u0d86\u0dad\u0dad\u0dd2\u0dba\u0dd9\u0db1\u0dca \u0dba\u0dd4\u0d9a\u0dca\u0dad \u0dc0\u0dda</li></ul>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0d94\u0dc3\u0dca\u0dc3\u0dda \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Add the attention results </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db5\u0dbd \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db4\u0ddd\u0dc2\u0dab\u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0db4\u0dca\u0dbb\u0dad\u0dd2 results \u0dbd \u0db1\u0dd0\u0dc0\u0dad \u0d91\u0d9a\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Add to the list of feature vectors </p>\n": "<p>\u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c\u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0da7 \u0d91\u0d9a\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Attention </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba </p>\n",
 "<p>Calculate query, key and value projections </p>\n": "<p>\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1, \u0dba\u0dad\u0dd4\u0dbb \u0dc3\u0dc4 \u0d85\u0d9c\u0dba \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab </p>\n",
 "<p>Calculate the attention with compressed memory </p>\n": "<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad\u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate the attention with uncompressed memory </p>\n": "<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad\u0db1\u0ddc\u0dc0\u0db1 \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate the losses for each layer </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dcf\u0da9\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate the mean square error </p>\n": "<p>\u0db8\u0db0\u0dca\u0dba\u0db1\u0dca\u0dba\u0dc0\u0dbb\u0dca\u0d9c \u0daf\u0ddd\u0dc2\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Compress the memory with <span translate=no>_^_0_^_</span>. The parameters of <span translate=no>_^_1_^_</span> are the only parameters not detached from gradient computation. </p>\n": "<p>\u0dc3\u0db8\u0d9f\u0db8\u0dad\u0d9a\u0dba \u0dc3\u0d82\u0d9a\u0ddd\u0da0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>. \u0dc4\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca <span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0dda \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0d91\u0d9a\u0db8 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc0\u0dda. </p>\n",
 "<p>Compressed Memory </p>\n": "<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad\u0db8\u0dad\u0d9a\u0dba </p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>. \u0db8\u0dd9\u0dba \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2 <span translate=no>_^_1_^_</span>. </p>\n",
 "<p>Concatenate normalized memory and normalized token embeddings </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba\u0d9a\u0dc5 \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0dc4 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dc5 \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Detach projection weights and bias </p>\n": "<p>\u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab\u0db6\u0dbb \u0dc3\u0dc4 \u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0 \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Detach shift(<span translate=no>_^_0_^_</span>) and scaling(<span translate=no>_^_1_^_</span>) parameters </p>\n": "<p>\u0dc0\u0dd9\u0db1\u0dca\u0db8\u0dcf\u0dbb\u0dd4\u0dc0 (<span translate=no>_^_0_^_</span>) \u0dc3\u0dc4 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba (<span translate=no>_^_1_^_</span>) \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca </p>\n",
 "<p>Detach the token embeddings and memory. </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1\u0dca\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc3\u0dc4 \u0db8\u0dad\u0d9a\u0dba \u0dc0\u0dd9\u0db1\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab \u0dc3\u0dca\u0dad\u0dbb\u0dba </p>\n",
 "<p>Finally, normalize the vectors </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1\u0dc0\u0dc1\u0dba\u0dd9\u0db1\u0dca, \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get compressed memory by running it through the convolution layer </p>\n": "<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad\u0db8\u0dad\u0d9a\u0dba \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd4\u0dab\u0dd4 \u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>If there are compressed memory concatenate that with memory </p>\n": "<p>\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad\u0db8\u0dad\u0d9a\u0dba \u0dad\u0dd2\u0db6\u0dda \u0db1\u0db8\u0dca \u0d91\u0dba \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0db8\u0d9f \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0dc0\u0dda </p>\n",
 "<p>If there is no memory just return the token embeddings </p>\n": "<p>\u0db8\u0dad\u0d9a\u0dba\u0d9a\u0dca\u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0db1\u0dd0\u0dc0\u0dad \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1 </p>\n",
 "<p>Layer normalization </p>\n": "<p>\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba </p>\n",
 "<p>Linear transform </p>\n": "<p>\u0dbb\u0dda\u0d9b\u0dd3\u0dba\u0db4\u0dbb\u0dd2\u0dab\u0dcf\u0db8\u0db1\u0dba </p>\n",
 "<p>List to store token level feature vectors, which will become the memories for the next sequential batch. </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1\u0dca\u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0d9c\u0db6\u0da9\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4 \u0d9c\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1, \u0d91\u0dba \u0d8a\u0dc5\u0d9f \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0db6\u0dc0\u0da7 \u0db4\u0dad\u0dca\u0dc0\u0db1\u0dd4 \u0d87\u0dad. </p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0dd2\u0da7\u0db4\u0dad\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Memory </p>\n": "<p>\u0db8\u0dad\u0d9a\u0dba </p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0d9c\u0dba\u0db1\u0dca\u0d85\u0db1\u0dd4\u0dc0 \u0d9c\u0dd4\u0dab \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Normalize and concatenate memory and compressed memory </p>\n": "<p>\u0db8\u0dad\u0d9a\u0dba\u0dc3\u0dc4 \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u0db4\u0ddd\u0dc2\u0dab\u0dba\u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Normalize the embeddings and memories </p>\n": "<p>\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca\u0dc3\u0dc4 \u0db8\u0dad\u0d9a\u0dba\u0db1\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Output has shape <span translate=no>_^_0_^_</span> or <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0db1\u0dd2\u0db8\u0dd0\u0dc0\u0dd4\u0db8\u0dda\u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_0_^_</span> \u0dc4\u0ddd <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>Feed-forward\u0da2\u0dcf\u0dbd\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d9c\u0db8\u0db1\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Permute back to form <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db1\u0dd0\u0dc0\u0dad\u0db4\u0dd2\u0dc4\u0dd2\u0da7\u0dd4\u0dc0\u0dd3\u0db8\u0da7 \u0d85\u0dc0\u0dc3\u0dbb \u0daf\u0dd9\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Permute the dimensions of <span translate=no>_^_0_^_</span> so that we can run it through the convolution layer. The convolution layer accepts in the form <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0dc3\u0d82\u0dc0\u0dc4\u0db1\u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d91\u0dba \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2 \u0dc0\u0db1 <span translate=no>_^_0_^_</span> \u0db4\u0dbb\u0dd2\u0daf\u0dd2 \u0db8\u0dcf\u0db1\u0dba\u0db1\u0dca \u0db4\u0dbb\u0dd2\u0db4\u0dd6\u0dbb\u0dca\u0dab \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd4\u0dab\u0dd4 \u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc3\u0dca\u0dc0\u0dbb\u0dd6\u0db4\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dd2\u0dc5\u0dd2\u0d9c\u0db1\u0dd3 <span translate=no>_^_1_^_</span> </p>\n",
 "<p>Run the memory through the normalization layer </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0db8\u0dad\u0d9a\u0dba \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Run through each transformer layer </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Run through the transformer XL layer </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dcaXL \u0dc3\u0dca\u0dad\u0dbb\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Shape of the input except embedding dimension; <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0dc4\u0dd0\u0dbb \u0d86\u0daf\u0dcf\u0db1 \u0dc4\u0dd0\u0da9\u0dba; <span translate=no>_^_0_^_</span>. </p>\n",
 "<p>Split last dimension into heads </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1\u0db8\u0dcf\u0db1\u0dba \u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dc0\u0da7 \u0db6\u0dd9\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Sum of the losses </p>\n": "<p>\u0d85\u0dbd\u0dcf\u0db7\u0dc0\u0dbd\u0d91\u0d9a\u0dad\u0dd4\u0dc0 </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the embedding size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc0\u0dda</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token embeddings vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a list of tensors of the past token level feature vectors of shape  <span translate=no>_^_3_^_</span> for each layer </li>\n<li><span translate=no>_^_4_^_</span> is a list of tensors of the compressed memory  <span translate=no>_^_5_^_</span> for each layer </li>\n<li><span translate=no>_^_6_^_</span> is the masking matrix</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dc0\u0dbd \u0d86\u0dad\u0d9a\u0dba\u0d9a\u0dd2 <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc3\u0dca\u0dad\u0dbb\u0dba <span translate=no>_^_3_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0d85\u0dad\u0dd3\u0dad \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0d86\u0dad\u0dad\u0dd2 \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0d9a\u0dd2 </li>\n<li><span translate=no>_^_4_^_</span> \u0dba\u0db1\u0dd4 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba <span translate=no>_^_5_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2 \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0d9a\u0dd2 </li>\n<li><span translate=no>_^_6_^_</span> \u0d86\u0dc0\u0dbb\u0dab \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc0\u0dda</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of token level feature vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a tensor of the past token level feature vectors (memory) of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is a tensor of the compressed memory <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is a matrix of shape <span translate=no>_^_7_^_</span> or <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> is true if token at <span translate=no>_^_10_^_</span> can see token at <span translate=no>_^_11_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a\u0dc0\u0dbd \u0d86\u0dad\u0d9a\u0dba\u0d9a\u0dd2 <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0d85\u0dad\u0dd3\u0dad \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0daf\u0ddb\u0dc1\u0dd2\u0d9a (\u0db8\u0dad\u0d9a\u0dba) \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dba\u0d9a\u0dd2 <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dd2\u0dad \u0db8\u0dad\u0d9a\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0dc0\u0dda <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> \u0dba\u0db1\u0dd4 \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca <span translate=no>_^_7_^_</span> \u0dc4\u0ddd <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0dc0\u0dbd\u0da7 \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0daf\u0dd0\u0d9a\u0dd2\u0dba <span translate=no>_^_10_^_</span> \u0dc4\u0dd0\u0d9a\u0dd2 \u0db1\u0db8\u0dca <span translate=no>_^_11_^_</span>\u0dc3\u0dad\u0dca\u0dba\u0dba\u0d9a\u0dd2. </li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the <a href=\"../xl/relative_mha.html\">self attention module</a> </li>\n<li><span translate=no>_^_2_^_</span> is the <a href=\"../feed_forward.html\">feed forward module</a> </li>\n<li><span translate=no>_^_3_^_</span> is the probability of dropping out after self attention and FFN </li>\n<li><span translate=no>_^_4_^_</span> is the compression function <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2 </li>\n<li><span translate=no>_^_1_^_</span> <a href=\"../xl/relative_mha.html\">\u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba</a> </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 <a href=\"../feed_forward.html\">\u0d86\u0dc4\u0dcf\u0dbb \u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba\u0dba\u0dd2</a> </li>\n<li><span translate=no>_^_3_^_</span> \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0d89\u0dc0\u0dad\u0dca \u0dc0\u0dd3\u0db8\u0dda \u0dc3\u0db8\u0dca\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf\u0dc0 \u0dc3\u0dc4 FFN </li>\n<li><span translate=no>_^_4_^_</span> \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0db1 \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba <span translate=no>_^_5_^_</span></li></ul>\n",
 "Compressive Transformer": "\u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca",
 "Documented implementation with explanations of a Compressive Transformer model.": "\u0d91\u0dba, \u0dc3\u0db8\u0dca\u0db4\u0dd3\u0da9\u0dca\u0dba\u0dad\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0db4\u0dd0\u0dc4\u0dd0\u0daf\u0dd2\u0dbd\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0dc3\u0db8\u0d9c \u0dbd\u0dda\u0d9b\u0db1\u0d9c\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8."
}