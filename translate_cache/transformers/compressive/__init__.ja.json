{
 "<h1>Compressive Transformer</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1911.05507\">Compressive Transformers for Long-Range Sequence Modelling</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>This is an extension of <a href=\"../xl/index.html\">Transformer XL</a> where past memories are compressed to give a longer attention range. That is, the furthest <span translate=no>_^_0_^_</span> memories are compressed into <span translate=no>_^_1_^_</span> memories, where <span translate=no>_^_2_^_</span> is the compression rate.</p>\n<h2>Compression operation</h2>\n<p>The compression operation is defined as <span translate=no>_^_3_^_</span>. The paper introduces multiple choices for <span translate=no>_^_4_^_</span> and we have only implemented 1D convolution which seems to give the best results. Each layer has a separate compression operation <span translate=no>_^_5_^_</span> where <span translate=no>_^_6_^_</span> is the layer number.</p>\n<h2>Training compression operation</h2>\n<p>Since training compression with BPTT requires maintaining a very large computational graph (many time steps), the paper proposes an <em>auto-encoding loss</em> and an <em>attention reconstruction loss</em>. The auto-encoding loss decodes the original memories from the compressed memories and calculates the loss. Attention reconstruction loss computes the multi-headed attention results on the compressed memory and on uncompressed memory and gets a mean squared error between them. We have implemented the latter here since it gives better results.</p>\n<p>This implementation uses pre-layer normalization while the paper uses post-layer normalization. Pre-layer norm does the layer norm before <a href=\"../feedforward.html\">FFN</a> and self-attention, and the pass-through in the residual connection is not normalized. This is supposed to be more stable in standard transformer setups.</p>\n<p>Here are <a href=\"experiment.html\">the training code</a> and a notebook for training a compressive transformer model on the Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a></p>\n": "<h1>\u5727\u7e2e\u5909\u5727\u5668</h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f PyTorch <a href=\"https://papers.labml.ai/paper/1911.05507\">\u306e\u9577\u8ddd\u96e2\u30b7\u30fc\u30b1\u30f3\u30b9\u30e2\u30c7\u30ea\u30f3\u30b0\u7528\u306e\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</a></p>\n<p><a href=\"../xl/index.html\">\u3053\u308c\u306fTransformer XL\u306e\u62e1\u5f35\u7248\u3067</a>\u3001\u904e\u53bb\u306e\u8a18\u61b6\u3092\u5727\u7e2e\u3057\u3066\u6ce8\u610f\u7bc4\u56f2\u3092\u5e83\u3052\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u6700\u3082\u9060\u3044\u30e1\u30e2\u30ea\u304c\u30e1\u30e2\u30ea\u306b\u5727\u7e2e\u3055\u308c\u307e\u3059\u3002\u3053\u3053\u3067\u3001<span translate=no>_^_2_^_</span>\u306f\u5727\u7e2e\u7387\u3067\u3059</p>\u3002\n<h2>\u5727\u7e2e\u64cd\u4f5c</h2>\n<p>\u5727\u7e2e\u64cd\u4f5c\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u307e\u3059<span translate=no>_^_3_^_</span>\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u8907\u6570\u306e\u9078\u629e\u80a2\u3092\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3059\u304c<span translate=no>_^_4_^_</span>\u3001\u6700\u826f\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u3068\u601d\u308f\u308c\u308b1\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u306e\u307f\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002\u5404\u30ec\u30a4\u30e4\u30fc\u306b\u306f\u500b\u5225\u306e\u5727\u7e2e\u64cd\u4f5c\u304c\u3042\u308a\u307e\u3059\u3002<span translate=no>_^_5_^_</span>\u3053\u3053\u3067<span translate=no>_^_6_^_</span>\u3001\u306f\u30ec\u30a4\u30e4\u30fc\u756a\u53f7\u3067\u3059\u3002</p>\n<h2>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u7528\u5727\u7e2e\u64cd\u4f5c</h2>\n<p><em>BPTT\u306b\u3088\u308b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u5727\u7e2e\u3067\u306f\u3001\u975e\u5e38\u306b\u5927\u304d\u306a\u8a08\u7b97\u30b0\u30e9\u30d5\uff08\u591a\u304f\u306e\u30bf\u30a4\u30e0\u30b9\u30c6\u30c3\u30d7\uff09\u3092\u7dad\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001<em>\u3053\u306e\u8ad6\u6587\u3067\u306f\u81ea\u52d5\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u640d\u5931\u3068\u6ce8\u610f\u518d\u69cb\u6210\u640d\u5931\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059</em>\u3002</em>\u81ea\u52d5\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u640d\u5931\u306f\u3001\u5727\u7e2e\u3055\u308c\u305f\u30e1\u30e2\u30ea\u304b\u3089\u5143\u306e\u30e1\u30e2\u30ea\u3092\u30c7\u30b3\u30fc\u30c9\u3057\u3001\u640d\u5931\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u518d\u69cb\u6210\u640d\u5931\u3067\u306f\u3001\u5727\u7e2e\u30e1\u30e2\u30ea\u3068\u975e\u5727\u7e2e\u30e1\u30e2\u30ea\u3067\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u7d50\u679c\u3092\u8a08\u7b97\u3057\u3001\u305d\u308c\u3089\u306e\u9593\u306e\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002\u5f8c\u8005\u306e\u65b9\u304c\u826f\u3044\u7d50\u679c\u304c\u5f97\u3089\u308c\u308b\u305f\u3081\u3001\u3053\u3053\u3067\u306f\u5f8c\u8005\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002</p>\n<p>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u30ec\u30a4\u30e4\u30fc\u524d\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u307e\u3059\u304c\u3001\u30da\u30fc\u30d1\u30fc\u3067\u306f\u30ec\u30a4\u30e4\u30fc\u5f8c\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002<a href=\"../feedforward.html\">\u524d\u5c64\u30ce\u30eb\u30e0\u306fFFN\u3084\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u524d\u306e\u5c64\u30ce\u30eb\u30e0\u3092\u884c\u3044</a>\u3001\u6b8b\u5dee\u63a5\u7d9a\u3067\u306e\u30d1\u30b9\u30b9\u30eb\u30fc\u306f\u6b63\u898f\u5316\u3055\u308c\u307e\u305b\u3093\u3002\u3053\u308c\u306f\u6a19\u6e96\u7684\u306a\u5909\u5727\u5668\u306e\u8a2d\u5b9a\u3067\u306f\u3088\u308a\u5b89\u5b9a\u3057\u3066\u3044\u308b\u306f\u305a\u3067\u3059</p>\u3002\n<p>Tiny <a href=\"experiment.html\">Shakespeare\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3068\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a></p>\n",
 "<h2>1D Convolution Compression <span translate=no>_^_0_^_</span></h2>\n<p>This is a simple wrapper around <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a> with some tensor dimension permutations.</p>\n": "<h2>1D \u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u5727\u7e2e <span translate=no>_^_0_^_</span></h2>\n<p>\u3053\u308c\u306f\u3001<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a>\u30c6\u30f3\u30bd\u30eb\u6b21\u5143\u306e\u9806\u5217\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u5358\u7d14\u306a\u30e9\u30c3\u30d1\u30fc\u3067\u3059\u3002</p>\n",
 "<h2>Attention Reconstruction Loss</h2>\n<p>Attention reconstruction loss recreates the self-attention output with uncompressed memory and with compressed memory and calculates the mean squared error between the two. It does this without positional encoding.</p>\n<p>When calculating and training the compression function <span translate=no>_^_0_^_</span> with attention reconstruction loss, all parameters but <span translate=no>_^_1_^_</span> are frozen. This includes key/value projections and bias/scaling after normalization.</p>\n<p>Since this loss can be computed independently of the cross-entropy-loss of the model you can have a separate optimizer that only updates <span translate=no>_^_2_^_</span>. However, we use the same optimizer to update <span translate=no>_^_3_^_</span> so when calculating attention reconstruction loss, we detach all other parameters except <span translate=no>_^_4_^_</span> from the gradient computation.</p>\n": "<h2>\u6ce8\u610f\u529b\u518d\u5efa\u30ed\u30b9</h2>\n<p>\u6ce8\u610f\u518d\u69cb\u6210\u640d\u5931\u306f\u3001\u975e\u5727\u7e2e\u30e1\u30e2\u30ea\u3068\u5727\u7e2e\u30e1\u30e2\u30ea\u3067\u81ea\u5df1\u6ce8\u610f\u51fa\u529b\u3092\u518d\u73fe\u3057\u3001\u4e21\u8005\u306e\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306a\u3057\u3067\u884c\u3044\u307e\u3059</p>\u3002\n<p><span translate=no>_^_0_^_</span>\u6ce8\u610f\u518d\u69cb\u6210\u640d\u5931\u3092\u4f34\u3046\u5727\u7e2e\u95a2\u6570\u306e\u8a08\u7b97\u3068\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u884c\u3046\u3068\u3001<span translate=no>_^_1_^_</span>\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u30d5\u30ea\u30fc\u30ba\u3057\u307e\u3059\u3002\u3053\u308c\u306b\u306f\u3001\u6b63\u898f\u5316\u5f8c\u306e\u30ad\u30fc/\u5024\u306e\u4e88\u6e2c\u3068\u30d0\u30a4\u30a2\u30b9/\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u542b\u307e\u308c\u307e\u3059</p>\u3002\n<p>\u3053\u306e\u640d\u5931\u306f\u30e2\u30c7\u30eb\u306e\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3068\u306f\u72ec\u7acb\u3057\u3066\u8a08\u7b97\u3067\u304d\u308b\u305f\u3081\u3001\u66f4\u65b0\u306e\u307f\u3092\u884c\u3046\u5225\u306e\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002<span translate=no>_^_2_^_</span>\u305f\u3060\u3057\u3001<span translate=no>_^_3_^_</span>\u66f4\u65b0\u306b\u306f\u540c\u3058\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u3001\u6ce8\u610f\u518d\u69cb\u6210\u640d\u5931\u3092\u8a08\u7b97\u3059\u308b\u3068\u304d\u306f\u3001<span translate=no>_^_4_^_</span>\u52fe\u914d\u8a08\u7b97\u3092\u9664\u304f\u4ed6\u306e\u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u5207\u308a\u96e2\u3057\u307e\u3059</p>\u3002\n",
 "<h2>Compressive Transformer Layer</h2>\n<p>This is the implementation of a single compressive transformer layer</p>\n": "<h2>\u5727\u7e2e\u5909\u5727\u5668\u5c64</h2>\n<p>\u3053\u308c\u306f\u5358\u4e00\u306e\u5727\u7e2e\u5909\u5727\u5668\u5c64\u306e\u5b9f\u88c5\u3067\u3059</p>\n",
 "<h2>Compressive Transformer Model</h2>\n<p>This consists of multiple compressive transformer layers</p>\n": "<h2>\u5727\u7e2e\u5909\u5727\u5668\u30e2\u30c7\u30eb</h2>\n<p>\u3053\u308c\u306f\u8907\u6570\u306e\u5727\u7e2e\u5909\u5727\u5668\u5c64\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u5f62\u304c\u3042\u308b <span translate=no>_^_1_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is the list of Compressive Transformer layers</p>\n": "<p><span translate=no>_^_0_^_</span>\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30ea\u30b9\u30c8\u3067\u3059</p>\n",
 "<p> Concatenate the normalized token embeddings with memory and compressed memory.</p>\n<ul><li><span translate=no>_^_0_^_</span> is layer normalized token embeddings. </li>\n<li><span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are memory and compressed memory (not normalized).</li></ul>\n": "<p>\u6b63\u898f\u5316\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u30e1\u30e2\u30ea\u3068\u5727\u7e2e\u30e1\u30e2\u30ea\u3068\u9023\u7d50\u3057\u307e\u3059\u3002</p>\n<ul><li><span translate=no>_^_0_^_</span>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3067\u3059\u3002</li>\n</ul><li><span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u30e1\u30e2\u30ea\u3068\u5727\u7e2e\u30e1\u30e2\u30ea (\u6b63\u898f\u5316\u3055\u308c\u3066\u3044\u306a\u3044) \u3067\u3059\u3002</li>\n",
 "<p> Perform layer normalization with shift and scale parameters detached.</p>\n": "<p>\u30b7\u30d5\u30c8\u3068\u30b9\u30b1\u30fc\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u30c7\u30bf\u30c3\u30c1\u3057\u3066\u30ec\u30a4\u30e4\u30fc\u306e\u6b63\u898f\u5316\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p>\n",
 "<p> This calculates the loss for a layer</p>\n": "<p>\u3053\u308c\u306b\u3088\u308a\u3001\u30ec\u30a4\u30e4\u30fc\u306e\u640d\u5931\u304c\u8a08\u7b97\u3055\u308c\u307e\u3059</p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#MHA\">&#x27;Multi-Head Attention&#x27;</a> which calls <span translate=no>_^_0_^_</span> instead of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> to detach projection parameters.</p>\n": "<p>\u3053\u308c\u306f <a href=\"../mha.html#MHA\">\u300c\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u300d\u3092\u518d\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3001\u300c<a href=\"../mha.html#PrepareMHA\">PrepareForMultiHead Attention\u300d<span translate=no>_^_0_^_</span></a></a> \u306e\u4ee3\u308f\u308a\u306b\u547c\u3073\u51fa\u3057\u3066\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30c7\u30bf\u30c3\u30c1\u3057\u307e\u3059\u3002</p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> where the projections are done with the parameters detached from gradient computation.</p>\n<ul><li><span translate=no>_^_0_^_</span> is the <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> module </li>\n<li><span translate=no>_^_1_^_</span> is tensor with the token embeddings</li></ul>\n": "<p>\u3053\u308c\u306f <a href=\"../mha.html#PrepareMHA\">\u300cPrepareForMultiHeadAttention\u300d\u3092\u518d\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3001\u52fe\u914d\u8a08\u7b97\u304b\u3089\u5207\u308a\u96e2\u3055\u308c\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3092\u4f7f\u7528\u3057\u3066\u6295\u5f71\u304c\u884c\u308f\u308c\u307e\u3059</a>\u3002</p>\n<ul><li><span translate=no>_^_0_^_</span>\u306f <a href=\"../mha.html#PrepareMHA\">\u300c\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5bfe\u7b56\u300d\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</a></li>\n<li><span translate=no>_^_1_^_</span>\u30c8\u30fc\u30af\u30f3\u304c\u57cb\u3081\u8fbc\u307e\u308c\u305f\u30c6\u30f3\u30bd\u30eb\u3067\u3059</li></ul>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u30ad\u30fc\u30b7\u30fc\u30b1\u30f3\u30b9\u6b21\u5143\u306b\u6cbf\u3063\u3066\u6ce8\u76ee <span translate=no>_^_1_^_</span></p>\n",
 "<p>Add the attention results </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7d50\u679c\u3092\u8ffd\u52a0</p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u7d50\u679c\u3092\u8ffd\u52a0\u3057\u76f4\u3059</p>\n",
 "<p>Add to the list of feature vectors </p>\n": "<p>\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0</p>\n",
 "<p>Attention </p>\n": "<p>\u6ce8\u610f</p>\n",
 "<p>Calculate query, key and value projections </p>\n": "<p>\u30af\u30a8\u30ea\u3001\u30ad\u30fc\u3001\u5024\u306e\u4e88\u6e2c\u3092\u8a08\u7b97</p>\n",
 "<p>Calculate the attention with compressed memory </p>\n": "<p>\u5727\u7e2e\u30e1\u30e2\u30ea\u3067\u6ce8\u610f\u5ea6\u3092\u8a08\u7b97</p>\n",
 "<p>Calculate the attention with uncompressed memory </p>\n": "<p>\u975e\u5727\u7e2e\u30e1\u30e2\u30ea\u3067\u6ce8\u610f\u5ea6\u3092\u8a08\u7b97</p>\n",
 "<p>Calculate the losses for each layer </p>\n": "<p>\u5404\u5c64\u306e\u640d\u5931\u306e\u8a08\u7b97</p>\n",
 "<p>Calculate the mean square error </p>\n": "<p>\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\u306e\u8a08\u7b97</p>\n",
 "<p>Compress the memory with <span translate=no>_^_0_^_</span>. The parameters of <span translate=no>_^_1_^_</span> are the only parameters not detached from gradient computation. </p>\n": "<p><span translate=no>_^_0_^_</span>\u3067\u30e1\u30e2\u30ea\u3092\u5727\u7e2e\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span>\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u3001\u52fe\u914d\u8a08\u7b97\u304b\u3089\u5207\u308a\u96e2\u3055\u308c\u306a\u3044\u552f\u4e00\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u3059</p>\u3002\n",
 "<p>Compressed Memory </p>\n": "<p>\u5727\u7e2e\u30e1\u30e2\u30ea</p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p><span translate=no>_^_0_^_</span>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span>\u3053\u308c\u306b\u3088\u308a\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u304c\u5f97\u3089\u308c\u307e\u3059</p>\u3002\n",
 "<p>Concatenate normalized memory and normalized token embeddings </p>\n": "<p>\u6b63\u898f\u5316\u3055\u308c\u305f\u30e1\u30e2\u30ea\u3068\u6b63\u898f\u5316\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u9023\u7d50\u3059\u308b</p>\n",
 "<p>Detach projection weights and bias </p>\n": "<p>\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u30a6\u30a7\u30a4\u30c8\u3068\u30d0\u30a4\u30a2\u30b9\u3092\u30c7\u30bf\u30c3\u30c1</p>\n",
 "<p>Detach shift(<span translate=no>_^_0_^_</span>) and scaling(<span translate=no>_^_1_^_</span>) parameters </p>\n": "<p>shift (<span translate=no>_^_0_^_</span>) \u3068\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0 (<span translate=no>_^_1_^_</span>) \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u30c7\u30bf\u30c3\u30c1</p>\n",
 "<p>Detach the token embeddings and memory. </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3068\u30e1\u30e2\u30ea\u3092\u5207\u308a\u96e2\u3057\u307e\u3059\u3002</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7d42\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Finally, normalize the vectors </p>\n": "<p>\u6700\u5f8c\u306b\u3001\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u898f\u5316\u3057\u307e\u3059\u3002</p>\n",
 "<p>Get compressed memory by running it through the convolution layer </p>\n": "<p>\u7573\u307f\u8fbc\u307f\u5c64\u306b\u901a\u3057\u3066\u5727\u7e2e\u30e1\u30e2\u30ea\u3092\u53d6\u5f97</p>\n",
 "<p>If there are compressed memory concatenate that with memory </p>\n": "<p>\u5727\u7e2e\u30e1\u30e2\u30ea\u304c\u3042\u308b\u5834\u5408\u306f\u3001\u305d\u308c\u3092\u30e1\u30e2\u30ea\u3068\u9023\u7d50\u3057\u307e\u3059\u3002</p>\n",
 "<p>If there is no memory just return the token embeddings </p>\n": "<p>\u30e1\u30e2\u30ea\u304c\u306a\u3044\u5834\u5408\u306f\u3001\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Layer normalization </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</p>\n",
 "<p>Linear transform </p>\n": "<p>\u7dda\u5f62\u5909\u63db</p>\n",
 "<p>List to store token level feature vectors, which will become the memories for the next sequential batch. </p>\n": "<p>\u6b21\u306e\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d0\u30c3\u30c1\u306e\u30e1\u30e2\u30ea\u3068\u306a\u308b\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u683c\u7d0d\u3059\u308b\u30ea\u30b9\u30c8\u3002</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210</p>\n",
 "<p>Memory </p>\n": "<p>\u30e1\u30e2\u30ea\u30fc</p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5024\u306b\u3088\u308b\u4e57\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalize and concatenate memory and compressed memory </p>\n": "<p>\u30e1\u30e2\u30ea\u3068\u5727\u7e2e\u30e1\u30e2\u30ea\u306e\u6b63\u898f\u5316\u3068\u9023\u7d50</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u7528\u306b\u6b63\u898f\u5316</p>\n",
 "<p>Normalize the embeddings and memories </p>\n": "<p>\u57cb\u3081\u8fbc\u307f\u3068\u30e1\u30e2\u30ea\u3092\u6b63\u898f\u5316</p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u884c\u3046\u524d\u306b\u30d9\u30af\u30c8\u30eb\u3092\u6b63\u898f\u5316\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Output has shape <span translate=no>_^_0_^_</span> or <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u51fa\u529b\u306e\u5f62\u72b6\u304c\u3042\u308b\u304b <span translate=no>_^_1_^_</span></p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u904e</p>\n",
 "<p>Permute back to form <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30d5\u30a9\u30fc\u30e0\u306b\u623b\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Permute the dimensions of <span translate=no>_^_0_^_</span> so that we can run it through the convolution layer. The convolution layer accepts in the form <span translate=no>_^_1_^_</span> </p>\n": "<p>\u306e\u6b21\u5143\u3092\u4e26\u3079\u66ff\u3048\u3066\u3001<span translate=no>_^_0_^_</span>\u7573\u307f\u8fbc\u307f\u5c64\u306b\u901a\u305b\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002\u7573\u307f\u8fbc\u307f\u5c64\u306f\u6b21\u306e\u5f62\u5f0f\u3092\u53d7\u3051\u5165\u308c\u307e\u3059 <span translate=no>_^_1_^_</span></p>\n",
 "<p>Run the memory through the normalization layer </p>\n": "<p>\u30e1\u30e2\u30ea\u3092\u6b63\u898f\u5316\u5c64\u306b\u901a\u3059</p>\n",
 "<p>Run through each transformer layer </p>\n": "<p>\u5404\u5909\u5727\u5668\u5c64\u306b\u901a\u3059</p>\n",
 "<p>Run through the transformer XL layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fcXL\u30ec\u30a4\u30e4\u30fc\u3092\u901a\u3059</p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b9\u30b1\u30fc\u30eb\u30b9\u30b3\u30a2 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Shape of the input except embedding dimension; <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u57cb\u3081\u8fbc\u307f\u5bf8\u6cd5\u4ee5\u5916\u306e\u5165\u529b\u306e\u5f62\u72b6;<span translate=no>_^_0_^_</span>.</p>\n",
 "<p>Split last dimension into heads </p>\n": "<p>\u6700\u5f8c\u306e\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3\u3092\u30d8\u30c3\u30c9\u306b\u5206\u5272</p>\n",
 "<p>Sum of the losses </p>\n": "<p>\u640d\u5931\u306e\u5408\u8a08</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the embedding size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token embeddings vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a list of tensors of the past token level feature vectors of shape  <span translate=no>_^_3_^_</span> for each layer </li>\n<li><span translate=no>_^_4_^_</span> is a list of tensors of the compressed memory  <span translate=no>_^_5_^_</span> for each layer </li>\n<li><span translate=no>_^_6_^_</span> is the masking matrix</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u904e\u53bb\u306e\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3001<span translate=no>_^_3_^_</span>\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u5f62\u72b6\u30d9\u30af\u30c8\u30eb\u306e\u30ea\u30b9\u30c8\u3067\u3059</li>\n<li><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u5727\u7e2e\u30e1\u30e2\u30ea\u306e\u30c6\u30f3\u30bd\u30eb\u306e\u30ea\u30b9\u30c8\u3067\u3059</li>\n<li><span translate=no>_^_6_^_</span>\u306f\u30de\u30b9\u30ad\u30f3\u30b0\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of token level feature vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a tensor of the past token level feature vectors (memory) of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is a tensor of the compressed memory <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is a matrix of shape <span translate=no>_^_7_^_</span> or <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> is true if token at <span translate=no>_^_10_^_</span> can see token at <span translate=no>_^_11_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u306e\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u904e\u53bb\u306e\u30c8\u30fc\u30af\u30f3\u30ec\u30d9\u30eb\u306e\u5f62\u72b6\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb (\u30e1\u30e2\u30ea) \u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u5727\u7e2e\u30e1\u30e2\u30ea\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_5_^_</span></li>\n<li><span translate=no>_^_6_^_</span><span translate=no>_^_7_^_</span><span translate=no>_^_8_^_</span>\u306f\u5f62\u72b6\u306e\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u304b<span translate=no>_^_9_^_</span>\u30c8\u30fc\u30af\u30f3 at \u304c at <span translate=no>_^_10_^_</span> \u306e\u30c8\u30fc\u30af\u30f3\u3092\u53c2\u7167\u3067\u304d\u308b\u5834\u5408\u306f true <span translate=no>_^_11_^_</span> \u306b\u306a\u308a\u307e\u3059\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the <a href=\"../xl/relative_mha.html\">self attention module</a> </li>\n<li><span translate=no>_^_2_^_</span> is the <a href=\"../feed_forward.html\">feed forward module</a> </li>\n<li><span translate=no>_^_3_^_</span> is the probability of dropping out after self attention and FFN </li>\n<li><span translate=no>_^_4_^_</span> is the compression function <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span><a href=\"../xl/relative_mha.html\">\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</a></li>\n<li><span translate=no>_^_2_^_</span><a href=\"../feed_forward.html\">\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059</a></li>\n<li><span translate=no>_^_3_^_</span>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3068FFN\u306e\u5f8c\u306b\u8131\u843d\u3059\u308b\u78ba\u7387\u3067\u3059</li>\n<li><span translate=no>_^_4_^_</span>\u306f\u5727\u7e2e\u95a2\u6570\u3067\u3059 <span translate=no>_^_5_^_</span></li></ul>\n",
 "Compressive Transformer": "\u5727\u7e2e\u5909\u5727\u5668",
 "Documented implementation with explanations of a Compressive Transformer model.": "\u5727\u7e2e\u30c8\u30e9\u30f3\u30b9\u30e2\u30c7\u30eb\u306e\u8aac\u660e\u3092\u542b\u3080\u5b9f\u88c5\u304c\u6587\u66f8\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002"
}