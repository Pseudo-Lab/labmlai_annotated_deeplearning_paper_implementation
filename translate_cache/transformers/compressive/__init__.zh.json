{
 "<h1>Compressive Transformer</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1911.05507\">Compressive Transformers for Long-Range Sequence Modelling</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>This is an extension of <a href=\"../xl/index.html\">Transformer XL</a> where past memories are compressed to give a longer attention range. That is, the furthest <span translate=no>_^_0_^_</span> memories are compressed into <span translate=no>_^_1_^_</span> memories, where <span translate=no>_^_2_^_</span> is the compression rate.</p>\n<h2>Compression operation</h2>\n<p>The compression operation is defined as <span translate=no>_^_3_^_</span>. The paper introduces multiple choices for <span translate=no>_^_4_^_</span> and we have only implemented 1D convolution which seems to give the best results. Each layer has a separate compression operation <span translate=no>_^_5_^_</span> where <span translate=no>_^_6_^_</span> is the layer number.</p>\n<h2>Training compression operation</h2>\n<p>Since training compression with BPTT requires maintaining a very large computational graph (many time steps), the paper proposes an <em>auto-encoding loss</em> and an <em>attention reconstruction loss</em>. The auto-encoding loss decodes the original memories from the compressed memories and calculates the loss. Attention reconstruction loss computes the multi-headed attention results on the compressed memory and on uncompressed memory and gets a mean squared error between them. We have implemented the latter here since it gives better results.</p>\n<p>This implementation uses pre-layer normalization while the paper uses post-layer normalization. Pre-layer norm does the layer norm before <a href=\"../feedforward.html\">FFN</a> and self-attention, and the pass-through in the residual connection is not normalized. This is supposed to be more stable in standard transformer setups.</p>\n<p>Here are <a href=\"experiment.html\">the training code</a> and a notebook for training a compressive transformer model on the Tiny Shakespeare dataset.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a></p>\n": "<h1>\u538b\u7f29\u53d8\u538b\u5668</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u4e2d<a href=\"https://papers.labml.ai/paper/1911.05507\">\u7528\u4e8e\u8fdc\u7a0b\u5e8f\u5217\u5efa\u6a21\u7684\u538b\u7f29\u8f6c\u6362\u5668\u7684</a>\u5b9e\u73b0\u3002</p>\n<p>\u8fd9\u662f Transfor <a href=\"../xl/index.html\">mer XL</a> \u7684\u6269\u5c55\uff0c\u5b83\u538b\u7f29\u4e86\u8fc7\u53bb\u7684\u8bb0\u5fc6\u4ee5\u63d0\u4f9b\u66f4\u957f\u7684\u6ce8\u610f\u529b\u8303\u56f4\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6700\u8fdc\u7684<span translate=no>_^_0_^_</span>\u5185\u5b58\u88ab\u538b\u7f29\u5230<span translate=no>_^_1_^_</span>\u5185\u5b58\u4e2d\uff0c\u538b\u7f29\u7387\u5728<span translate=no>_^_2_^_</span>\u54ea\u91cc\u3002</p>\n<h2>\u538b\u7f29\u64cd\u4f5c</h2>\n<p>\u538b\u7f29\u64cd\u4f5c\u5b9a\u4e49\u4e3a<span translate=no>_^_3_^_</span>\u3002\u672c\u6587\u5f15\u5165\u4e86\u591a\u79cd\u9009\u62e9<span translate=no>_^_4_^_</span>\uff0c\u6211\u4eec\u53ea\u5b9e\u73b0\u4e86\u4e00\u7ef4\u5377\u79ef\uff0c\u8fd9\u4f3c\u4e4e\u53ef\u4ee5\u7ed9\u51fa\u6700\u4f73\u7ed3\u679c\u3002\u6bcf\u4e2a\u5c42\u90fd\u6709\u5355\u72ec\u7684\u538b\u7f29\u64cd\u4f5c<span translate=no>_^_6_^_</span>\uff0c<span translate=no>_^_5_^_</span>\u5176\u4e2d\u662f\u5c42\u53f7\u3002</p>\n<h2>\u8bad\u7ec3\u538b\u7f29\u64cd\u4f5c</h2>\n<p>\u7531\u4e8e\u4f7f\u7528 BPTT \u8bad\u7ec3\u538b\u7f29\u9700\u8981\u7ef4\u62a4\u975e\u5e38\u5927\u7684\u8ba1\u7b97\u56fe\uff08\u8bb8\u591a\u65f6\u95f4\u6b65\u957f\uff09\uff0c\u56e0\u6b64\u8be5\u8bba\u6587\u63d0\u51fa\u4e86<em>\u81ea\u52a8\u7f16\u7801\u635f\u5931</em>\u548c<em>\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931</em>\u3002\u81ea\u52a8\u7f16\u7801\u4e22\u5931\u5bf9\u538b\u7f29\u5b58\u50a8\u5668\u4e2d\u7684\u539f\u59cb\u5b58\u50a8\u5668\u8fdb\u884c\u89e3\u7801\u5e76\u8ba1\u7b97\u635f\u5931\u3002\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931\u8ba1\u7b97\u538b\u7f29\u5185\u5b58\u548c\u672a\u538b\u7f29\u5185\u5b58\u4e0a\u7684\u591a\u5934\u6ce8\u610f\u529b\u7ed3\u679c\uff0c\u5e76\u5f97\u51fa\u4e24\u8005\u4e4b\u95f4\u7684\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u5b9e\u73b0\u4e86\u540e\u8005\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u7ed3\u679c\u3002</p>\n<p>\u8be5\u5b9e\u73b0\u4f7f\u7528\u5c42\u524d\u6807\u51c6\u5316\uff0c\u800c\u8bba\u6587\u4f7f\u7528\u5c42\u540e\u5f52\u4e00\u5316\u3002\u524d\u5c42\u8303\u6570\u5728 <a href=\"../feedforward.html\">FFN</a> \u548c\u81ea\u6211\u6ce8\u610f\u529b\u4e4b\u524d\u5bf9\u5c42\u8fdb\u884c\u8303\u6570\uff0c\u5e76\u4e14\u6b8b\u5dee\u8fde\u63a5\u4e2d\u7684\u76f4\u901a\u672a\u6807\u51c6\u5316\u3002\u5728\u6807\u51c6\u53d8\u538b\u5668\u8bbe\u7f6e\u4e2d\uff0c\u8fd9\u5e94\u8be5\u66f4\u7a33\u5b9a\u3002</p>\n<p>\u4ee5\u4e0b\u662f\u7528\u4e8e<a href=\"experiment.html\">\u5728 Tiny Shakespeare \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u538b\u7f29\u53d8\u538b\u5668\u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\u548c\u7b14\u8bb0\u672c\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/compressive/experiment.ipynb\"><span translate=no>_^_7_^_</span></a></p>\n",
 "<h2>1D Convolution Compression <span translate=no>_^_0_^_</span></h2>\n<p>This is a simple wrapper around <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a> with some tensor dimension permutations.</p>\n": "<h2>\u4e00\u7ef4\u5377\u79ef\u538b\u7f29<span translate=no>_^_0_^_</span></h2>\n<p>\u8fd9\u662f\u4e00\u4e2a<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\"><span translate=no>_^_1_^_</span></a>\u5305\u542b\u4e00\u4e9b\u5f20\u91cf\u7ef4\u5ea6\u6392\u5217\u7684\u7b80\u5355\u5305\u88c5\u3002</p>\n",
 "<h2>Attention Reconstruction Loss</h2>\n<p>Attention reconstruction loss recreates the self-attention output with uncompressed memory and with compressed memory and calculates the mean squared error between the two. It does this without positional encoding.</p>\n<p>When calculating and training the compression function <span translate=no>_^_0_^_</span> with attention reconstruction loss, all parameters but <span translate=no>_^_1_^_</span> are frozen. This includes key/value projections and bias/scaling after normalization.</p>\n<p>Since this loss can be computed independently of the cross-entropy-loss of the model you can have a separate optimizer that only updates <span translate=no>_^_2_^_</span>. However, we use the same optimizer to update <span translate=no>_^_3_^_</span> so when calculating attention reconstruction loss, we detach all other parameters except <span translate=no>_^_4_^_</span> from the gradient computation.</p>\n": "<h2>\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931</h2>\n<p>\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931\u4f7f\u7528\u672a\u538b\u7f29\u7684\u5185\u5b58\u548c\u538b\u7f29\u7684\u5185\u5b58\u91cd\u73b0\u81ea\u6211\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u5e76\u8ba1\u7b97\u4e24\u8005\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002\u5b83\u5728\u6ca1\u6709\u4f4d\u7f6e\u7f16\u7801\u7684\u60c5\u51b5\u4e0b\u505a\u5230\u8fd9\u4e00\u70b9\u3002</p>\n<p>\u5f53\u8ba1\u7b97\u548c\u8bad\u7ec3<span translate=no>_^_0_^_</span>\u5177\u6709\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931\u7684\u538b\u7f29\u51fd\u6570\u65f6\uff0c\u6240\u6709\u53c2\u6570<span translate=no>_^_1_^_</span>\u90fd\u5c06\u88ab\u51bb\u7ed3\u3002\u8fd9\u5305\u62ec\u6807\u51c6\u5316\u540e\u7684\u952e/\u503c\u6295\u5f71\u548c\u504f\u5dee/\u7f29\u653e\u3002</p>\n<p>\u7531\u4e8e\u6b64\u635f\u5931\u53ef\u4ee5\u72ec\u7acb\u4e8e\u6a21\u578b\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u8ba1\u7b97\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u4f7f\u7528\u5355\u72ec\u7684\u4ec5\u66f4\u65b0\u4f18\u5316\u5668<span translate=no>_^_2_^_</span>\u3002\u4f46\u662f\uff0c\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u4f18\u5316\u5668\u8fdb\u884c\u66f4\u65b0\uff0c<span translate=no>_^_3_^_</span>\u56e0\u6b64\u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u91cd\u5efa\u635f\u5931\u65f6\uff0c\u6211\u4eec\u4f1a\u5206\u79bb\u9664\u68af\u5ea6\u8ba1\u7b97<span translate=no>_^_4_^_</span>\u4e4b\u5916\u7684\u6240\u6709\u5176\u4ed6\u53c2\u6570\u3002</p>\n",
 "<h2>Compressive Transformer Layer</h2>\n<p>This is the implementation of a single compressive transformer layer</p>\n": "<h2>\u538b\u7f29\u53d8\u538b\u5668\u5c42</h2>\n<p>\u8fd9\u662f\u5355\u4e2a\u538b\u7f29\u53d8\u538b\u5668\u5c42\u7684\u5b9e\u73b0</p>\n",
 "<h2>Compressive Transformer Model</h2>\n<p>This consists of multiple compressive transformer layers</p>\n": "<h2>\u538b\u7f29\u53d8\u538b\u5668\u578b\u53f7</h2>\n<p>\u5b83\u7531\u591a\u4e2a\u538b\u7f29\u53d8\u538b\u5668\u5c42\u7ec4\u6210</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_1_^_</span></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is the list of Compressive Transformer layers</p>\n": "<p><span translate=no>_^_0_^_</span>\u662f\u538b\u7f29\u53d8\u538b\u5668\u5c42\u7684\u5217\u8868</p>\n",
 "<p> Concatenate the normalized token embeddings with memory and compressed memory.</p>\n<ul><li><span translate=no>_^_0_^_</span> is layer normalized token embeddings. </li>\n<li><span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are memory and compressed memory (not normalized).</li></ul>\n": "<p>\u5c06\u6807\u51c6\u5316\u4ee4\u724c\u5d4c\u5165\u4e0e\u5185\u5b58\u548c\u538b\u7f29\u5185\u5b58\u8fde\u63a5\u8d77\u6765\u3002</p>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u5c42\u89c4\u8303\u5316\u4ee4\u724c\u5d4c\u5165\u3002</li>\n<li><span translate=no>_^_1_^_</span>\u548c<span translate=no>_^_2_^_</span>\u662f\u5185\u5b58\u548c\u538b\u7f29\u5185\u5b58\uff08\u672a\u89c4\u8303\u5316\uff09\u3002</li></ul>\n",
 "<p> Perform layer normalization with shift and scale parameters detached.</p>\n": "\u5728@@ <p>\u5206\u79bb\u79fb\u4f4d\u548c\u7f29\u653e\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u56fe\u5c42\u5f52\u4e00\u5316\u3002</p>\n",
 "<p> This calculates the loss for a layer</p>\n": "<p>\u8fd9\u5c06\u8ba1\u7b97\u4e00\u5c42\u7684\u635f\u5931</p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#MHA\">&#x27;Multi-Head Attention&#x27;</a> which calls <span translate=no>_^_0_^_</span> instead of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> to detach projection parameters.</p>\n": "<p>\u8fd9\u662f <a href=\"../mha.html#MHA\">\u201c\u591a\u5934\u6ce8\u610f\u201d \u7684\u91cd\u65b0\u5b9e\u73b0\uff0c\u5b83\u8c03\u7528<span translate=no>_^_0_^_</span>\u800c\u4e0d\u662f \u201c</a>prep <a href=\"../mha.html#PrepareMHA\">areFormultiHeadAttention\u201d</a> \u6765\u5206\u79bb\u6295\u5f71\u53c2\u6570\u3002</p>\n",
 "<p> This is a reimplementation of <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> where the projections are done with the parameters detached from gradient computation.</p>\n<ul><li><span translate=no>_^_0_^_</span> is the <a href=\"../mha.html#PrepareMHA\">&#x27;PrepareForMultiHeadAttention&#x27;</a> module </li>\n<li><span translate=no>_^_1_^_</span> is tensor with the token embeddings</li></ul>\n": "<p>\u8fd9\u662f <a href=\"../mha.html#PrepareMHA\">\u201cprepareFormultiHeadAttention\u201d</a> \u7684\u91cd\u65b0\u5b9e\u73b0\uff0c\u5176\u4e2d\u6295\u5f71\u662f\u4f7f\u7528\u4e0e\u68af\u5ea6\u8ba1\u7b97\u5206\u79bb\u7684\u53c2\u6570\u5b8c\u6210\u7684\u3002</p>\n<ul><li><span translate=no>_^_0_^_</span>\u662f <a href=\"../mha.html#PrepareMHA\">\u201cprepareFormultiHeadAttion\u201d</a></li>\n</ul><li><span translate=no>_^_1_^_</span>\u662f\u5e26\u6709\u4ee4\u724c\u5d4c\u5165\u7684\u5f20\u91cf</li>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u5173\u6ce8\u6309\u952e\u5e8f\u5217\u7ef4\u5ea6<span translate=no>_^_1_^_</span></p>\n",
 "<p>Add the attention results </p>\n": "<p>\u6dfb\u52a0\u5173\u6ce8\u7ed3\u679c</p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u5c06\u524d\u9988\u7ed3\u679c\u6dfb\u52a0\u56de\u6765</p>\n",
 "<p>Add to the list of feature vectors </p>\n": "<p>\u6dfb\u52a0\u5230\u7279\u5f81\u5411\u91cf\u5217\u8868\u4e2d</p>\n",
 "<p>Attention </p>\n": "<p>\u6ce8\u610f</p>\n",
 "<p>Calculate query, key and value projections </p>\n": "<p>\u8ba1\u7b97\u67e5\u8be2\u3001\u952e\u548c\u503c\u9884\u6d4b</p>\n",
 "<p>Calculate the attention with compressed memory </p>\n": "<p>\u4f7f\u7528\u538b\u7f29\u5185\u5b58\u8ba1\u7b97\u6ce8\u610f\u529b</p>\n",
 "<p>Calculate the attention with uncompressed memory </p>\n": "<p>\u4f7f\u7528\u672a\u538b\u7f29\u7684\u5185\u5b58\u8ba1\u7b97\u6ce8\u610f\u529b</p>\n",
 "<p>Calculate the losses for each layer </p>\n": "<p>\u8ba1\u7b97\u6bcf\u5c42\u7684\u635f\u5931</p>\n",
 "<p>Calculate the mean square error </p>\n": "<p>\u8ba1\u7b97\u5747\u65b9\u8bef\u5dee</p>\n",
 "<p>Compress the memory with <span translate=no>_^_0_^_</span>. The parameters of <span translate=no>_^_1_^_</span> are the only parameters not detached from gradient computation. </p>\n": "<p>\u4f7f\u7528\u538b\u7f29\u5185\u5b58<span translate=no>_^_0_^_</span>\u3002\u7684\u53c2\u6570<span translate=no>_^_1_^_</span>\u662f\u552f\u4e00\u672a\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u5206\u79bb\u51fa\u6765\u7684\u53c2\u6570\u3002</p>\n",
 "<p>Compressed Memory </p>\n": "<p>\u538b\u7f29\u5185\u5b58</p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570<span translate=no>_^_0_^_</span>\u3002\u8fd9\u7ed9\u51fa\u4e86\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span>\u3002</p>\n",
 "<p>Concatenate normalized memory and normalized token embeddings </p>\n": "<p>\u8fde\u63a5\u89c4\u8303\u5316\u5185\u5b58\u548c\u89c4\u8303\u5316\u4ee4\u724c\u5d4c\u5165</p>\n",
 "<p>Detach projection weights and bias </p>\n": "<p>\u5206\u79bb\u6295\u5f71\u6743\u91cd\u548c\u504f\u5dee</p>\n",
 "<p>Detach shift(<span translate=no>_^_0_^_</span>) and scaling(<span translate=no>_^_1_^_</span>) parameters </p>\n": "<p>\u5206\u79bb shift (<span translate=no>_^_0_^_</span>) \u548c\u7f29\u653e (<span translate=no>_^_1_^_</span>) \u53c2\u6570</p>\n",
 "<p>Detach the token embeddings and memory. </p>\n": "<p>\u5206\u79bb\u4ee4\u724c\u5d4c\u5165\u548c\u5185\u5b58\u3002</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7ec8\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>Finally, normalize the vectors </p>\n": "<p>\u6700\u540e\uff0c\u5bf9\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>Get compressed memory by running it through the convolution layer </p>\n": "<p>\u901a\u8fc7\u5377\u79ef\u5c42\u8fd0\u884c\u538b\u7f29\u5185\u5b58\u6765\u83b7\u53d6\u538b\u7f29\u5185\u5b58</p>\n",
 "<p>If there are compressed memory concatenate that with memory </p>\n": "<p>\u5982\u679c\u6709\u538b\u7f29\u7684\u5185\u5b58\uff0c\u5219\u5c06\u5176\u4e0e\u5185\u5b58\u8fde\u63a5\u8d77\u6765</p>\n",
 "<p>If there is no memory just return the token embeddings </p>\n": "<p>\u5982\u679c\u6ca1\u6709\u5185\u5b58\uff0c\u5219\u8fd4\u56de\u4ee4\u724c\u5d4c\u5165</p>\n",
 "<p>Layer normalization </p>\n": "<p>\u5c42\u89c4\u8303\u5316</p>\n",
 "<p>Linear transform </p>\n": "<p>\u7ebf\u6027\u53d8\u6362</p>\n",
 "<p>List to store token level feature vectors, which will become the memories for the next sequential batch. </p>\n": "<p>\u7528\u4e8e\u5b58\u50a8\u4ee4\u724c\u7ea7\u7279\u5f81\u5411\u91cf\u7684\u5217\u8868\uff0c\u8fd9\u4e9b\u5411\u91cf\u5c06\u6210\u4e3a\u4e0b\u4e00\u4e2a\u8fde\u7eed\u6279\u6b21\u7684\u8bb0\u5fc6\u3002</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u5236\u4f5c\u53d8\u538b\u5668\u5c42\u7684\u526f\u672c</p>\n",
 "<p>Memory </p>\n": "<p>\u8bb0\u5fc6</p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e58\u4ee5\u503c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Normalize and concatenate memory and compressed memory </p>\n": "<p>\u89c4\u8303\u5316\u5e76\u8fde\u63a5\u5185\u5b58\u548c\u538b\u7f29\u5185\u5b58</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u6807\u51c6\u5316\u4ee5\u8fdb\u884c\u524d\u9988</p>\n",
 "<p>Normalize the embeddings and memories </p>\n": "<p>\u89c4\u8303\u5316\u5d4c\u5165\u548c\u8bb0\u5fc6</p>\n",
 "<p>Normalize the vectors before doing self attention </p>\n": "<p>\u5728\u8fdb\u884c\u81ea\u6211\u6ce8\u610f\u4e4b\u524d\u5bf9\u5411\u91cf\u8fdb\u884c\u5f52\u4e00\u5316</p>\n",
 "<p>Output has shape <span translate=no>_^_0_^_</span> or <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8f93\u51fa\u5177\u6709\u5f62\u72b6<span translate=no>_^_0_^_</span>\u6216<span translate=no>_^_1_^_</span></p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u901a\u8fc7\u524d\u9988\u7f51\u7edc</p>\n",
 "<p>Permute back to form <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6392\u5217\u56de\u539f\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Permute the dimensions of <span translate=no>_^_0_^_</span> so that we can run it through the convolution layer. The convolution layer accepts in the form <span translate=no>_^_1_^_</span> </p>\n": "<p>\u6392\u5217\u7684\u7ef4\u5ea6\uff0c<span translate=no>_^_0_^_</span>\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u5377\u79ef\u5c42\u4e2d\u8fd0\u884c\u5b83\u3002\u5377\u79ef\u5c42\u63a5\u53d7\u4ee5\u4e0b\u5f62\u5f0f<span translate=no>_^_1_^_</span></p>\n",
 "<p>Run the memory through the normalization layer </p>\n": "<p>\u901a\u8fc7\u89c4\u8303\u5316\u5c42\u8fd0\u884c\u5185\u5b58</p>\n",
 "<p>Run through each transformer layer </p>\n": "<p>\u7a7f\u8fc7\u6bcf\u4e2a\u53d8\u538b\u5668\u5c42</p>\n",
 "<p>Run through the transformer XL layer </p>\n": "<p>\u7a7f\u8fc7\u53d8\u538b\u5668 XL \u5c42</p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u97f3\u9636\u5206\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Shape of the input except embedding dimension; <span translate=no>_^_0_^_</span>. </p>\n": "<p>\u9664\u5d4c\u5165\u5c3a\u5bf8\u4e4b\u5916\u7684\u8f93\u5165\u5f62\u72b6\uff1b<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p>Split last dimension into heads </p>\n": "<p>\u5c06\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u62c6\u5206\u6210\u5934\u90e8</p>\n",
 "<p>Sum of the losses </p>\n": "<p>\u635f\u5931\u603b\u548c</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the embedding size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u5d4c\u5165\u7684\u5927\u5c0f</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of the token embeddings vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a list of tensors of the past token level feature vectors of shape  <span translate=no>_^_3_^_</span> for each layer </li>\n<li><span translate=no>_^_4_^_</span> is a list of tensors of the compressed memory  <span translate=no>_^_5_^_</span> for each layer </li>\n<li><span translate=no>_^_6_^_</span> is the masking matrix</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5d4c\u5165\u5f62\u72b6\u5411\u91cf\u7684\u4ee4\u724c\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8fc7\u53bb\u4ee4\u724c\u7ea7\u522b\u7684\u5f20\u91cf\u5217\u8868\uff0c\u6bcf\u4e2a\u5c42\u7684\u5f62\u72b6<span translate=no>_^_3_^_</span>\u5411\u91cf\u7279\u5f81</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u6bcf\u5c42\u538b\u7f29\u5185\u5b58<span translate=no>_^_5_^_</span>\u7684\u5f20\u91cf\u5217\u8868</li>\n<li><span translate=no>_^_6_^_</span>\u662f\u63a9\u7801\u77e9\u9635</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of token level feature vectors of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is a tensor of the past token level feature vectors (memory) of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is a tensor of the compressed memory <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is a matrix of shape <span translate=no>_^_7_^_</span> or <span translate=no>_^_8_^_</span>. <span translate=no>_^_9_^_</span> is true if token at <span translate=no>_^_10_^_</span> can see token at <span translate=no>_^_11_^_</span>.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u7684\u4ee4\u724c\u7ea7\u7279\u5f81\u5411\u91cf\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8fc7\u53bb\u4ee4\u724c\u7ea7\u522b\u5f62\u72b6\u7279\u5f81\u5411\u91cf\uff08\u5185\u5b58\uff09\u7684\u5f20\u91cf<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u662f\u538b\u7f29\u5185\u5b58\u7684\u5f20\u91cf<span translate=no>_^_5_^_</span></li>\n<li><span translate=no>_^_6_^_</span>\u662f\u5f62\u72b6\u7684\u77e9\u9635<span translate=no>_^_7_^_</span>\u6216<span translate=no>_^_8_^_</span>\u3002<span translate=no>_^_9_^_</span>\u5982\u679c token<span translate=no>_^_10_^_</span> \u53ef\u4ee5\u5728\u5904\u770b\u5230\u4ee4\u724c\uff0c\u5219\u4e3a true<span translate=no>_^_11_^_</span>\u3002</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span> is the <a href=\"../xl/relative_mha.html\">self attention module</a> </li>\n<li><span translate=no>_^_2_^_</span> is the <a href=\"../feed_forward.html\">feed forward module</a> </li>\n<li><span translate=no>_^_3_^_</span> is the probability of dropping out after self attention and FFN </li>\n<li><span translate=no>_^_4_^_</span> is the compression function <span translate=no>_^_5_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4ee4\u724c\u5d4c\u5165\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_1_^_</span>\u662f<a href=\"../xl/relative_mha.html\">\u81ea\u6211\u5173\u6ce8\u6a21\u5757</a></li>\n<li><span translate=no>_^_2_^_</span>\u662f<a href=\"../feed_forward.html\">\u524d\u9988\u6a21\u5757</a></li>\n<li><span translate=no>_^_3_^_</span>\u662f\u81ea\u6211\u5173\u6ce8\u548c FFN \u540e\u9000\u5b66\u7684\u6982\u7387</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u538b\u7f29\u51fd\u6570<span translate=no>_^_5_^_</span></li></ul>\n",
 "Compressive Transformer": "\u538b\u7f29\u53d8\u538b\u5668",
 "Documented implementation with explanations of a Compressive Transformer model.": "\u8bb0\u5f55\u4e86\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5e76\u89e3\u91ca\u4e86\u538b\u7f29\u53d8\u538b\u5668\u6a21\u578b\u3002"
}