{
 "<p> </p>\n": "<p></p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u306e\u7d50\u679c\u3092\u8ffd\u52a0\u3057\u76f4\u3059</p>\n",
 "<p>Add the self attention results </p>\n": "<p>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u7d50\u679c\u3092\u8ffd\u52a0</p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u8907\u6570\u306e\u30d8\u30c3\u30c9\u3092\u9023\u7d50</p>\n",
 "<p>Dropout </p>\n": "<p>\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7d42\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>For each input step </p>\n": "<p>\u5404\u30a4\u30f3\u30d7\u30c3\u30c8\u30b9\u30c6\u30c3\u30d7\u306b\u3064\u3044\u3066</p>\n",
 "<p>Get layer output </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u51fa\u529b\u3092\u53d6\u5f97</p>\n",
 "<p>List to store the outputs </p>\n": "<p>\u51fa\u529b\u3092\u4fdd\u5b58\u3059\u308b\u30ea\u30b9\u30c8</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u30b3\u30d4\u30fc\u3092\u4f5c\u6210</p>\n",
 "<p>Normalization layers </p>\n": "<p>\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u7528\u306b\u6b63\u898f\u5316</p>\n",
 "<p>Normalize the output </p>\n": "<p>\u51fa\u529b\u3092\u6b63\u898f\u5316</p>\n",
 "<p>Number of features per head </p>\n": "<p>\u30d8\u30c3\u30c9\u3042\u305f\u308a\u306e\u6a5f\u80fd\u6570</p>\n",
 "<p>Output layer </p>\n": "<p>\u51fa\u529b\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u904e</p>\n",
 "<p>Run through each layer </p>\n": "<p>\u5404\u30ec\u30a4\u30e4\u30fc\u3092\u8cab\u901a\u3059\u308b</p>\n",
 "<p>Split the input to a list along the sequence axis </p>\n": "<p>\u5165\u529b\u3092\u30b7\u30fc\u30b1\u30f3\u30b9\u8ef8\u306b\u6cbf\u3063\u3066\u30ea\u30b9\u30c8\u306b\u5206\u5272\u3057\u307e\u3059</p>\n",
 "<p>Stack the output tensors </p>\n": "<p>\u51fa\u529b\u30c6\u30f3\u30bd\u30eb\u3092\u7a4d\u307f\u91cd\u306d\u308b</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for multi-headed attention. </p>\n": "<p>\u3053\u308c\u3089\u306f\u982d\u306e\u4e2d\u3092\u4e00\u5909\u3055\u305b<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u591a\u9762\u7684\u306a\u6ce8\u76ee\u3092\u96c6\u3081\u307e\u3059\u3002</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> multi-headed attention. </p>\n": "<p><span translate=no>_^_0_^_</span>\u3053\u308c\u3089\u306f\u591a\u9762\u7684\u306a\u6ce8\u610f\u529b\u3092\u5909\u3048\u307e\u3059\u3002</p>\n",
 "<p>Transformer size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5909\u5727\u5668\u30b5\u30a4\u30ba <span translate=no>_^_0_^_</span></p>\n",
 "Fast Weight Systems": "\u30d5\u30a1\u30b9\u30c8\u30fb\u30a6\u30a7\u30a4\u30c8\u30fb\u30b7\u30b9\u30c6\u30e0",
 "This is an annotated implementation/tutorial of Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch.": "\u3053\u308c\u306f\u3001PyTorch\u306e\u30ea\u30cb\u30a2\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3072\u305d\u304b\u306b\u9ad8\u901f\u30a6\u30a7\u30a4\u30c8\u30e1\u30e2\u30ea\u30b7\u30b9\u30c6\u30e0\u3067\u3042\u308b\u3068\u3044\u3046\u6ce8\u91c8\u4ed8\u304d\u306e\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002"
}