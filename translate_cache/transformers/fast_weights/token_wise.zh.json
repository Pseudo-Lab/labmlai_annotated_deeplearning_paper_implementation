{
 "<p> </p>\n": "<p></p>\n",
 "<p>Add the feed-forward results back </p>\n": "<p>\u5c06\u524d\u9988\u7ed3\u679c\u6dfb\u52a0\u56de\u6765</p>\n",
 "<p>Add the self attention results </p>\n": "<p>\u6dfb\u52a0\u81ea\u6211\u5173\u6ce8\u7684\u7ed3\u679c</p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u8fde\u63a5\u591a\u4e2a\u5934</p>\n",
 "<p>Dropout </p>\n": "<p>\u8f8d\u5b66</p>\n",
 "<p>Final normalization layer </p>\n": "<p>\u6700\u7ec8\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>For each input step </p>\n": "<p>\u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u6b65\u9aa4</p>\n",
 "<p>Get layer output </p>\n": "<p>\u83b7\u53d6\u56fe\u5c42\u8f93\u51fa</p>\n",
 "<p>List to store the outputs </p>\n": "<p>\u5b58\u50a8\u8f93\u51fa\u7684\u5217\u8868</p>\n",
 "<p>Make copies of the transformer layer </p>\n": "<p>\u5236\u4f5c\u53d8\u538b\u5668\u5c42\u7684\u526f\u672c</p>\n",
 "<p>Normalization layers </p>\n": "<p>\u5f52\u4e00\u5316\u5c42</p>\n",
 "<p>Normalize for feed-forward </p>\n": "<p>\u6807\u51c6\u5316\u4ee5\u8fdb\u884c\u524d\u9988</p>\n",
 "<p>Normalize the output </p>\n": "<p>\u89c4\u8303\u5316\u8f93\u51fa</p>\n",
 "<p>Number of features per head </p>\n": "<p>\u6bcf\u5934\u7279\u5f81\u6570</p>\n",
 "<p>Output layer </p>\n": "<p>\u8f93\u51fa\u5c42</p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>\u901a\u8fc7\u524d\u9988\u7f51\u7edc</p>\n",
 "<p>Run through each layer </p>\n": "<p>\u7a7f\u8fc7\u6bcf\u4e00\u5c42</p>\n",
 "<p>Split the input to a list along the sequence axis </p>\n": "<p>\u6cbf\u5e8f\u5217\u8f74\u5c06\u8f93\u5165\u62c6\u5206\u4e3a\u4e00\u4e2a\u5217\u8868</p>\n",
 "<p>Stack the output tensors </p>\n": "<p>\u5806\u53e0\u8f93\u51fa\u5f20\u91cf</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for multi-headed attention. </p>\n": "<p>\u8fd9\u4e9b\u6539\u53d8\u4e86\u591a<span translate=no>_^_1_^_</span>\u5934\u6ce8\u610f\u529b\u7684<span translate=no>_^_0_^_</span>\u548c\u3002</p>\n",
 "<p>These transform the <span translate=no>_^_0_^_</span> multi-headed attention. </p>\n": "<p>\u8fd9\u4e9b\u6539\u53d8\u4e86<span translate=no>_^_0_^_</span>\u591a\u5934\u6ce8\u610f\u529b\u3002</p>\n",
 "<p>Transformer size <span translate=no>_^_0_^_</span> </p>\n": "<p>\u53d8\u538b\u5668\u5c3a\u5bf8<span translate=no>_^_0_^_</span></p>\n",
 "Fast Weight Systems": "\u5feb\u901f\u79f0\u91cd\u7cfb\u7edf",
 "This is an annotated implementation/tutorial of Linear Transformers Are Secretly Fast Weight Memory Systems in PyTorch.": "\u8fd9\u662f PyTorch \u4e2d\u7ebf\u6027\u53d8\u538b\u5668\u662f\u79d8\u5bc6\u7684\u5feb\u901f\u91cd\u91cf\u5b58\u50a8\u7cfb\u7edf\u7684\u5e26\u6ce8\u91ca\u7684\u5b9e\u73b0/\u6559\u7a0b\u3002"
}