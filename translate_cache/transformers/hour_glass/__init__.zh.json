{
 "<h1>Hierarchical Transformers Are More Efficient Language Models</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2110.13711\">Hierarchical Transformers Are More Efficient Language Models</a>.</p>\n<p>This paper introduces a hierarchical transformer architecture to handle long sequences efficiently. The first half of the transformer layers down-sample tokens and the second half up-samples with direct skip connections between layers of the same resolution. This is a little similar to <a href=\"../../diffusion/ddpm/unet.html\">U-Net</a> for vision tasks.</p>\n<p>They try different up-sampling and down-sampling techniques and build a model with the best performing up and down-sampling techniques which they call the hourglass model.</p>\n<p>Here we have implemented the simplest up-sampling and down-sampling techniques for simplicity. We will consider adding more complex (and better performing) implementations later.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for the hourglass model.</p>\n": "<h1>\u5206\u5c42\u8f6c\u6362\u5668\u662f\u66f4\u6709\u6548\u7684\u8bed\u8a00\u6a21\u578b</h1>\n<p>\u8fd9\u662f\u8bba\u6587\u300a<a href=\"https://papers.labml.ai/paper/2110.13711\">\u5206\u5c42\u8f6c\u6362\u5668\u662f\u66f4\u6709\u6548\u7684\u8bed\u8a00\u6a21\u578b\u300b\u7684</a> <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0\u3002</p>\n<p>\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5206\u5c42\u53d8\u538b\u5668\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u957f\u5e8f\u5217\u3002\u53d8\u538b\u5668\u5c42\u7684\u524d\u534a\u90e8\u5206\u5411\u4e0b\u91c7\u6837\u4ee4\u724c\uff0c\u540e\u534a\u90e8\u5206\u5728\u76f8\u540c\u5206\u8fa8\u7387\u7684\u5c42\u4e4b\u95f4\u4f7f\u7528\u76f4\u63a5\u8df3\u8fc7\u8fde\u63a5\u5411\u4e0a\u53d6\u6837\u3002\u8fd9\u4e0e\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u7684 <a href=\"../../diffusion/ddpm/unet.html\">U-Net</a> \u6709\u70b9\u76f8\u4f3c\u3002</p>\n<p>\u4ed6\u4eec\u5c1d\u8bd5\u4e0d\u540c\u7684\u5411\u4e0a\u91c7\u6837\u548c\u5411\u4e0b\u91c7\u6837\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u6027\u80fd\u6700\u4f73\u7684\u5411\u4e0a\u548c\u5411\u4e0b\u91c7\u6837\u6280\u672f\u6784\u5efa\u6a21\u578b\uff0c\u4ed6\u4eec\u79f0\u4e4b\u4e3a\u6c99\u6f0f\u6a21\u578b\u3002</p>\n\u4e3a\u7b80\u5355\u8d77@@ <p>\u89c1\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u5b9e\u73b0\u4e86\u6700\u7b80\u5355\u7684\u4e0a\u91c7\u6837\u548c\u5411\u4e0b\u91c7\u6837\u6280\u672f\u3002\u7a0d\u540e\u6211\u4eec\u4f1a\u8003\u8651\u6dfb\u52a0\u66f4\u590d\u6742\uff08\u6027\u80fd\u66f4\u597d\uff09\u7684\u5b9e\u73b0\u3002</p>\n<p>\u8fd9\u662f\u6c99\u6f0f\u6a21\u578b\u7684<a href=\"experiment.html\">\u8bad\u7ec3\u4ee3\u7801</a>\u3002</p>\n",
 "<h2>Hourglass model</h2>\n<p>This model recursively adds layers to the middle while shortening the sequence by down-sampling. The shortened sequence processed by another hourglass model is sandwiched between two normal transformer layers. (A transformer layer has a <a href=\"../mha.html\">self-attention layer</a>  and a <a href=\"../feed_forward.html\">position-wise feed-forward layer</a>).</p>\n": "<h2>\u6c99\u6f0f\u578b\u53f7</h2>\n<p>\u8be5\u6a21\u578b\u9012\u5f52\u5730\u5728\u4e2d\u95f4\u6dfb\u52a0\u56fe\u5c42\uff0c\u540c\u65f6\u901a\u8fc7\u7f29\u51cf\u91c7\u6837\u6765\u7f29\u77ed\u5e8f\u5217\u3002\u7531\u53e6\u4e00\u4e2a\u6c99\u6f0f\u6a21\u578b\u5904\u7406\u7684\u7f29\u77ed\u5e8f\u5217\u5939\u5728\u4e24\u4e2a\u666e\u901a\u7684\u53d8\u538b\u5668\u5c42\u4e4b\u95f4\u3002\uff08\u53d8\u538b\u5668\u5c42\u5177\u6709<a href=\"../mha.html\">\u81ea\u6ce8\u610f\u529b\u5c42</a>\u548c<a href=\"../feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u5c42</a>\uff09\u3002</p>\n",
 "<h3>Average pool shortening</h3>\n<p>This down-samples by a given factor with average pooling</p>\n": "<h3>\u6c60\u5e73\u5747\u7f29\u77ed</h3>\n<p>\u8fd9\u4f1a\u6309\u7ed9\u5b9a\u56e0\u5b50\u5411\u4e0b\u91c7\u6837\uff0c\u5e76\u4f7f\u7528\u5e73\u5747\u6c47\u96c6</p>\n",
 "<h3>Generate auto-regressive mask</h3>\n": "<h3>\u751f\u6210\u81ea\u52a8\u56de\u5f52\u63a9\u7801</h3>\n",
 "<h3>Naive up-sampling</h3>\n<p>This up-samples by repeating</p>\n": "<h3>\u6734\u7d20\u7684\u5411\u4e0a\u91c7\u6837</h3>\n<p>\u8fd9\u901a\u8fc7\u91cd\u590d\u5411\u4e0a\u91c7\u6837</p>\n",
 "<h3>Shift right operation</h3>\n<p>This shifts the sequence to the right by the given number of steps</p>\n": "<h3>\u5411\u53f3\u79fb\u64cd\u4f5c</h3>\n<p>\u8fd9\u4f1a\u5c06\u5e8f\u5217\u5411\u53f3\u79fb\u52a8\u7ed9\u5b9a\u6b65\u6570</p>\n",
 "<h3>\ud83d\udea7 Attention based up-sampling</h3>\n<span translate=no>_^_0_^_</span><p>where <span translate=no>_^_1_^_</span></p>\n": "<h3>\ud83d\udea7 \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5411\u4e0a\u91c7\u6837</h3>\n<span translate=no>_^_0_^_</span><p>\u5728\u54ea\u91cc<span translate=no>_^_1_^_</span></p>\n",
 "<h3>\ud83d\udea7 Down-sampling with attention</h3>\n<span translate=no>_^_0_^_</span><p>where <span translate=no>_^_1_^_</span> is average pooling or linear pooling.</p>\n": "<h3>\ud83d\udea7 \u6ce8\u610f\u5411\u4e0b\u91c7\u6837</h3>\n<span translate=no>_^_0_^_</span><p>\u5176\u4e2d<span translate=no>_^_1_^_</span>\u662f\u5e73\u5747\u6c60\u5316\u6216\u7ebf\u6027\u6c60\u3002</p>\n",
 "<h3>\ud83d\udea7 Linear pooling for down-sampling</h3>\n<p>This concatenates the consecutive tokens embeddings that need to be merged and do a linear transformation to map it to the size of a single token embedding.</p>\n": "<h3>\ud83d\udea7 \u7528\u4e8e\u7f29\u51cf\u91c7\u6837\u7684\u7ebf\u6027\u6c60</h3>\n<p>\u8fd9\u5c06\u9700\u8981\u5408\u5e76\u7684\u8fde\u7eed\u4ee4\u724c\u5d4c\u5165\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u4ee5\u5c06\u5176\u6620\u5c04\u5230\u5355\u4e2a\u4ee4\u724c\u5d4c\u5165\u7684\u5927\u5c0f\u3002</p>\n",
 "<h3>\ud83d\udea7 Linear projection for up-sampling</h3>\n<p>Make a linear projection of dense token embeddings to a size of <span translate=no>_^_0_^_</span>.</p>\n": "<h3>\ud83d\udea7 \u7528\u4e8e\u5411\u4e0a\u91c7\u6837\u7684\u7ebf\u6027\u6295\u5f71</h3>\n\u5c06@@ <p>\u5bc6\u96c6\u4ee4\u724c\u5d4c\u5165\u8fdb\u884c\u7ebf\u6027\u6295\u5f71\uff0c\u4f7f\u5176\u5927\u5c0f\u4e3a<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><a href=\".. / feed_forward.html\">Position wise feed-forward layers</a> </p>\n": "<p><a href=\".. / feed_forward.html\">\u5b9a\u4f4d\u660e\u667a\u7684\u524d\u9988\u5c42</a></p>\n",
 "<p><a href=\"../mha.html\">Multi-head attention layer</a> </p>\n": "<p><a href=\"../mha.html\">\u591a\u5934\u6ce8\u610f\u5c42</a></p>\n",
 "<p><a href=\"../utils.html\">Subsequent mask</a>, will mask out tokens from seeing future tokens </p>\n": "<p><a href=\"../utils.html\">\u540e\u7eed\u7684\u63a9\u7801</a>\uff0c\u5c06\u63a9\u76d6\u4ee4\u724c\u4ee5\u514d\u770b\u5230\u672a\u6765\u7684\u4ee3\u5e01</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Auto-regressive mask </p>\n": "<p>\u81ea\u52a8\u56de\u5f52\u63a9\u7801</p>\n",
 "<p>Autoregressive mask </p>\n": "<p>\u81ea\u56de\u5f52\u906e\u7f69</p>\n",
 "<p>Average pooling layer </p>\n": "<p>\u5e73\u5747\u6c60\u5c42</p>\n",
 "<p>Center transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e2d\u5fc3\u53d8\u538b\u5668\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Concatenate the zeros and truncate the right </p>\n": "<p>\u8fde\u63a5\u96f6\u5e76\u622a\u65ad\u53f3\u8fb9</p>\n",
 "<p>Create a mask if we haven&#x27;t created or sizes have changed </p>\n": "<p>\u5982\u679c\u6211\u4eec\u5c1a\u672a\u521b\u5efa\u6216\u5927\u5c0f\u5df2\u66f4\u6539\uff0c\u8bf7\u521b\u5efa\u8499\u7248</p>\n",
 "<p>Final transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u7ec8\u7684\u53d8\u538b\u5668\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>If the shift is <span translate=no>_^_0_^_</span> return the original </p>\n": "<p>\u5982\u679c\u79fb\u4f4d\u662f<span translate=no>_^_0_^_</span>\u8fd4\u56de\u539f\u6765\u7684</p>\n",
 "<p>If there are no more shortening (middle of the hourglass) </p>\n": "<p>\u5982\u679c\u6ca1\u6709\u66f4\u591a\u7684\u7f29\u77ed\uff08\u6c99\u6f0f\u7684\u4e2d\u95f4\uff09</p>\n",
 "<p>If we are at the center of the hourglass, <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5982\u679c\u6211\u4eec\u5728\u6c99\u6f0f\u7684\u4e2d\u5fc3<span translate=no>_^_0_^_</span></p>\n",
 "<p>Initial transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u521d\u59cb\u53d8\u538b\u5668\u5c42<span translate=no>_^_0_^_</span></p>\n",
 "<p>Insert another hourglass model recursively </p>\n": "<p>\u9012\u5f52\u5730\u63d2\u5165\u53e6\u4e00\u4e2a\u6c99\u6f0f\u6a21\u578b</p>\n",
 "<p>Pooling layer accepts shape <span translate=no>_^_0_^_</span> so we permute axes. </p>\n": "<p>\u6c60\u5316\u5c42\u63a5\u53d7\u5f62\u72b6<span translate=no>_^_0_^_</span>\uff0c\u6240\u4ee5\u6211\u4eec\u6392\u5217\u8f74\u3002</p>\n",
 "<p>Repeat across the sequence dimension </p>\n": "<p>\u5728\u5e8f\u5217\u7ef4\u5ea6\u4e0a\u91cd\u590d</p>\n",
 "<p>Shifting and shortening <span translate=no>_^_0_^_</span> </p>\n": "<p>\u79fb\u4f4d\u548c\u7f29\u77ed<span translate=no>_^_0_^_</span></p>\n",
 "<p>Shortening or the down-sampling layer. We use the simplest form - average pooling. The paper shows that attention based down sampling works best, which we haven&#x27;t implemented yet. </p>\n": "<p>\u7f29\u77ed\u6216\u7f29\u51cf\u91c7\u6837\u5c42\u3002\u6211\u4eec\u4f7f\u7528\u6700\u7b80\u5355\u7684\u5f62\u5f0f\u2014\u2014\u5e73\u5747\u6c47\u96c6\u3002\u8be5\u8bba\u6587\u8868\u660e\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5411\u4e0b\u91c7\u6837\u6548\u679c\u6700\u597d\uff0c\u4f46\u6211\u4eec\u5c1a\u672a\u5b9e\u65bd\u3002</p>\n",
 "<p>The center layer is another transformer layer </p>\n": "<p>\u4e2d\u5fc3\u5c42\u662f\u53e6\u4e00\u4e2a\u53d8\u538b\u5668\u5c42</p>\n",
 "<p>The final transformer layer after up-sampling </p>\n": "<p>\u4e0a\u91c7\u6837\u540e\u7684\u6700\u7ec8\u53d8\u538b\u5668\u5c42</p>\n",
 "<p>The shortening factor <span translate=no>_^_0_^_</span> (or the down-sampling rate) </p>\n": "<p>\u7f29\u77ed\u7cfb\u6570<span translate=no>_^_0_^_</span>\uff08\u6216\u7f29\u51cf\u91c7\u6837\u7387\uff09</p>\n",
 "<p>The transformer layer before down-sampling </p>\n": "<p>\u4e0b\u91c7\u6837\u524d\u7684\u53d8\u538b\u5668\u5c42</p>\n",
 "<p>Truncate the extra embeddings at the end </p>\n": "<p>\u5728\u6700\u540e\u622a\u65ad\u591a\u4f59\u7684\u5d4c\u5165</p>\n",
 "<p>Up-sample the shortened sequence and add a skip connection <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5bf9\u7f29\u77ed\u7684\u5e8f\u5217\u8fdb\u884c\u5411\u4e0a\u91c7\u6837\u5e76\u6dfb\u52a0\u8df3\u8fc7\u8fde\u63a5<span translate=no>_^_0_^_</span></p>\n",
 "<p>Up-sampling layer. We use naive up-sampling for simplicity and the paper shows attention based up sampling works better. </p>\n": "<p>\u5411\u4e0a\u91c7\u6837\u56fe\u5c42\u3002\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u4f7f\u7528\u5929\u771f\u7684\u5411\u4e0a\u91c7\u6837\uff0c\u672c\u6587\u663e\u793a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u91c7\u6837\u6548\u679c\u66f4\u597d\u3002</p>\n",
 "<p>We shift the tokens to the right by <span translate=no>_^_0_^_</span> steps to make sure information doesn&#x27;t leak from the future tokens to past tokens as a result of down-sampling and up-sampling </p>\n": "<p>\u6211\u4eec\u901a\u8fc7<span translate=no>_^_0_^_</span>\u6b65\u9aa4\u5c06\u4ee4\u724c\u5411\u53f3\u79fb\u52a8\uff0c\u4ee5\u786e\u4fdd\u4fe1\u606f\u4e0d\u4f1a\u56e0\u4e3a\u7f29\u51cf\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u800c\u4ece\u672a\u6765\u7684\u4ee3\u5e01\u6cc4\u6f0f\u5230\u8fc7\u53bb\u7684\u4ee3\u5e01\u4e0a</p>\n",
 "<p>Zeros to be appended to the left </p>\n": "<p>\u8981\u8ffd\u52a0\u5230\u5de6\u8fb9\u7684\u96f6</p>\n",
 "<p>cannot be negative </p>\n": "<p>\u4e0d\u80fd\u4e3a\u8d1f\u6570</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u5f20\u91cf<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u4e0d\u9519<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of heads in <a href=\"../mha.html\">multi-head attention layers</a> </li>\n<li><span translate=no>_^_1_^_</span> is the size of the token embeddings </li>\n<li><span translate=no>_^_2_^_</span> is the dropout probability </li>\n<li><span translate=no>_^_3_^_</span> is the dimensionality of the hidden layer in <a href=\"../feed_forward.html\">position-wise feed-forward layers</a> </li>\n<li><span translate=no>_^_4_^_</span> is the list of shortening factors</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f<a href=\"../mha.html\">\u591a\u5934\u6ce8\u610f\u5c42\u4e2d\u7684\u5934\u90e8</a>\u6570\u91cf</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u4ee4\u724c\u5d4c\u5165\u7684\u5927\u5c0f</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8f8d\u5b66\u6982\u7387</li>\n<li><span translate=no>_^_3_^_</span>\u662f<a href=\"../feed_forward.html\">\u4f4d\u7f6e\u524d\u9988\u5c42\u4e2d\u9690\u85cf\u5c42\u7684</a>\u7ef4\u5ea6</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u7f29\u77ed\u56e0\u5b50\u6e05\u5355</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of steps to shift by</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8981\u79fb\u4f4d\u7684\u6b65\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the shortening factor</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u7f29\u77ed\u7cfb\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the tensor with embeddings before down-sampling </li>\n<li><span translate=no>_^_1_^_</span> is the tensor of higher density (to be up-sampled) representations</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5411\u4e0b\u91c7\u6837\u4e4b\u524d\u6709\u5d4c\u5165\u7684\u5f20\u91cf</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u8f83\u9ad8\u5bc6\u5ea6\uff08\u5f85\u5411\u4e0a\u91c7\u6837\uff09\u8868\u793a\u7684\u5f20\u91cf</li></ul>\n",
 "Hierarchical Transformers Are More Efficient Language Models": "\u5206\u5c42\u53d8\u6362\u5668\u662f\u66f4\u6709\u6548\u7684\u8bed\u8a00\u6a21\u578b",
 "This is an annotated implementation/tutorial of hourglass model in PyTorch.": "\u8fd9\u662f PyTorch \u4e2d\u6c99\u6f0f\u6a21\u578b\u7684\u5e26\u6ce8\u91ca\u7684\u5b9e\u73b0/\u6559\u7a0b\u3002"
}