{
 "<h1>Hierarchical Transformers Are More Efficient Language Models</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2110.13711\">Hierarchical Transformers Are More Efficient Language Models</a>.</p>\n<p>This paper introduces a hierarchical transformer architecture to handle long sequences efficiently. The first half of the transformer layers down-sample tokens and the second half up-samples with direct skip connections between layers of the same resolution. This is a little similar to <a href=\"../../diffusion/ddpm/unet.html\">U-Net</a> for vision tasks.</p>\n<p>They try different up-sampling and down-sampling techniques and build a model with the best performing up and down-sampling techniques which they call the hourglass model.</p>\n<p>Here we have implemented the simplest up-sampling and down-sampling techniques for simplicity. We will consider adding more complex (and better performing) implementations later.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for the hourglass model.</p>\n": "<h1>\u968e\u5c64\u578b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3088\u308a\u52b9\u7387\u7684\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb</h1>\n<p>\u3053\u308c\u306f\u3001\u300c<a href=\"https://papers.labml.ai/paper/2110.13711\">\u968e\u5c64\u578b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3088\u308a\u52b9\u7387\u7684\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb</a>\u300d<a href=\"https://pytorch.org\">\u3068\u3044\u3046\u8ad6\u6587\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a>\u3002</p>\n<p>\u672c\u7a3f\u3067\u306f\u3001\u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u52b9\u7387\u7684\u306b\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u968e\u5c64\u578b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30ec\u30a4\u30e4\u30fc\u306e\u524d\u534a\u306f\u30c8\u30fc\u30af\u30f3\u3092\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u5f8c\u534a\u306f\u540c\u3058\u89e3\u50cf\u5ea6\u306e\u30ec\u30a4\u30e4\u30fc\u9593\u3092\u76f4\u63a5\u30b9\u30ad\u30c3\u30d7\u63a5\u7d9a\u3057\u3066\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002<a href=\"../../diffusion/ddpm/unet.html\">\u3053\u308c\u306f\u30d3\u30b8\u30e7\u30f3\u30bf\u30b9\u30af\u7528\u306eU-Net\u306b\u5c11\u3057\u4f3c\u3066\u3044\u307e\u3059</a></p>\u3002\n<p>\u5f7c\u3089\u306f\u3055\u307e\u3056\u307e\u306a\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u624b\u6cd5\u3092\u8a66\u3057\u3001\u7802\u6642\u8a08\u30e2\u30c7\u30eb\u3068\u547c\u3070\u308c\u308b\u6700\u3082\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u306e\u9ad8\u3044\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u624b\u6cd5\u3068\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u624b\u6cd5\u3092\u4f7f\u7528\u3057\u3066\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002</p>\n<p>\u3053\u3053\u3067\u306f\u3001\u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u3001\u6700\u3082\u5358\u7d14\u306a\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u624b\u6cd5\u3092\u5b9f\u88c5\u3057\u307e\u3057\u305f\u3002\u5f8c\u307b\u3069\u3001\u3088\u308a\u8907\u96d1\u306a (\u305d\u3057\u3066\u3088\u308a\u9ad8\u6027\u80fd\u306a) \u5b9f\u88c5\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3092\u691c\u8a0e\u3057\u307e\u3059</p>\u3002\n<p><a href=\"experiment.html\">\u7802\u6642\u8a08\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n",
 "<h2>Hourglass model</h2>\n<p>This model recursively adds layers to the middle while shortening the sequence by down-sampling. The shortened sequence processed by another hourglass model is sandwiched between two normal transformer layers. (A transformer layer has a <a href=\"../mha.html\">self-attention layer</a>  and a <a href=\"../feed_forward.html\">position-wise feed-forward layer</a>).</p>\n": "<h2>\u7802\u6642\u8a08\u30e2\u30c7\u30eb</h2>\n<p>\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306b\u3088\u3063\u3066\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u77ed\u7e2e\u3057\u306a\u304c\u3089\u3001\u4e2d\u592e\u306b\u30ec\u30a4\u30e4\u30fc\u3092\u518d\u5e30\u7684\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002\u5225\u306e\u7802\u6642\u8a08\u30e2\u30c7\u30eb\u3067\u51e6\u7406\u3055\u308c\u305f\u77ed\u7e2e\u30b7\u30fc\u30b1\u30f3\u30b9\u306f\u30012\u3064\u306e\u901a\u5e38\u306e\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc\u306e\u9593\u306b\u631f\u307e\u308c\u307e\u3059\u3002\uff08<a href=\"../mha.html\">\u30c8\u30e9\u30f3\u30b9\u5c64\u306b\u306f\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\u3068\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u5c64\u304c\u3042\u308a\u307e\u3059</a><a href=\"../feed_forward.html\">\uff09</a></p>\u3002\n",
 "<h3>Average pool shortening</h3>\n<p>This down-samples by a given factor with average pooling</p>\n": "<h3>\u5e73\u5747\u7684\u306a\u30d7\u30fc\u30eb\u77ed\u7e2e</h3>\n<p>\u3053\u308c\u306f\u3001\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u4f7f\u7528\u3057\u3066\u7279\u5b9a\u306e\u4fc2\u6570\u3067\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002</p>\n",
 "<h3>Generate auto-regressive mask</h3>\n": "<h3>\u81ea\u52d5\u56de\u5e30\u30de\u30b9\u30af\u3092\u751f\u6210</h3>\n",
 "<h3>Naive up-sampling</h3>\n<p>This up-samples by repeating</p>\n": "<h3>\u30ca\u30a4\u30fc\u30d6\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0</h3>\n<p>\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3057\u3066\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u307e\u3059</p>\n",
 "<h3>Shift right operation</h3>\n<p>This shifts the sequence to the right by the given number of steps</p>\n": "<h3>\u53f3\u30b7\u30d5\u30c8\u64cd\u4f5c</h3>\n<p>\u3053\u308c\u306b\u3088\u308a\u3001\u6307\u5b9a\u3057\u305f\u30b9\u30c6\u30c3\u30d7\u6570\u3060\u3051\u30b7\u30fc\u30b1\u30f3\u30b9\u304c\u53f3\u306b\u30b7\u30d5\u30c8\u3057\u307e\u3059\u3002</p>\n",
 "<h3>\ud83d\udea7 Attention based up-sampling</h3>\n<span translate=no>_^_0_^_</span><p>where <span translate=no>_^_1_^_</span></p>\n": "<h3>\ud83d\udea7 \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d9\u30fc\u30b9\u306e\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0</h3>\n<span translate=no>_^_0_^_</span><p>\u3069\u3053 <span translate=no>_^_1_^_</span></p>\n",
 "<h3>\ud83d\udea7 Down-sampling with attention</h3>\n<span translate=no>_^_0_^_</span><p>where <span translate=no>_^_1_^_</span> is average pooling or linear pooling.</p>\n": "<h3>\ud83d\udea7 \u6ce8\u610f\u3057\u3066\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0</h3>\n<span translate=no>_^_0_^_</span><p>\u3053\u3053\u3067<span translate=no>_^_1_^_</span>\u3001\u306f\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u304b\u7dda\u5f62\u30d7\u30fc\u30ea\u30f3\u30b0\u304b\u3067\u3059\u3002</p>\n",
 "<h3>\ud83d\udea7 Linear pooling for down-sampling</h3>\n<p>This concatenates the consecutive tokens embeddings that need to be merged and do a linear transformation to map it to the size of a single token embedding.</p>\n": "<h3>\ud83d\udea7 \u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u7528\u306e\u30ea\u30cb\u30a2\u30d7\u30fc\u30ea\u30f3\u30b0</h3>\n<p>\u3053\u308c\u306b\u3088\u308a\u3001\u30de\u30fc\u30b8\u304c\u5fc5\u8981\u306a\u9023\u7d9a\u3057\u305f\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u304c\u9023\u7d50\u3055\u308c\u30011 \u3064\u306e\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u306e\u30b5\u30a4\u30ba\u306b\u5408\u308f\u305b\u3066\u7dda\u5f62\u5909\u63db\u304c\u884c\u308f\u308c\u307e\u3059\u3002</p>\n",
 "<h3>\ud83d\udea7 Linear projection for up-sampling</h3>\n<p>Make a linear projection of dense token embeddings to a size of <span translate=no>_^_0_^_</span>.</p>\n": "<h3>\ud83d\udea7 \u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u7528\u306e\u30ea\u30cb\u30a2\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3</h3>\n<p>\u5bc6\u5ea6\u306e\u9ad8\u3044\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u3001\u306e\u30b5\u30a4\u30ba\u306b\u5408\u308f\u305b\u3066\u7dda\u5f62\u6295\u5f71\u3057\u307e\u3059\u3002<span translate=no>_^_0_^_</span></p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><a href=\".. / feed_forward.html\">Position wise feed-forward layers</a> </p>\n": "<p><a href=\".. / feed_forward.html\">\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u30ec\u30a4\u30e4\u30fc</a></p>\n",
 "<p><a href=\"../mha.html\">Multi-head attention layer</a> </p>\n": "<p><a href=\"../mha.html\">\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc</a></p>\n",
 "<p><a href=\"../utils.html\">Subsequent mask</a>, will mask out tokens from seeing future tokens </p>\n": "<p><a href=\"../utils.html\">\u6b21\u306b\u30de\u30b9\u30af\u3059\u308b\u3068</a>\u3001\u30c8\u30fc\u30af\u30f3\u304c\u30de\u30b9\u30af\u3055\u308c\u3001\u5c06\u6765\u306e\u30c8\u30fc\u30af\u30f3\u304c\u898b\u3048\u306a\u304f\u306a\u308a\u307e\u3059</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Auto-regressive mask </p>\n": "<p>\u81ea\u52d5\u56de\u5e30\u30de\u30b9\u30af</p>\n",
 "<p>Autoregressive mask </p>\n": "<p>\u81ea\u5df1\u56de\u5e30\u30de\u30b9\u30af</p>\n",
 "<p>Average pooling layer </p>\n": "<p>\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64</p>\n",
 "<p>Center transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30bb\u30f3\u30bf\u30fc\u30c8\u30e9\u30f3\u30b9\u5c64 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Concatenate the zeros and truncate the right </p>\n": "<p>0 \u3092\u9023\u7d50\u3057\u3066\u53f3\u3092\u5207\u308a\u6368\u3066\u308b</p>\n",
 "<p>Create a mask if we haven&#x27;t created or sizes have changed </p>\n": "<p>\u307e\u3060\u4f5c\u6210\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u3084\u30b5\u30a4\u30ba\u304c\u5909\u66f4\u3055\u308c\u305f\u5834\u5408\u306f\u30de\u30b9\u30af\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Final transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6700\u7d42\u5909\u5727\u5668\u5c64 <span translate=no>_^_0_^_</span></p>\n",
 "<p>If the shift is <span translate=no>_^_0_^_</span> return the original </p>\n": "<p>\u30b7\u30d5\u30c8\u304c\u306e\u5834\u5408\u3001<span translate=no>_^_0_^_</span>\u5143\u306e\u72b6\u614b\u306b\u623b\u3059</p>\n",
 "<p>If there are no more shortening (middle of the hourglass) </p>\n": "<p>\u30b7\u30e7\u30fc\u30c8\u30cb\u30f3\u30b0\u304c\u306a\u304f\u306a\u3063\u305f\u3089 (\u7802\u6642\u8a08\u306e\u771f\u3093\u4e2d)</p>\n",
 "<p>If we are at the center of the hourglass, <span translate=no>_^_0_^_</span> </p>\n": "<p>\u7802\u6642\u8a08\u306e\u4e2d\u5fc3\u306b\u3044\u308b\u3068 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Initial transformer layer <span translate=no>_^_0_^_</span> </p>\n": "<p>\u521d\u671f\u5909\u5727\u5668\u5c64 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Insert another hourglass model recursively </p>\n": "<p>\u5225\u306e\u7802\u6642\u8a08\u30e2\u30c7\u30eb\u3092\u518d\u5e30\u7684\u306b\u633f\u5165</p>\n",
 "<p>Pooling layer accepts shape <span translate=no>_^_0_^_</span> so we permute axes. </p>\n": "<p><span translate=no>_^_0_^_</span>\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306f\u5f62\u72b6\u3092\u53d7\u3051\u5165\u308c\u308b\u306e\u3067\u3001\u8ef8\u3092\u4e26\u3079\u66ff\u3048\u307e\u3059\u3002</p>\n",
 "<p>Repeat across the sequence dimension </p>\n": "<p>\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3\u5168\u4f53\u3067\u7e70\u308a\u8fd4\u3057\u307e\u3059</p>\n",
 "<p>Shifting and shortening <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b7\u30d5\u30c8\u3068\u30b7\u30e7\u30fc\u30c8\u30cb\u30f3\u30b0 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Shortening or the down-sampling layer. We use the simplest form - average pooling. The paper shows that attention based down sampling works best, which we haven&#x27;t implemented yet. </p>\n": "<p>\u30b7\u30e7\u30fc\u30c8\u30cb\u30f3\u30b0\u307e\u305f\u306f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc\u3002\u6700\u3082\u5358\u7d14\u306a\u5f62\u5f0f\u3001\u3064\u307e\u308a\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u6ce8\u610f\u306b\u57fa\u3065\u304f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u304c\u6700\u3082\u52b9\u679c\u7684\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u307e\u3060\u5b9f\u88c5\u3057\u3066\u3044\u307e\u305b\u3093\u3002</p>\n",
 "<p>The center layer is another transformer layer </p>\n": "<p>\u4e2d\u592e\u306e\u5c64\u306f\u5225\u306e\u5909\u5727\u5668\u5c64\u3067\u3059</p>\n",
 "<p>The final transformer layer after up-sampling </p>\n": "<p>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u5f8c\u306e\u6700\u5f8c\u306e\u30c8\u30e9\u30f3\u30b9\u5c64</p>\n",
 "<p>The shortening factor <span translate=no>_^_0_^_</span> (or the down-sampling rate) </p>\n": "<p>\u77ed\u7e2e\u4fc2\u6570 <span translate=no>_^_0_^_</span> (\u307e\u305f\u306f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30fc\u30c8)</p>\n",
 "<p>The transformer layer before down-sampling </p>\n": "<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u524d\u306e\u30c8\u30e9\u30f3\u30b9\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Truncate the extra embeddings at the end </p>\n": "<p>\u6700\u5f8c\u306e\u4f59\u5206\u306a\u57cb\u3081\u8fbc\u307f\u306f\u5207\u308a\u6368\u3066\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Up-sample the shortened sequence and add a skip connection <span translate=no>_^_0_^_</span> </p>\n": "<p>\u77ed\u7e2e\u3055\u308c\u305f\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3001\u30b9\u30ad\u30c3\u30d7\u63a5\u7d9a\u3092\u8ffd\u52a0\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Up-sampling layer. We use naive up-sampling for simplicity and the paper shows attention based up sampling works better. </p>\n": "<p>\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ec\u30a4\u30e4\u30fc\u3002\u7c21\u7565\u5316\u306e\u305f\u3081\u306b\u30ca\u30a4\u30fc\u30d6\u306a\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3092\u4f7f\u7528\u3057\u3066\u304a\u308a\u3001\u8ad6\u6587\u3067\u306f\u6ce8\u610f\u306b\u57fa\u3065\u304f\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u65b9\u304c\u52b9\u679c\u7684\u3067\u3042\u308b\u3053\u3068\u304c\u793a\u3055\u308c\u3066\u3044\u307e\u3059</p>\u3002\n",
 "<p>We shift the tokens to the right by <span translate=no>_^_0_^_</span> steps to make sure information doesn&#x27;t leak from the future tokens to past tokens as a result of down-sampling and up-sampling </p>\n": "<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3068\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u7d50\u679c\u3001\u5c06\u6765\u306e\u30c8\u30fc\u30af\u30f3\u304b\u3089\u904e\u53bb\u306e\u30c8\u30fc\u30af\u30f3\u306b\u60c5\u5831\u304c\u6f0f\u308c\u306a\u3044\u3088\u3046\u306b\u3001<span translate=no>_^_0_^_</span>\u30c8\u30fc\u30af\u30f3\u3092\u6bb5\u968e\u7684\u306b\u53f3\u306b\u30b7\u30d5\u30c8\u3057\u307e\u3059\u3002</p>\n",
 "<p>Zeros to be appended to the left </p>\n": "<p>\u5de6\u306b\u30bc\u30ed\u3092\u8ffd\u52a0</p>\n",
 "<p>cannot be negative </p>\n": "<p>\u8ca0\u306e\u5024\u306b\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is a tensor of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059 <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u5f62\u304c\u5408\u3063\u3066\u3044\u308b <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of heads in <a href=\"../mha.html\">multi-head attention layers</a> </li>\n<li><span translate=no>_^_1_^_</span> is the size of the token embeddings </li>\n<li><span translate=no>_^_2_^_</span> is the dropout probability </li>\n<li><span translate=no>_^_3_^_</span> is the dimensionality of the hidden layer in <a href=\"../feed_forward.html\">position-wise feed-forward layers</a> </li>\n<li><span translate=no>_^_4_^_</span> is the list of shortening factors</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><a href=\"../mha.html\">\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30fb\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30fb\u30ec\u30a4\u30e4\u30fc\u5185\u306e\u30d8\u30c3\u30c9\u6570\u3067\u3059</a></li>\n<li><span translate=no>_^_1_^_</span>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u306e\u30b5\u30a4\u30ba\u3067\u3059</li>\n<li><span translate=no>_^_2_^_</span>\u306f\u8131\u843d\u78ba\u7387\u3067\u3059</li>\n<li><span translate=no>_^_3_^_</span><a href=\"../feed_forward.html\">\u4f4d\u7f6e\u3054\u3068\u306e\u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\u5c64\u306b\u304a\u3051\u308b\u96a0\u308c\u5c64\u306e\u6b21\u5143\u3067\u3059</a></li>\n<li><span translate=no>_^_4_^_</span>\u77ed\u7e2e\u4fc2\u6570\u306e\u30ea\u30b9\u30c8\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of steps to shift by</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u30b7\u30d5\u30c8\u3059\u308b\u30b9\u30c6\u30c3\u30d7\u6570\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the shortening factor</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f\u77ed\u7e2e\u4fc2\u6570</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the tensor with embeddings before down-sampling </li>\n<li><span translate=no>_^_1_^_</span> is the tensor of higher density (to be up-sampled) representations</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u524d\u306e\u57cb\u3081\u8fbc\u307f\u3092\u542b\u3080\u30c6\u30f3\u30bd\u30eb\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span>\u3088\u308a\u9ad8\u3044\u5bc6\u5ea6\u306e (\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u5bfe\u8c61\u306e) \u8868\u73fe\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059</li></ul>\n",
 "Hierarchical Transformers Are More Efficient Language Models": "\u968e\u5c64\u578b\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306f\u3088\u308a\u52b9\u7387\u7684\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb",
 "This is an annotated implementation/tutorial of hourglass model in PyTorch.": "\u3053\u308c\u306fPyTorch\u306e\u7802\u6642\u8a08\u30e2\u30c7\u30eb\u306e\u6ce8\u91c8\u4ed8\u304d\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002"
}