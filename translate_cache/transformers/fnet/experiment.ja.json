{
 "<h1><a href=\"index.html\">FNet</a> Experiment</h1>\n<p>This is an annotated PyTorch experiment to train a <a href=\"index.html\">FNet model</a>.</p>\n<p>This is based on <a href=\"../../experiments/nlp_classification.html\">general training loop and configurations for AG News classification task</a>.</p>\n": "<h1><a href=\"index.html\">FNet \u5b9f\u9a13</a></h1>\n<p><a href=\"index.html\">\u3053\u308c\u306f\u3001FNet\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u6ce8\u91c8\u4ed8\u304d\u306ePyTorch\u5b9f\u9a13\u3067\u3059\u3002</a></p>\n<p>\u3053\u308c\u306f\u3001<a href=\"../../experiments/nlp_classification.html\">AG News\u5206\u985e\u30bf\u30b9\u30af\u306e\u4e00\u822c\u7684\u306a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30eb\u30fc\u30d7\u3068\u69cb\u6210\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059</a>\u3002</p>\n",
 "<h1>Transformer based classifier model</h1>\n": "<h1>\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u30d9\u30fc\u30b9\u306e\u5206\u985e\u5668\u30e2\u30c7\u30eb</h1>\n",
 "<h2>Configurations</h2>\n<p>This inherits from <a href=\"../../experiments/nlp_classification.html\"><span translate=no>_^_0_^_</span></a></p>\n": "<h2>\u30b3\u30f3\u30d5\u30a3\u30ae\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</h2>\n<p>\u3053\u308c\u306f\u4ee5\u4e0b\u304b\u3089\u7d99\u627f\u3055\u308c\u307e\u3059 <a href=\"../../experiments/nlp_classification.html\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h3>Transformer configurations</h3>\n": "<h3>\u5909\u5727\u5668\u69cb\u6210</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Create <span translate=no>_^_0_^_</span> module that can replace the self-attention in <a href=\"../models.html#TransformerLayer\">transformer encoder layer</a> .</p>\n": "<p><span translate=no>_^_0_^_</span><a href=\"../models.html#TransformerLayer\">\u30c8\u30e9\u30f3\u30b9\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\u306e\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306b\u4ee3\u308f\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044</a>\u3002</p>\n",
 "<p> Create classification model</p>\n": "<p>\u5206\u985e\u30e2\u30c7\u30eb\u306e\u4f5c\u6210</p>\n",
 "<p>Classification model </p>\n": "<p>\u5206\u985e\u30e2\u30c7\u30eb</p>\n",
 "<p>Create configs </p>\n": "<p>\u30b3\u30f3\u30d5\u30a3\u30b0\u306e\u4f5c\u6210</p>\n",
 "<p>Create experiment </p>\n": "<p>\u5b9f\u9a13\u3092\u4f5c\u6210</p>\n",
 "<p>Get logits for classification.</p>\n<p>We set the <span translate=no>_^_0_^_</span> token at the last position of the sequence. This is extracted by <span translate=no>_^_1_^_</span>, where <span translate=no>_^_2_^_</span> is of shape <span translate=no>_^_3_^_</span> </p>\n": "<p>\u5206\u985e\u7528\u306e\u30ed\u30b8\u30c3\u30c8\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p>\n<p><span translate=no>_^_0_^_</span>\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u6700\u5f8c\u306e\u4f4d\u7f6e\u306b\u30c8\u30fc\u30af\u30f3\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u5f62\u72b6\u304c\u3069\u3053\u306b\u3042\u308b\u304b\u306b\u3088\u3063\u3066\u62bd\u51fa\u3055\u308c\u307e\u3059 <span translate=no>_^_3_^_</span></p>\n",
 "<p>Get the token embeddings with positional encodings </p>\n": "<p>\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3088\u308b\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u53d6\u5f97</p>\n",
 "<p>Override configurations </p>\n": "<p>\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u8a2d\u5b9a</p>\n",
 "<p>Return results (second value is for state, since our trainer is used with RNNs also) </p>\n": "<p>\u7d50\u679c\u3092\u8fd4\u3057\u307e\u3059\uff08\u30c8\u30ec\u30fc\u30ca\u30fc\u306fRNN\u3067\u3082\u4f7f\u7528\u3055\u308c\u308b\u305f\u3081\u30012\u756a\u76ee\u306e\u5024\u306f\u72b6\u614b\u7528\u3067\u3059\uff09</p>\n",
 "<p>Run training </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u884c</p>\n",
 "<p>Set models for saving and loading </p>\n": "<p>\u4fdd\u5b58\u304a\u3088\u3073\u8aad\u307f\u8fbc\u307f\u7528\u306e\u30e2\u30c7\u30eb\u3092\u8a2d\u5b9a\u3059\u308b</p>\n",
 "<p>Set the vocabulary sizes for embeddings and generating logits </p>\n": "<p>\u57cb\u3081\u8fbc\u307f\u3084\u30ed\u30b8\u30c3\u30c8\u306e\u751f\u6210\u306b\u4f7f\u7528\u3059\u308b\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u30fc\u30b5\u30a4\u30ba\u3092\u8a2d\u5b9a</p>\n",
 "<p>Start the experiment </p>\n": "<p>\u5b9f\u9a13\u3092\u59cb\u3081\u308b</p>\n",
 "<p>Switch between training and validation for <span translate=no>_^_0_^_</span> times per epoch </p>\n": "<p>\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3068\u691c\u8a3c\u3092\u5207\u308a\u66ff\u3048\u308b <span translate=no>_^_0_^_</span></p>\n",
 "<p>Train for <span translate=no>_^_0_^_</span> epochs </p>\n": "<p><span translate=no>_^_0_^_</span>\u6642\u4ee3\u306b\u5408\u308f\u305b\u305f\u5217\u8eca</p>\n",
 "<p>Transformer </p>\n": "<p>\u5909\u5727\u5668</p>\n",
 "<p>Transformer configurations (same as defaults) </p>\n": "<p>\u5909\u5727\u5668\u69cb\u6210 (\u30c7\u30d5\u30a9\u30eb\u30c8\u3068\u540c\u3058)</p>\n",
 "<p>Transformer encoder </p>\n": "<p>\u30c8\u30e9\u30f3\u30b9\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc</p>\n",
 "<p>Use <a href=\"../../optimizers/noam.html\">Noam optimizer</a> </p>\n": "<p><a href=\"../../optimizers/noam.html\">Noam</a> \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3092\u4f7f\u3046</p>\n",
 "<p>Use <a href=\"index.html\">FNet</a> instead of self-a ttention </p>\n": "<p><a href=\"index.html\">\u81ea\u5df1\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u4ee3\u308f\u308a\u306bFNet\u3092\u4f7f\u3046</a></p>\n",
 "<p>Use world level tokenizer </p>\n": "<p>\u30ef\u30fc\u30eb\u30c9\u30ec\u30d9\u30eb\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u4f7f\u3046</p>\n",
 "<p>We use our <a href=\"../configs.html#TransformerConfigs\">configurable transformer implementation</a> </p>\n": "<p><a href=\"../configs.html#TransformerConfigs\">\u8a2d\u5b9a\u53ef\u80fd\u306a\u30c8\u30e9\u30f3\u30b9\u5b9f\u88c5\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</a></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the transformer <a href=\"../models.html#Encoder\">Encoder</a> </li>\n<li><span translate=no>_^_1_^_</span> is the token <a href=\"../models.html#EmbeddingsWithLearnedPositionalEncoding\">embedding module (with positional encodings)</a> </li>\n<li><span translate=no>_^_2_^_</span> is the <a href=\"../models.html#Generator\">final fully connected layer</a> that gives the logits.</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><a href=\"../models.html#Encoder\">\u5909\u5727\u5668\u30a8\u30f3\u30b3\u30fc\u30c0\u3067\u3059</a></li>\n<li><span translate=no>_^_1_^_</span><a href=\"../models.html#EmbeddingsWithLearnedPositionalEncoding\">\u306f\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059 (\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4ed8\u304d)</a></li>\n</ul><li><span translate=no>_^_2_^_</span><a href=\"../models.html#Generator\">\u30ed\u30b8\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\u6700\u5f8c\u306e\u5b8c\u5168\u63a5\u7d9a\u5c64\u3067\u3059</a>\u3002</li>\n",
 "FNet Experiment": "FNet \u5b9f\u9a13",
 "This experiment trains a FNet based model on AG News dataset.": "\u3053\u306e\u5b9f\u9a13\u3067\u306f\u3001AG News\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u57fa\u3065\u3044\u3066FNet\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002"
}