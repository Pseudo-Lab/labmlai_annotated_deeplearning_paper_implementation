{
 "<h1>FNet: Mixing Tokens with Fourier Transforms</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2105.03824\">FNet: Mixing Tokens with Fourier Transforms</a>.</p>\n<p>This paper replaces the <a href=\"../mha.html\">self-attention layer</a> with two <a href=\"https://en.wikipedia.org/wiki/Discrete_Fourier_transform\">Fourier transforms</a> to <em>mix</em> tokens. This is a <span translate=no>_^_0_^_</span> more efficient than self-attention. The accuracy loss of using this over self-attention is about 92% for <a href=\"https://paperswithcode.com/method/bert\">BERT</a> on <a href=\"https://paperswithcode.com/dataset/glue\">GLUE benchmark</a>.</p>\n<h2>Mixing tokens with two Fourier transforms</h2>\n<p>We apply Fourier transform along the hidden dimension (embedding dimension)  and then along the sequence dimension.</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>where <span translate=no>_^_2_^_</span> is the embedding input, <span translate=no>_^_3_^_</span> stands for the fourier transform and <span translate=no>_^_4_^_</span> stands for the real component in complex numbers.</p>\n<p>This is very simple to implement on PyTorch - just 1 line of code. The paper suggests using a precomputed DFT matrix and doing matrix multiplication to get the Fourier transformation.</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for using a FNet based model for classifying <a href=\"https://paperswithcode.com/dataset/ag-news\">AG News</a>.</p>\n": "<h1>FNet: \u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306b\u3088\u308b\u30c8\u30fc\u30af\u30f3\u306e\u6df7\u5408</h1>\n<p>\u3053\u308c\u306f\u3001\u8ad6\u6587\u300c<a href=\"https://papers.labml.ai/paper/2105.03824\">FNet: \u30c8\u30fc\u30af\u30f3\u3092\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3068\u6df7\u5408\u3059\u308b\u300d<a href=\"https://pytorch.org\">\u3092PyTorch\u3067\u5b9f\u88c5\u3057\u305f\u3082\u306e\u3067\u3059</a></a>\u3002</p>\n<p><em>\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001<a href=\"../mha.html\"><a href=\"https://en.wikipedia.org/wiki/Discrete_Fourier_transform\">\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\u30922\u3064\u306e\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306b\u7f6e\u304d\u63db\u3048\u3066\u30c8\u30fc\u30af\u30f3\u3092\u6df7\u5408\u3057\u307e\u3059</a></a>\u3002</em><span translate=no>_^_0_^_</span>\u3053\u308c\u306f\u81ea\u5df1\u6ce8\u610f\u3088\u308a\u3082\u52b9\u7387\u7684\u3067\u3059\u3002<a href=\"https://paperswithcode.com/method/bert\">BERT</a> on <a href=\"https://paperswithcode.com/dataset/glue\">GLUE</a> \u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3067\u306f\u3001\u81ea\u5df1\u6ce8\u610f\u3088\u308a\u3082\u3053\u308c\u3092\u4f7f\u7528\u3057\u305f\u5834\u5408\u306e\u7cbe\u5ea6\u306e\u4f4e\u4e0b\u306f\u7d04 92%</p> \u3067\u3059\u3002\n<h2>2 \u3064\u306e\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306b\u3088\u308b\u30c8\u30fc\u30af\u30f3\u306e\u6df7\u5408</h2>\n<p>\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3092\u975e\u8868\u793a\u6b21\u5143 (\u57cb\u3081\u8fbc\u307f\u6b21\u5143) \u306b\u6cbf\u3063\u3066\u9069\u7528\u3057\u3001\u6b21\u306b\u30b7\u30fc\u30b1\u30f3\u30b9\u6b21\u5143\u306b\u6cbf\u3063\u3066\u9069\u7528\u3057\u307e\u3059\u3002</p>\n<p><span translate=no>_^_1_^_</span></p>\n<p>\u3053\u3053\u3067\u3001<span translate=no>_^_2_^_</span>\u306f\u57cb\u3081\u8fbc\u307f\u5165\u529b\u3067\u3001<span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3092\u8868\u3057\u3001\u8907\u7d20\u6570\u306e\u5b9f\u6570\u6210\u5206\u3092\u8868\u3057\u307e\u3059\u3002</p>\n<p>\u3053\u308c\u3092PyTorch\u306b\u5b9f\u88c5\u3059\u308b\u306e\u306f\u3068\u3066\u3082\u7c21\u5358\u3067\u3059\u3002\u305f\u3063\u305f1\u884c\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\u3053\u306e\u8ad6\u6587\u3067\u306f\u3001\u4e8b\u524d\u306b\u8a08\u7b97\u3055\u308c\u305fDFT\u884c\u5217\u3092\u4f7f\u7528\u3057\u3001\u884c\u5217\u306e\u4e57\u7b97\u3092\u884c\u3063\u3066\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3092\u884c\u3046\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059</p>\u3002\n<p>\u4ee5\u4e0b\u306f<a href=\"experiment.html\">\u3001<a href=\"https://paperswithcode.com/dataset/ag-news\">FNet\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066AG</a> News\u3092\u5206\u985e\u3059\u308b\u305f\u3081\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n",
 "<h2>FNet - Mix tokens</h2>\n<p>This module simply implements <span translate=no>_^_0_^_</span></p>\n<p>The structure of this module is made similar to a <a href=\"../mha.html\">standard attention module</a> so that we can simply replace it.</p>\n": "<h2>FNet-\u30df\u30c3\u30af\u30b9\u30c8\u30fc\u30af\u30f3</h2>\n<p>\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306f\u5358\u7d14\u306b\u5b9f\u88c5\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n<p>\u3053\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u69cb\u9020\u306f\u3001<a href=\"../mha.html\">\u6a19\u6e96\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u540c\u69d8\u306e\u69cb\u9020\u306b\u306a\u3063\u3066\u3044\u308b\u305f\u3081</a>\u3001\u7c21\u5358\u306b\u4ea4\u63db\u3067\u304d\u307e\u3059\u3002</p>\n",
 "<p> The <a href=\"../mha.html\">normal attention module</a> can be fed with different token embeddings for <span translate=no>_^_0_^_</span>,<span translate=no>_^_1_^_</span>, and <span translate=no>_^_2_^_</span> and a mask.</p>\n<p>We follow the same function signature so that we can replace it directly.</p>\n<p>For FNet mixing, <span translate=no>_^_3_^_</span> and masking is not possible. Shape of <span translate=no>_^_4_^_</span> (and <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span>) is <span translate=no>_^_7_^_</span>.</p>\n": "<p><a href=\"../mha.html\">\u901a\u5e38\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u306f</a>\u3001\u3001\u3001<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u30de\u30b9\u30af\u306b\u3055\u307e\u3056\u307e\u306a\u30c8\u30fc\u30af\u30f3\u3092\u57cb\u3081\u8fbc\u3080\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002<span translate=no>_^_2_^_</span></p>\n<p>\u540c\u3058\u95a2\u6570\u30b7\u30b0\u30cd\u30c1\u30e3\u306b\u5f93\u3046\u306e\u3067\u3001\u76f4\u63a5\u7f6e\u63db\u3067\u304d\u307e\u3059\u3002</p>\n<p>FNet\u30df\u30ad\u30b7\u30f3\u30b0\u306e\u5834\u5408<span translate=no>_^_3_^_</span>\u3001\u30de\u30b9\u30ad\u30f3\u30b0\u306f\u3067\u304d\u307e\u305b\u3093\u3002<span translate=no>_^_4_^_</span>(<span translate=no>_^_5_^_</span>\u3068<span translate=no>_^_6_^_</span>) \u306e\u5f62\u306f\u3067\u3059<span translate=no>_^_7_^_</span>\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span>,<span translate=no>_^_1_^_</span>, and <span translate=no>_^_2_^_</span> all should be equal to <span translate=no>_^_3_^_</span> for token mixing </p>\n": "<p><span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u3001<span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u305d\u3057\u3066\u30c8\u30fc\u30af\u30f3\u306e\u30df\u30ad\u30b7\u30f3\u30b0\u3067\u306f\u3059\u3079\u3066\u304c\u7b49\u3057\u304f\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093</p>\n",
 "<p>Apply the Fourier transform along the hidden (embedding) dimension <span translate=no>_^_0_^_</span></p>\n<p>The output of the Fourier transform is a tensor of <a href=\"https://pytorch.org/docs/stable/complex_numbers.html\">complex numbers</a>. </p>\n": "<p>\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3092\u975e\u8868\u793a (\u57cb\u3081\u8fbc\u307f) \u6b21\u5143\u306b\u6cbf\u3063\u3066\u9069\u7528\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n<p><a href=\"https://pytorch.org/docs/stable/complex_numbers.html\">\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306e\u51fa\u529b\u306f\u8907\u7d20\u6570\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059</a>\u3002</p>\n",
 "<p>Apply the Fourier transform along the sequence dimension <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u6b21\u5143\u306b\u6cbf\u3063\u3066\u30d5\u30fc\u30ea\u30a8\u5909\u63db\u3092\u9069\u7528\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Assign to <span translate=no>_^_0_^_</span> for clarity </p>\n": "<p><span translate=no>_^_0_^_</span>\u308f\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u5272\u308a\u5f53\u3066\u308b</p>\n",
 "<p>Get the real component <span translate=no>_^_0_^_</span> </p>\n": "<p>\u672c\u7269\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u624b\u306b\u5165\u308c\u3088\u3046 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Token mixing doesn&#x27;t support masking. i.e. all tokens will see all other token embeddings. </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u306e\u30df\u30ad\u30b7\u30f3\u30b0\u306f\u30de\u30b9\u30ad\u30f3\u30b0\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u305b\u3093\u3002\u3064\u307e\u308a\u3001\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u306b\u4ed6\u306e\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p>\n",
 "FNet: Mixing Tokens with Fourier Transforms": "FNet: \u30d5\u30fc\u30ea\u30a8\u5909\u63db\u306b\u3088\u308b\u30c8\u30fc\u30af\u30f3\u306e\u6df7\u5408",
 "This is an annotated implementation/tutorial of FNet in PyTorch.": "\u3053\u308c\u306f PyTorch \u306b\u304a\u3051\u308b FNet \u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u4ed8\u304d\u306e\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u3059\u3002"
}