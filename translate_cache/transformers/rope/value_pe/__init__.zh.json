{
 "<h2>Multi-head attention with rotary positional embeddings</h2>\n<p>We override <a href=\"../mha.html\">multi-head attention from original transformer</a>.</p>\n": "<h2>\u901a\u8fc7\u65cb\u8f6c\u5b9a\u4f4d\u5d4c\u5165\u5b9e\u73b0\u591a\u5934\u5173\u6ce8</h2>\n<p>\u6211\u4eec\u8d85\u8d8a\u4e86<a href=\"../mha.html\">\u539f\u88c5\u53d8\u538b\u5668\u7684\u591a\u5934\u6ce8\u610f\u529b</a>\u3002</p>\n",
 "<h2>RoPE module that rotates in the opposite direction</h2>\n<p>This inherits from <a href=\"../index.html\">RoPE rotation implementation</a> and changes the direction.</p>\n": "<h2>\u5411\u76f8\u53cd\u65b9\u5411\u65cb\u8f6c\u7684\u7ef3\u7d22\u6a21\u5757</h2>\n<p>\u8fd9\u7ee7\u627f\u4e86 Ro <a href=\"../index.html\">Pe \u65cb\u8f6c\u5b9e\u73b0</a>\u5e76\u6539\u53d8\u4e86\u65b9\u5411\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are the tensors that store collection of <em>query</em>, <em>key</em> and <em>value</em> vectors. They have shape <span translate=no>_^_3_^_</span>.</p>\n<p><span translate=no>_^_4_^_</span> has shape <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span> indicates whether for batch <span translate=no>_^_7_^_</span>, query at position <span translate=no>_^_8_^_</span> has access to key-value at position <span translate=no>_^_9_^_</span>.</p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u548c<span translate=no>_^_2_^_</span>\u662f\u5b58\u50a8<em>\u67e5\u8be2</em>\u3001<em>\u952e</em>\u548c<em>\u503c</em>\u5411\u91cf\u96c6\u5408\u7684\u5f20\u91cf\u3002\u5b83\u4eec\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span>\u3002</p>\n<p><span translate=no>_^_4_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_5_^_</span>\u5e76<span translate=no>_^_6_^_</span>\u6307\u793a\u662f\u5426\u4e3a\u6279\u91cf\u67e5\u8be2<span translate=no>_^_7_^_</span>\uff0c\u4f4d\u7f6e\u5904\u7684\u67e5\u8be2<span translate=no>_^_8_^_</span>\u6709\u6743\u8bbf\u95ee\u4f4d\u7f6e\u5904\u7684\u952e\u503c<span translate=no>_^_9_^_</span>\u3002</p>\n",
 "<p><em>RoPER is work by <a href=\"https://twitter.com/gharik\">Georges Harik (@gharik)</a>, and this implementation is based on his original code.</em></p>\n<h1>Rotary Positional Embeddings with Relative distance (RoPER)</h1>\n<p><a href=\"https://papers.labml.ai/paper/2104.09864\">Rotary Positional Embeddings (RoPE)</a> includes relative positions in attention score calculation. However, the embeddings themselves do not get any positional information , <a href=\"https://papers.labml.ai/paper/2c364684b15b11ecac827bce58715ee7\">except what it can get implicitly from causal attention</a>.</p>\n<p>RoPER adds relative positional information explicitly to value embeddings. Specifically, it adds the relative positions of the tokens it paid attention to. We use same rotary positional embeddings to rotate the values in attention, Then, after taking the weighted sum,  we rotate the final in the opposite direction. Which is equivalent to rotating each of the values (before attention) relative to the current position.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> for training a transformer model with RoPER  on an arithmetic addition where we can see significant improvement over RoPE.</p>\n<h3>Relative distances in embeddings</h3>\n<p>For any head, let <span translate=no>_^_0_^_</span> be the attention from position <span translate=no>_^_1_^_</span> to position <span translate=no>_^_2_^_</span>, and <span translate=no>_^_3_^_</span> be the value embeddings at position <span translate=no>_^_4_^_</span>. Let&#x27;s denote individual features as <span translate=no>_^_5_^_</span>.</p>\n<p>Normally, we would take the weight sum of value embeddings</p>\n<p><span translate=no>_^_6_^_</span></p>\n<p>This doesn&#x27;t explicitly add any distance information about the positions <span translate=no>_^_7_^_</span> to final result <span translate=no>_^_8_^_</span>.</p>\n<p>RoPER pairs features like RoPE and transform them. For a pair <span translate=no>_^_9_^_</span> and <span translate=no>_^_10_^_</span> it transforms them by  <span translate=no>_^_11_^_</span>. Let us donate the transformed features with <span translate=no>_^_12_^_</span>. Then it rotates the weighted sum <span translate=no>_^_13_^_</span> in the the reverse direction with  <span translate=no>_^_14_^_</span>. <em>Note the </em><span translate=no>_^_15_^_</span>.</p>\n<p>Note that,</p>\n<span translate=no>_^_16_^_</span><p>Final output after with the transformations is,</p>\n<span translate=no>_^_17_^_</span><p><em>Note that </em><span translate=no>_^_18_^_</span>.</p>\n<p>Let&#x27;s expand the first term <span translate=no>_^_19_^_</span>,</p>\n<span translate=no>_^_20_^_</span><p>Simiarly we can show the second term is equal to,</p>\n<p><span translate=no>_^_21_^_</span></p>\n<p>Which gives,</p>\n<span translate=no>_^_22_^_</span><p>That is, the weighted average of values rotated relative to current position.</p>\n<p><a href=\"arithmetic_experiment.html\">Here&#x27;s an experiment</a> that uses RoPER on an arthmetic addition task.</p>\n": "<p><em>RoPer \u7531 <a href=\"https://twitter.com/gharik\">Georges Harik (@gharik)</a> \u521b\u4f5c\uff0c\u8fd9\u4e2a\u5b9e\u73b0\u57fa\u4e8e\u4ed6\u7684\u539f\u59cb\u4ee3\u7801\u3002</em></p>\n<h1>\u5177\u6709\u76f8\u5bf9\u8ddd\u79bb\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (RoPer)</h1>\n<p><a href=\"https://papers.labml.ai/paper/2104.09864\">\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08ROPE\uff09</a>\u5305\u62ec\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\u4e2d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002\u4f46\u662f\uff0c\u5d4c\u5165\u672c\u8eab\u4e0d\u4f1a\u83b7\u5f97\u4efb\u4f55\u4f4d\u7f6e\u4fe1\u606f\uff0c<a href=\"https://papers.labml.ai/paper/2c364684b15b11ecac827bce58715ee7\">\u9664\u4e86\u5b83\u53ef\u4ee5\u9690\u542b\u5730\u4ece\u56e0\u679c\u5173\u6ce8\u4e2d\u83b7\u5f97</a>\u7684\u4fe1\u606f\u4e4b\u5916\u3002</p>\n<p>RoPer \u5c06\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u663e\u5f0f\u6dfb\u52a0\u5230\u503c\u5d4c\u5165\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u6dfb\u52a0\u4e86\u5b83\u5173\u6ce8\u7684\u4ee3\u5e01\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u6765\u65cb\u8f6c\u6ce8\u610f\u7684\u503c\uff0c\u7136\u540e\uff0c\u5728\u53d6\u52a0\u6743\u603b\u548c\u4e4b\u540e\uff0c\u6211\u4eec\u5411\u76f8\u53cd\u7684\u65b9\u5411\u65cb\u8f6c\u51b3\u8d5b\u3002\u8fd9\u76f8\u5f53\u4e8e\u76f8\u5bf9\u4e8e\u5f53\u524d\u4f4d\u7f6e\u65cb\u8f6c\u6bcf\u4e2a\u503c\uff08\u6ce8\u610f\u4e4b\u524d\uff09\u3002</p>\n<p><a href=\"experiment.html\">\u4ee5\u4e0b\u662f\u4f7f\u7528RoPer\u5728\u7b97\u672f\u52a0\u6cd5\u4e0a\u8bad\u7ec3\u53d8\u538b\u5668\u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e0eRoPe\u76f8\u6bd4\u6709\u4e86\u663e\u8457\u6539\u8fdb\u3002</p>\n<h3>\u5d4c\u5165\u4e2d\u7684\u76f8\u5bf9\u8ddd\u79bb</h3>\n<p>\u5bf9\u4e8e\u4efb\u4f55\u5934\u90e8\uff0c\u8ba9<span translate=no>_^_0_^_</span>\u6ce8\u610f\u529b\u4ece<span translate=no>_^_1_^_</span>\u4e00\u4e2a\u4f4d\u7f6e\u5230\u53e6\u4e00\u4e2a\u4f4d\u7f6e<span translate=no>_^_2_^_</span>\uff0c\u5e76<span translate=no>_^_3_^_</span>\u6210\u4e3a\u4f4d\u7f6e\u4e0a\u7684\u4ef7\u503c\u5d4c\u5165<span translate=no>_^_4_^_</span>\u3002\u8ba9\u6211\u4eec\u5c06\u5355\u4e2a\u529f\u80fd\u8868\u793a\u4e3a<span translate=no>_^_5_^_</span>\u3002</p>\n<p>\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u4f1a\u53d6\u503c\u5d4c\u5165\u7684\u6743\u91cd\u548c</p>\n<p><span translate=no>_^_6_^_</span></p>\n<p>\u8fd9\u4e0d\u4f1a\u5c06\u6709\u5173\u4f4d\u7f6e\u7684\u4efb\u4f55\u8ddd\u79bb\u4fe1\u606f\u660e\u786e<span translate=no>_^_7_^_</span>\u6dfb\u52a0\u5230\u6700\u7ec8\u7ed3\u679c\u4e2d<span translate=no>_^_8_^_</span>\u3002</p>\n<p>RoPer \u5c06\u8bf8\u5982 RoPe \u4e4b\u7c7b\u7684\u529f\u80fd\u914d\u5bf9\u5e76\u8fdb\u884c\u53d8\u6362\u5bf9\u4e8e\u4e00\u5bf9<span translate=no>_^_9_^_</span>\uff0c<span translate=no>_^_10_^_</span>\u5b83\u4f1a\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a<span translate=no>_^_11_^_</span>\u3002\u8ba9\u6211\u4eec\u4e00\u8d77\u6350\u8d60\u53d8\u6362\u540e\u7684\u8981\u7d20<span translate=no>_^_12_^_</span>\u3002\u7136\u540e\uff0c\u5b83\u5c06\u52a0\u6743\u603b\u548c\u6cbf<span translate=no>_^_13_^_</span>\u76f8\u53cd\u7684\u65b9\u5411\u65cb\u8f6c<span translate=no>_^_14_^_</span>\u3002<em>\u6ce8\u610f</em><span translate=no>_^_15_^_</span>\u3002</p>\n<p>\u8bf7\u6ce8\u610f\uff0c</p>\n<span translate=no>_^_16_^_</span><p>\u8f6c\u6362\u540e\u7684\u6700\u7ec8\u8f93\u51fa\u662f\uff0c</p>\n<span translate=no>_^_17_^_</span><p><em>\u8bf7\u6ce8\u610f</em><span translate=no>_^_18_^_</span>\u3002</p>\n<p>\u8ba9\u6211\u4eec\u5c55\u5f00\u7b2c\u4e00\u4e2a\u5b66\u671f<span translate=no>_^_19_^_</span>\uff0c</p>\n<span translate=no>_^_20_^_</span><p>\u540c\u6837\uff0c\u6211\u4eec\u53ef\u4ee5\u663e\u793a\u7b2c\u4e8c\u4e2a\u9879\u7b49\u4e8e\uff0c</p>\n<p><span translate=no>_^_21_^_</span></p>\n<p>\u8fd9\u7ed9\u4e86\uff0c</p>\n<span translate=no>_^_22_^_</span><p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u76f8\u5bf9\u4e8e\u5f53\u524d\u4f4d\u7f6e\u65cb\u8f6c\u7684\u503c\u7684\u52a0\u6743\u5e73\u5747\u503c\u3002</p>\n<p><a href=\"arithmetic_experiment.html\">\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528 RoPer \u6267\u884c\u7b97\u672f\u52a0\u6cd5\u4efb\u52a1\u7684\u5b9e\u9a8c</a>\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u5173\u6ce8\u6309\u952e\u5e8f\u5217\u7ef4\u5ea6<span translate=no>_^_1_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> have shape <span translate=no>_^_3_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u5e76\u4e14<span translate=no>_^_2_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span></p>\n",
 "<p>Apply dropout </p>\n": "<p>\u7533\u8bf7\u9000\u5b66</p>\n",
 "<p>Apply mask </p>\n": "<p>\u6d82\u62b9\u9762\u819c</p>\n",
 "<p>Cache <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> values </p>\n": "<p>\u7f13\u5b58<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span>\u503c</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate</p>\n<span translate=no>_^_0_^_</span><p>for <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8ba1\u7b97</p>\n<span translate=no>_^_0_^_</span><p>\u5bf9\u4e8e<span translate=no>_^_1_^_</span></p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570<span translate=no>_^_0_^_</span>\u3002\u8fd9\u7ed9\u51fa\u4e86\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span>\u3002</p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u8fde\u63a5\u591a\u4e2a\u5934</p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e58\u4ee5\u503c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Output layer </p>\n": "<p>\u8f93\u51fa\u5c42</p>\n",
 "<p>Prepare <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> for attention computation. These will then have shape <span translate=no>_^_3_^_</span>. </p>\n": "<p>\u51c6\u5907<span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u5e76<span translate=no>_^_2_^_</span>\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u7136\u540e\u8fd9\u4e9b\u5c31\u4f1a\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<p>Rotary positional embedding layers </p>\n": "<p>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u5c42</p>\n",
 "<p>Rotate in the opposite direction so that each embedding hold the relative positions </p>\n": "<p>\u5411\u76f8\u53cd\u65b9\u5411\u65cb\u8f6c\uff0c\u4f7f\u6bcf\u4e2a\u5d4c\u5165\u4fdd\u6301\u76f8\u5bf9\u4f4d\u7f6e</p>\n",
 "<p>Rotate value embeddings before taking the weighted sum so that they contain positional information </p>\n": "<p>\u5728\u83b7\u53d6\u52a0\u6743\u603b\u548c\u4e4b\u524d\u65cb\u8f6c\u503c\u5d4c\u5165\uff0c\u4f7f\u5176\u5305\u542b\u4f4d\u7f6e\u4fe1\u606f</p>\n",
 "<p>Save attentions for any other calculations </p>\n": "<p>\u4fdd\u5b58\u4efb\u4f55\u5176\u4ed6\u8ba1\u7b97\u7684\u6ce8\u610f\u529b</p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u97f3\u9636\u5206\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Split the features, we can choose to apply rotary embeddings only to a partial set of features. </p>\n": "<p>\u62c6\u5206\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4ec5\u5c06\u65cb\u8f6c\u5d4c\u5165\u5e94\u7528\u4e8e\u90e8\u5206\u7279\u5f81\u96c6\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the Tensor at the head of a key or a query with shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4f4d\u4e8e\u952e\u6216\u5e26\u6709\u5f62\u72b6\u7684\u67e5\u8be2\u5f00\u5934\u7684 Tensor<span translate=no>_^_1_^_</span></li></ul>\n",
 "Rotary Positional Embeddings with Relative distance (RoPER)": "\u5177\u6709\u76f8\u5bf9\u8ddd\u79bb\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (RoPer)",
 "This is an implementation of RoPER which adds relative distance information to embeddings on top of RoPE introduced in RoFormer: Enhanced Transformer with Rotary Position Embedding": "\u8fd9\u662f RoPer \u7684\u5b9e\u73b0\uff0c\u5b83\u5728 RoFormer\uff1a\u5177\u6709\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u529f\u80fd\u7684\u589e\u5f3a\u578b\u53d8\u538b\u5668\u4e2d\u5f15\u5165\u7684 RoPE \u9876\u90e8\u7684\u5d4c\u5165\u7269\u4e2d\u6dfb\u52a0\u76f8\u5bf9\u8ddd\u79bb\u4fe1\u606f"
}