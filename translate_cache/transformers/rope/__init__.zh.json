{
 "<h1>Rotary Positional Embeddings (RoPE)</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/2104.09864\">Rotary Positional Embeddings (RoPE)</a> in <a href=\"https://pytorch.org\">PyTorch</a>.</p>\n<p>Rotary Positional Embeddings (RoPE) encode position information of tokens with a rotation matrix that naturally incorporates explicit relative position dependency.</p>\n<p>Here&#x27;s <a href=\"experiment.html\">the training code</a> for training a transformer model with RoPE  on Tiny Shakespeare dataset.</p>\n": "<h1>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (RoPE)</h1>\n<p>\u8fd9\u662f PyT <a href=\"https://pytorch.org\">orch \u4e2d\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (RoP</a> <a href=\"https://papers.labml.ai/paper/2104.09864\">E)</a> \u7684\u5b9e\u73b0\u3002</p>\n<p>Rotary Positional Embeddings (RoPE) \u4f7f\u7528\u81ea\u7136\u5305\u542b\u660e\u786e\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56\u5173\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u5bf9\u4ee3\u5e01\u7684\u4f4d\u7f6e\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002</p>\n<p>\u4ee5\u4e0b\u662f<a href=\"experiment.html\">\u5728 Tiny Shakespeare \u6570\u636e\u96c6\u4e0a\u4f7f\u7528 RoPE \u8bad\u7ec3\u53d8\u538b\u5668\u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\u3002</p>\n",
 "<h2>Multi-head attention with rotary positional embeddings</h2>\n<p>We override <a href=\"../mha.html\">multi-head attention from original transformer</a>.</p>\n": "<h2>\u901a\u8fc7\u65cb\u8f6c\u5b9a\u4f4d\u5d4c\u5165\u5b9e\u73b0\u591a\u5934\u5173\u6ce8</h2>\n<p>\u6211\u4eec\u8d85\u8d8a\u4e86<a href=\"../mha.html\">\u539f\u88c5\u53d8\u538b\u5668\u7684\u591a\u5934\u6ce8\u610f\u529b</a>\u3002</p>\n",
 "<h2>RoPE module</h2>\n<p>Rotary encoding transforms pairs of features by rotating in the 2D plane. That is, it organizes the <span translate=no>_^_0_^_</span> features as <span translate=no>_^_1_^_</span> pairs. Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it by an angle depending on the position of the token.</p>\n<h3>For a pair of features</h3>\n<p>Let <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> be two features of the key or query of any head at position <span translate=no>_^_4_^_</span>. Or for simplicity assume <span translate=no>_^_5_^_</span> has only two features. Then the transformation is,</p>\n<span translate=no>_^_6_^_</span><p>where <span translate=no>_^_7_^_</span> is a constant angle. The other pairs of features are transformed similarly.</p>\n<h3>Attention is relative</h3>\n<p>For a pair of features, dot-product attention score between two positions <span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> would be</p>\n<span translate=no>_^_10_^_</span><p>This shows that for dot-production attention the rotary encodings gives relative attention.</p>\n<h3>For all features</h3>\n<p>The features are grouped into pairs and handled as above. They use a different <span translate=no>_^_11_^_</span> for each pair.</p>\n<p>The paper suggests using <span translate=no>_^_12_^_</span> for the <span translate=no>_^_13_^_</span> pairs of features.</p>\n<p>We pair feature <span translate=no>_^_14_^_</span> with feature <span translate=no>_^_15_^_</span>. So for position <span translate=no>_^_16_^_</span> we transform</p>\n<span translate=no>_^_17_^_</span><p>to</p>\n<span translate=no>_^_18_^_</span>": "<h2>\u7ef3\u7d22\u6a21\u5757</h2>\n<p>\u65cb\u8f6c\u7f16\u7801\u901a\u8fc7\u5728 2D \u5e73\u9762\u4e2d\u65cb\u8f6c\u6765\u8f6c\u6362\u6210\u5bf9\u7684\u8981\u7d20\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u5c06<span translate=no>_^_0_^_</span>\u8981\u7d20\u7ec4\u7ec7\u6210<span translate=no>_^_1_^_</span>\u5bf9\u3002\u6bcf\u5bf9\u90fd\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e8c\u7ef4\u5e73\u9762\u4e2d\u7684\u4e00\u4e2a\u5750\u6807\uff0c\u7f16\u7801\u5c06\u6839\u636e\u4ee4\u724c\u7684\u4f4d\u7f6e\u5c06\u5176\u65cb\u8f6c\u4e00\u4e2a\u89d2\u5ea6\u3002</p>\n<h3>\u5bf9\u4e8e\u4e00\u5bf9\u529f\u80fd</h3>\n<p>\u8ba9<span translate=no>_^_2_^_</span>\u548c<span translate=no>_^_3_^_</span>\u6210\u4e3a\u4efb\u4f55\u5934\u90e8\u4f4d\u7f6e\u7684\u952e\u6216\u67e5\u8be2\u7684\u4e24\u4e2a\u7279\u5f81<span translate=no>_^_4_^_</span>\u3002\u6216\u8005\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1<span translate=no>_^_5_^_</span>\uff0c\u5047\u8bbe\u53ea\u6709\u4e24\u4e2a\u529f\u80fd\u3002\u90a3\u4e48\u8f6c\u53d8\u5c31\u662f\uff0c</p>\n<span translate=no>_^_6_^_</span><p>\u5176\u4e2d<span translate=no>_^_7_^_</span>\u662f\u6052\u5b9a\u89d2\u5ea6\u3002\u5176\u4ed6\u8981\u7d20\u5bf9\u7684\u53d8\u6362\u65b9\u5f0f\u7c7b\u4f3c\u3002</p>\n<h3>\u6ce8\u610f\u529b\u662f\u76f8\u5bf9\u7684</h3>\n<p>\u5bf9\u4e8e\u4e00\u5bf9\u529f\u80fd\uff0c\u70b9\u4ea7\u54c1\u6ce8\u610f\u529b\u5206\u6570\u4ecb\u4e8e\u4e24\u4e2a\u4f4d\u7f6e<span translate=no>_^_8_^_</span>\u4e4b\u95f4\uff0c<span translate=no>_^_9_^_</span>\u5c06\u4e3a</p>\n<span translate=no>_^_10_^_</span><p>\u8fd9\u8868\u660e\uff0c\u5bf9\u4e8e\u70b9\u751f\u4ea7\u7684\u5173\u6ce8\uff0c\u65cb\u8f6c\u7f16\u7801\u7ed9\u4e88\u4e86\u76f8\u5bf9\u7684\u5173\u6ce8\u3002</p>\n<h3>\u5bf9\u4e8e\u6240\u6709\u529f\u80fd</h3>\n<p>\u8fd9\u4e9b\u8981\u7d20\u5206\u7ec4\u6210\u5bf9\uff0c\u5e76\u6309\u4e0a\u8ff0\u65b9\u5f0f\u5904\u7406\u3002\u4ed6\u4eec\u5bf9\u6bcf<span translate=no>_^_11_^_</span>\u5bf9\u4f7f\u7528\u4e0d\u540c\u7684\u3002</p>\n<p>\u672c\u6587\u5efa\u8bae\u4f7f\u7528<span translate=no>_^_13_^_</span>\u6210<span translate=no>_^_12_^_</span>\u5bf9\u7684\u7279\u5f81\u3002</p>\n<p>\u6211\u4eec\u5c06\u529f\u80fd<span translate=no>_^_14_^_</span>\u4e0e\u529f\u80fd\u914d\u5bf9<span translate=no>_^_15_^_</span>\u3002\u56e0\u6b64\uff0c\u5bf9\u4e8e\u4f4d\u7f6e<span translate=no>_^_16_^_</span>\u6211\u4eec\u8fdb\u884c\u8f6c\u6362</p>\n<span translate=no>_^_17_^_</span><p>\u81f3</p>\n<span translate=no>_^_18_^_</span>",
 "<h3>Calculate scores between queries and keys</h3>\n": "<h3>\u8ba1\u7b97\u67e5\u8be2\u548c\u952e\u4e4b\u95f4\u7684\u5206\u6570</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Cache <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> values</p>\n": "<p>\u7f13\u5b58<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span>\u503c</p>\n",
 "<p> Testing RoPE with a simple example</p>\n": "<p>\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u6d4b\u8bd5 RoPe</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Cache <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> values </p>\n": "<p>\u7f13\u5b58<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span>\u503c</p>\n",
 "<p>Cache them </p>\n": "<p>\u7f13\u5b58\u5b83\u4eec</p>\n",
 "<p>Calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate dot-product with RoPE </p>\n": "<p>\u4f7f\u7528 ROPE \u8ba1\u7b97\u70b9\u79ef</p>\n",
 "<p>Calculate the product of position index and <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8ba1\u7b97\u6301\u4ed3\u6307\u6570\u7684\u4e58\u79ef\u548c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate</p>\n<span translate=no>_^_0_^_</span><p>for <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8ba1\u7b97</p>\n<span translate=no>_^_0_^_</span><p>\u5bf9\u4e8e<span translate=no>_^_1_^_</span></p>\n",
 "<p>Concatenate so that for row <span translate=no>_^_0_^_</span> we have <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8fde\u63a5\u8fd9\u6837<span translate=no>_^_0_^_</span>\u6211\u4eec\u5c31\u6709 row<span translate=no>_^_1_^_</span></p>\n",
 "<p>Create position indexes <span translate=no>_^_0_^_</span> </p>\n": "<p>\u521b\u5efa\u5934\u5bf8\u6307\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get sequence length </p>\n": "<p>\u83b7\u53d6\u5e8f\u5217\u957f\u5ea6</p>\n",
 "<p>Return if cache is already built </p>\n": "<p>\u5982\u679c\u7f13\u5b58\u5df2\u7ecf\u6784\u5efa\uff0c\u5219\u8fd4\u56de</p>\n",
 "<p>Rotary positional embedding layers </p>\n": "<p>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u5c42</p>\n",
 "<p>Split the features, we can choose to apply rotary embeddings only to a partial set of features. </p>\n": "<p>\u62c6\u5206\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4ec5\u5c06\u65cb\u8f6c\u5d4c\u5165\u5e94\u7528\u4e8e\u90e8\u5206\u7279\u5f81\u96c6\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the Tensor at the head of a key or a query with shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u4f4d\u4e8e\u952e\u6216\u5e26\u6709\u5f62\u72b6\u7684\u67e5\u8be2\u5f00\u5934\u7684 Tensor<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of features <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is the constant used for calculating <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8981\u7d20\u7684\u6570\u91cf<span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u662f\u7528\u4e8e\u8ba1\u7b97\u7684\u5e38\u6570<span translate=no>_^_3_^_</span></li>\n",
 "Annotated implementation of RoPE from paper RoFormer: Enhanced Transformer with Rotary Position Embedding": "Paper RoFormer \u4e2d\u5e26\u6ce8\u91ca\u7684 ROPE \u5b9e\u73b0\uff1a\u5e26\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u7684\u589e\u5f3a\u578b\u53d8\u538b\u5668",
 "Rotary Positional Embeddings (RoPE)": "\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (ROPE)"
}