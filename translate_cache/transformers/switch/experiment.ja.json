{
 "<h1>Switch Transformer Experiment</h1>\n<p>This is an annotated PyTorch experiment to train a switch transformer.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n": "<h1>\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u5b9f\u9a13</h1>\n<p>\u3053\u308c\u306f\u3001\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u305f\u3081\u306e\u6ce8\u91c8\u4ed8\u304dPyTorch\u5b9f\u9a13\u3067\u3059\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<h2>Auto regressive model</h2>\n": "<h2>\u81ea\u52d5\u56de\u5e30\u30e2\u30c7\u30eb</h2>\n",
 "<h2>Configurations</h2>\n<p>This extends <a href=\"../../experiments/nlp_autoregression.html\"><span translate=no>_^_0_^_</span></a>.</p>\n<p>The default configs can and will be over-ridden when we start the experiment</p>\n": "<h2>\u30b3\u30f3\u30d5\u30a3\u30ae\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3</h2>\n<p>\u3053\u308c\u306f\u5e83\u304c\u308a\u307e\u3059\u3002<a href=\"../../experiments/nlp_autoregression.html\"><span translate=no>_^_0_^_</span></a></p>\n<p>\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u8a2d\u5b9a\u306f\u3001\u5b9f\u9a13\u3092\u958b\u59cb\u3057\u305f\u3068\u304d\u306b\u4e0a\u66f8\u304d\u3067\u304d\u3001\u307e\u305f\u4e0a\u66f8\u304d\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<h3>Initialize the auto-regressive model</h3>\n": "<h3>\u81ea\u5df1\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316</h3>\n",
 "<h3>Initialize the switch transformer</h3>\n": "<h3>\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u3092\u521d\u671f\u5316\u3057\u307e\u3059</h3>\n",
 "<h3>Run the experiment</h3>\n": "<h3>\u5b9f\u9a13\u3092\u5b9f\u884c\u3059\u308b</h3>\n",
 "<h3>Training or validation step</h3>\n": "<h3>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u307e\u305f\u306f\u691c\u8a3c\u30b9\u30c6\u30c3\u30d7</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>A dictionary of configurations to override </p>\n": "<p>\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3059\u308b\u8a2d\u5b9a\u306e\u8f9e\u66f8</p>\n",
 "<p>Calculate and cross entropy loss </p>\n": "<p>\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u8a08\u7b97\u3068\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931</p>\n",
 "<p>Calculate and log accuracy </p>\n": "<p>\u7cbe\u5ea6\u306e\u8a08\u7b97\u3068\u8a18\u9332</p>\n",
 "<p>Calculate gradients </p>\n": "<p>\u52fe\u914d\u306e\u8a08\u7b97</p>\n",
 "<p>Capacity factor to determine capacity of each model </p>\n": "<p>\u5404\u30e2\u30c7\u30eb\u306e\u5bb9\u91cf\u3092\u6c7a\u5b9a\u3059\u308b\u5bb9\u91cf\u4fc2\u6570</p>\n",
 "<p>Clear the gradients </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u30af\u30ea\u30a2</p>\n",
 "<p>Clip gradients </p>\n": "<p>\u30af\u30ea\u30c3\u30d7\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3</p>\n",
 "<p>Combined loss. The load balancing loss is multiplied by a coefficient <span translate=no>_^_0_^_</span> which is set to something small like <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u8907\u5408\u640d\u5931\u3002\u8ca0\u8377\u5206\u6563\u640d\u5931\u306b\u306f\u3001<span translate=no>_^_0_^_</span>\u6b21\u306e\u3088\u3046\u306a\u5c0f\u3055\u306a\u5024\u306b\u8a2d\u5b9a\u3055\u308c\u305f\u4fc2\u6570\u304c\u4e57\u7b97\u3055\u308c\u307e\u3059</p>\u3002<span translate=no>_^_1_^_</span>\n",
 "<p>Create configs </p>\n": "<p>\u30b3\u30f3\u30d5\u30a3\u30b0\u306e\u4f5c\u6210</p>\n",
 "<p>Create experiment </p>\n": "<p>\u5b9f\u9a13\u3092\u4f5c\u6210</p>\n",
 "<p>Dropout probability </p>\n": "<p>\u8131\u843d\u78ba\u7387</p>\n",
 "<p>Final layer </p>\n": "<p>\u6700\u7d42\u30ec\u30a4\u30e4\u30fc</p>\n",
 "<p>Fraction of tokens routed to each expert <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is the count of tokens where the argmax of <span translate=no>_^_2_^_</span> is equal to <span translate=no>_^_3_^_</span>. </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5404\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3055\u308c\u308b\u30c8\u30fc\u30af\u30f3\u306e\u5272\u5408\u306f\u3001argmax \u304c\u3068\u7b49\u3057\u3044\u30c8\u30fc\u30af\u30f3\u306e\u6570\u3067\u3059\u3002<span translate=no>_^_2_^_</span> <span translate=no>_^_3_^_</span></p>\n",
 "<p>Generate logits of the next token </p>\n": "<p>\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u306e\u30ed\u30b8\u30c3\u30c8\u3092\u751f\u6210</p>\n",
 "<p>Get model outputs. </p>\n": "<p>\u30e2\u30c7\u30eb\u51fa\u529b\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p>\n",
 "<p>Initialize the subsequent mask </p>\n": "<p>\u5f8c\u7d9a\u306e\u30de\u30b9\u30af\u3092\u521d\u671f\u5316</p>\n",
 "<p>Initialize tracking indicators </p>\n": "<p>\u30c8\u30e9\u30c3\u30ad\u30f3\u30b0\u30a4\u30f3\u30b8\u30b1\u30fc\u30bf\u3092\u521d\u671f\u5316</p>\n",
 "<p>Load balancing coefficient </p>\n": "<p>\u8ca0\u8377\u5206\u6563\u4fc2\u6570</p>\n",
 "<p>Load balancing loss <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is the loss for a single layer and here we are taking the sum of losses across all layers. </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u8ca0\u8377\u5206\u6563\u640d\u5931\u306f\u5358\u4e00\u30ec\u30a4\u30e4\u30fc\u306e\u640d\u5931\u3067\u3042\u308a\u3001\u3053\u3053\u3067\u306f\u3059\u3079\u3066\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u640d\u5931\u306e\u5408\u8a08\u3092\u6c42\u3081\u3066\u3044\u307e\u3059\u3002</p>\n",
 "<p>Load configurations </p>\n": "<p>\u69cb\u6210\u3092\u30ed\u30fc\u30c9</p>\n",
 "<p>Log the model parameters and gradients on last batch of every epoch </p>\n": "<p>\u5404\u30a8\u30dd\u30c3\u30af\u306e\u6700\u5f8c\u306e\u30d0\u30c3\u30c1\u3067\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u52fe\u914d\u3092\u8a18\u9332\u3057\u307e\u3059</p>\n",
 "<p>Mean routing probability <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5e73\u5747\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u78ba\u7387 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Move data to the device </p>\n": "<p>\u30c7\u30fc\u30bf\u3092\u30c7\u30d0\u30a4\u30b9\u306b\u79fb\u52d5</p>\n",
 "<p>Number of attention heads </p>\n": "<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d8\u30c3\u30c9\u306e\u6570</p>\n",
 "<p>Number of experts </p>\n": "<p>\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u6570</p>\n",
 "<p>Number of features in FFN hidden layer </p>\n": "<p>FFN \u96a0\u308c\u30ec\u30a4\u30e4\u30fc\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u6570</p>\n",
 "<p>Number of transformer layers </p>\n": "<p>\u5909\u5727\u5668\u5c64\u306e\u6570</p>\n",
 "<p>Run it through the transformer </p>\n": "<p>\u5909\u5727\u5668\u306b\u901a\u3057\u3066\u304f\u3060\u3055\u3044</p>\n",
 "<p>Save the tracked metrics </p>\n": "<p>\u8ffd\u8de1\u3057\u305f\u30e1\u30c8\u30ea\u30af\u30b9\u3092\u4fdd\u5b58\u3059\u308b</p>\n",
 "<p>Set models for saving and loading </p>\n": "<p>\u4fdd\u5b58\u304a\u3088\u3073\u8aad\u307f\u8fbc\u307f\u7528\u306e\u30e2\u30c7\u30eb\u3092\u8a2d\u5b9a\u3059\u308b</p>\n",
 "<p>Start the experiment </p>\n": "<p>\u5b9f\u9a13\u3092\u59cb\u3081\u308b</p>\n",
 "<p>Take optimizer step </p>\n": "<p>\u6700\u9069\u5316\u306e\u4e00\u6b69\u3092\u8e0f\u307f\u51fa\u3059</p>\n",
 "<p>Token embedding module </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\u30e2\u30b8\u30e5\u30fc\u30eb</p>\n",
 "<p>Token embedding size </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u30b5\u30a4\u30ba</p>\n",
 "<p>Token embeddings </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f</p>\n",
 "<p>Total number of tokens processed, <span translate=no>_^_0_^_</span>, in the current batch <span translate=no>_^_1_^_</span> </p>\n": "<p>\u73fe\u5728\u306e\u30d0\u30c3\u30c1\u3067\u51e6\u7406\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306e\u7dcf\u6570 <span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span></p>\n",
 "<p>Track stats </p>\n": "<p>\u30c8\u30e9\u30c3\u30af\u7d71\u8a08</p>\n",
 "<p>Train the model </p>\n": "<p>\u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0</p>\n",
 "<p>Transformer </p>\n": "<p>\u5909\u5727\u5668</p>\n",
 "<p>Update global step (number of tokens processed) when in training mode </p>\n": "<p>\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30e2\u30fc\u30c9\u6642\u306b\u30b0\u30ed\u30fc\u30d0\u30eb\u30b9\u30c6\u30c3\u30d7 (\u51e6\u7406\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306e\u6570) \u3092\u66f4\u65b0</p>\n",
 "<p>Whether to capture model outputs </p>\n": "<p>\u30e2\u30c7\u30eb\u51fa\u529b\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3059\u308b\u304b\u3069\u3046\u304b</p>\n",
 "<p>Whether to drop tokens </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u3092\u30c9\u30ed\u30c3\u30d7\u3059\u308b\u304b\u3069\u3046\u304b</p>\n",
 "<p>Whether to scale the chosen expert outputs by the routing probability </p>\n": "<p>\u9078\u629e\u3057\u305f\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3092\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u78ba\u7387\u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3059\u308b\u304b\u3069\u3046\u304b</p>\n",
 "Switch Transformer Experiment": "\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u5b9f\u9a13",
 "This experiment trains a small switch transformer on tiny Shakespeare dataset.": "\u3053\u306e\u5b9f\u9a13\u3067\u306f\u3001\u5c0f\u3055\u306a\u30b7\u30a7\u30a4\u30af\u30b9\u30d4\u30a2\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5c0f\u3055\u306a\u30b9\u30a4\u30c3\u30c1\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002"
}