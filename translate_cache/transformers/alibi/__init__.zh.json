{
 "<h1>Attention with Linear Biases (ALiBi)</h1>\n<p>This is an implementation of Attention with Linear Biases (ALiBi) from the paper <a href=\"https://papers.labml.ai/paper/2108.12409\">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>.</p>\n<p>This replaces positional encodings with biases added to attention scores (attention logits, before the softmax). This is a relative scheme tested on autoregressive tasks, and the bias is higher for closeby tokens and lower for far-away tokens. The biases decrease linearly in the log scale (because it&#x27;s before the softmax) and each head has a different slope.</p>\n<p>Here&#x27;s the attention formula for <span translate=no>_^_0_^_</span>-th token,</p>\n<span translate=no>_^_1_^_</span><p>where <span translate=no>_^_2_^_</span> is the query of the <span translate=no>_^_3_^_</span>-th token, <span translate=no>_^_4_^_</span> are the keys up to <span translate=no>_^_5_^_</span>, and <span translate=no>_^_6_^_</span> the number of features per head. Note that the above equality halts because <span translate=no>_^_7_^_</span> is invariant to translations  (you can add any constant to all elements without changing the result).</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for a ALiBi model.</p>\n": "<h1>\u6ce8\u610f\u7ebf\u6027\u504f\u5dee (AliBI)</h1>\n<p>\u8fd9\u662f\u300aT <a href=\"https://papers.labml.ai/paper/2108.12409\">rain Short\uff0cTest Long\uff1a\u4f7f\u7528\u7ebf\u6027\u504f\u5dee\u7684\u6ce8\u610f\u529b\u5b9e\u73b0\u8f93\u5165\u957f\u5ea6\u5916\u63a8\u300b\u4e00\u6587\u4e2d\u7684 \u201c\u4f7f\u7528\u7ebf\u6027\u504f\u5dee\u6ce8\u610f\u529b</a> (AliBI)\u201d \u7684\u5b9e\u73b0\u3002</p>\n<p>\u8fd9\u5c06\u7528\u5728\u6ce8\u610f\u529b\u5206\u6570\u4e2d\u6dfb\u52a0\u504f\u5dee\uff08\u6ce8\u610f\u529b\u5bf9\u6570\uff0c\u5728 softmax \u4e4b\u524d\uff09\u53d6\u4ee3\u4f4d\u7f6e\u7f16\u7801\u3002\u8fd9\u662f\u4e00\u79cd\u5728\u81ea\u56de\u5f52\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u7684\u76f8\u5bf9\u65b9\u6848\uff0ccloseby\u4ee3\u5e01\u7684\u504f\u5dee\u66f4\u9ad8\uff0c\u800c\u9065\u8fdc\u7684\u4ee3\u5e01\u7684\u504f\u5dee\u66f4\u4f4e\u3002\u504f\u5dee\u5728\u5bf9\u6570\u6807\u5ea6\u4e2d\u5448\u7ebf\u6027\u51cf\u5c0f\uff08\u56e0\u4e3a\u5b83\u5728softmax\u4e4b\u524d\uff09\uff0c\u5e76\u4e14\u6bcf\u4e2a\u5934\u90e8\u90fd\u6709\u4e0d\u540c\u7684\u659c\u7387\u3002</p>\n<p>\u8fd9\u662f<span translate=no>_^_0_^_</span>\u7b2c-th \u4ee3\u5e01\u7684\u6ce8\u610f\u529b\u516c\u5f0f\uff0c</p>\n<span translate=no>_^_1_^_</span><p>\u5176\u4e2d\uff0c<span translate=no>_^_2_^_</span>\u662f<span translate=no>_^_3_^_</span>\u7b2c-th \u4e2a\u4ee4\u724c\u7684\u67e5\u8be2\uff0c\u6700\u5927<span translate=no>_^_4_^_</span>\u662f\u5bc6\u94a5\u6570\u4ee5\u53ca<span translate=no>_^_6_^_</span>\u6bcf\u4e2a\u6807\u5934\u7684\u8981<span translate=no>_^_5_^_</span>\u7d20\u6570\u3002\u8bf7\u6ce8\u610f\uff0c\u4e0a\u8ff0\u7b49\u5f0f\u4e4b\u6240\u4ee5\u505c\u6b62\uff0c\u662f\u56e0\u4e3a\u7ffb\u8bd1\u662f\u4e0d\u53d8<span translate=no>_^_7_^_</span>\u7684\uff08\u60a8\u53ef\u4ee5\u5728\u4e0d\u66f4\u6539\u7ed3\u679c\u7684\u60c5\u51b5\u4e0b\u5411\u6240\u6709\u5143\u7d20\u6dfb\u52a0\u4efb\u4f55\u5e38\u91cf\uff09\u3002</p>\n<p><a href=\"experiment.html\">\u4ee5\u4e0b\u662f AliBi \u6a21\u578b\u7684\u8bad\u7ec3\u4ee3\u7801</a>\u3002</p>\n",
 "<h2>Attention with Linear Biases (ALiBi)</h2>\n<p>We override <a href=\"../mha.html\">Multi-Head Attention</a>.</p>\n": "<h2>\u6ce8\u610f\u7ebf\u6027\u504f\u5dee (AliBI)</h2>\n<p>\u6211\u4eec\u8986\u76d6<a href=\"../mha.html\">\u591a\u5934\u6ce8\u610f\u529b</a>\u3002</p>\n",
 "<h2>Calculate the attention biases matrix</h2>\n<ul><li><span translate=no>_^_0_^_</span> is the number of heads in the attention layer </li>\n<li><span translate=no>_^_1_^_</span> is the attention mask of shape <span translate=no>_^_2_^_</span></li></ul>\n<p>This returns a matrix of shape <span translate=no>_^_3_^_</span> with ALiBi attention biases.</p>\n": "<h2>\u8ba1\u7b97\u6ce8\u610f\u529b\u504f\u5dee\u77e9\u9635</h2>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u6ce8\u610f\u5c42\u4e2d\u7684\u5934\u90e8\u6570\u91cf</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u5f62\u72b6\u7684\u6ce8\u610f\u529b\u9762\u5177<span translate=no>_^_2_^_</span></li></ul>\n<p>\u8fd9\u5c06\u8fd4\u56de\u4e00\u4e2a<span translate=no>_^_3_^_</span>\u5177\u6709 AliBi \u6ce8\u610f\u529b\u504f\u5dee\u7684\u5f62\u72b6\u77e9\u9635\u3002</p>\n",
 "<h2>Get head-specific slope <span translate=no>_^_0_^_</span> for each head</h2>\n<ul><li><span translate=no>_^_1_^_</span> is the number of heads in the attention layer <span translate=no>_^_2_^_</span></li></ul>\n<p>The slope for first head is</p>\n<p><span translate=no>_^_3_^_</span></p>\n<p>The slopes for the rest of the heads are in a geometric series with a ratio same as above.</p>\n<p>For instance when the number of heads is <span translate=no>_^_4_^_</span> the slopes are <span translate=no>_^_5_^_</span></p>\n": "<h2><span translate=no>_^_0_^_</span>\u4e3a\u6bcf\u4e2a\u5934\u90e8\u83b7\u53d6\u7279\u5b9a\u4e8e\u5934\u90e8\u7684\u659c\u7387</h2>\n<ul><li><span translate=no>_^_1_^_</span>\u662f\u6ce8\u610f\u5c42\u4e2d\u7684\u5934\u90e8\u6570\u91cf<span translate=no>_^_2_^_</span></li></ul>\n<p>\u7b2c\u4e00\u4e2a\u5934\u7684\u659c\u7387\u662f</p>\n<p><span translate=no>_^_3_^_</span></p>\n<p>\u5176\u4f59\u5934\u90e8\u7684\u659c\u7387\u4e3a\u51e0\u4f55\u5e8f\u5217\uff0c\u5176\u6bd4\u4f8b\u4e0e\u4e0a\u9762\u76f8\u540c\u3002</p>\n<p>\u4f8b\u5982\uff0c\u5f53\u5934\u6570\u4e3a\u65f6<span translate=no>_^_4_^_</span>\uff0c\u659c\u7387\u4e3a<span translate=no>_^_5_^_</span></p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are the tensors that store collection of <em>query</em>, <em>key</em> and <em>value</em> vectors. They have shape <span translate=no>_^_3_^_</span>.</p>\n<p><span translate=no>_^_4_^_</span> has shape <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span> indicates whether for batch <span translate=no>_^_7_^_</span>, query at position <span translate=no>_^_8_^_</span> has access to key-value at position <span translate=no>_^_9_^_</span>.</p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u548c<span translate=no>_^_2_^_</span>\u662f\u5b58\u50a8<em>\u67e5\u8be2</em>\u3001<em>\u952e</em>\u548c<em>\u503c</em>\u5411\u91cf\u96c6\u5408\u7684\u5f20\u91cf\u3002\u5b83\u4eec\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span>\u3002</p>\n<p><span translate=no>_^_4_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_5_^_</span>\u5e76<span translate=no>_^_6_^_</span>\u6307\u793a\u662f\u5426\u4e3a\u6279\u91cf\u67e5\u8be2<span translate=no>_^_7_^_</span>\uff0c\u4f4d\u7f6e\u5904\u7684\u67e5\u8be2<span translate=no>_^_8_^_</span>\u6709\u6743\u8bbf\u95ee\u4f4d\u7f6e\u5904\u7684\u952e\u503c<span translate=no>_^_9_^_</span>\u3002</p>\n",
 "<p> Simple test function to see the slopes.</p>\n": "<p>\u67e5\u770b\u659c\u7387\u7684\u7b80\u5355\u6d4b\u8bd5\u529f\u80fd\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> Note that we take steps by <span translate=no>_^_1_^_</span> to avoid slopes added previously. </p>\n": "<p><span translate=no>_^_0_^_</span>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u4f1a\u91c7\u53d6\u63aa\u65bd<span translate=no>_^_1_^_</span>\u907f\u514d\u4e4b\u524d\u6dfb\u52a0\u7684\u659c\u5761\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u5173\u6ce8\u6309\u952e\u5e8f\u5217\u7ef4\u5ea6<span translate=no>_^_1_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> has shape <a href=\"seq_len, seq_len, 1, 1\">seq_len, seq_len, 1, 1</a> </p>\n": "<p><span translate=no>_^_0_^_</span>\u6709\u5f62\u72b6 <a href=\"seq_len, seq_len, 1, 1\">seq_len\u3001seq_len\u30011\u30011</a></p>\n",
 "<p><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> have shape <span translate=no>_^_3_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u5e76\u4e14<span translate=no>_^_2_^_</span>\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span></p>\n",
 "<p>ALiBi only works with causal masks. </p>\n": "<p>AliBi \u4ec5\u9002\u7528\u4e8e\u56e0\u679c\u53e3\u7f69\u3002</p>\n",
 "<p>Add AliBi biases to attention scores. ALiBi biases has shape <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> has shape <span translate=no>_^_2_^_</span> </p>\n": "<p>\u5c06 AliBi \u504f\u89c1\u6dfb\u52a0\u5230\u6ce8\u610f\u529b\u5206\u6570\u4e2d\u3002AliBi \u504f\u89c1\u6709\u5f62<span translate=no>_^_0_^_</span>\u72b6<span translate=no>_^_1_^_</span>\u4e5f\u6709\u5f62\u72b6<span translate=no>_^_2_^_</span></p>\n",
 "<p>Add head dimension to mask and check its shape. </p>\n": "<p>\u5c06\u5934\u90e8\u5c3a\u5bf8\u6dfb\u52a0\u5230\u8499\u7248\u5e76\u68c0\u67e5\u5176\u5f62\u72b6\u3002</p>\n",
 "<p>Apply dropout </p>\n": "<p>\u7533\u8bf7\u9000\u5b66</p>\n",
 "<p>Apply mask </p>\n": "<p>\u6d82\u62b9\u9762\u819c</p>\n",
 "<p>Calculate distances <span translate=no>_^_0_^_</span> Here we calculate the distances using the mask.</p>\n<p>Since it&#x27;s causal mask we can just use <span translate=no>_^_1_^_</span> too. <span translate=no>_^_2_^_</span> </p>\n": "<p>\u8ba1\u7b97\u8ddd\u79bb<span translate=no>_^_0_^_</span>\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u63a9\u7801\u8ba1\u7b97\u8ddd\u79bb\u3002</p>\n<p>\u65e2\u7136\u5b83\u662f\u56e0\u679c\u63a9\u7801\uff0c\u6211\u4eec<span translate=no>_^_1_^_</span>\u4e5f\u53ef\u4ee5\u4f7f\u7528\u3002<span translate=no>_^_2_^_</span></p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570<span translate=no>_^_0_^_</span>\u3002\u8fd9\u7ed9\u51fa\u4e86\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_1_^_</span>\u3002</p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u8fde\u63a5\u591a\u4e2a\u5934</p>\n",
 "<p>Concatenate the slopes with the remaining slopes. </p>\n": "<p>\u5c06\u659c\u5761\u4e0e\u5176\u4f59\u7684\u659c\u5761\u8fde\u63a5\u8d77\u6765\u3002</p>\n",
 "<p>Create AliBi biases if it&#x27;s not cached </p>\n": "<p>\u5982\u679c AliBI \u672a\u88ab\u7f13\u5b58\uff0c\u5219\u521b\u5efa\u504f\u5dee</p>\n",
 "<p>Get slopes <span translate=no>_^_0_^_</span> for each head </p>\n": "<p>\u83b7\u53d6\u6bcf\u4e2a<span translate=no>_^_0_^_</span>\u5934\u90e8\u7684\u659c\u7387</p>\n",
 "<p>Get the closest power of 2 to <span translate=no>_^_0_^_</span>. If <span translate=no>_^_1_^_</span> is not a power of 2, then we first calculate slopes to the closest (smaller) power of 2, and then add the remaining slopes. </p>\n": "<p>\u83b7\u5f97\u6700\u63a5\u8fd1 2 \u7684\u5e42<span translate=no>_^_0_^_</span>\u3002\u5982\u679c\u4e0d<span translate=no>_^_1_^_</span>\u662f 2 \u7684\u5e42\uff0c\u90a3\u4e48\u6211\u4eec\u9996\u5148\u8ba1\u7b97\u659c\u7387\u5230\u6700\u63a5\u8fd1\uff08\u8f83\u5c0f\uff09\u7684 2 \u5e42\uff0c\u7136\u540e\u518d\u52a0\u4e0a\u5269\u4f59\u7684\u659c\u7387\u3002</p>\n",
 "<p>If <span translate=no>_^_0_^_</span> is not a power of 2, then we add the remaining slopes. We calculate the remaining slopes for <span translate=no>_^_1_^_</span> (avoiding slopes added previously). And pick the slopes upto <span translate=no>_^_2_^_</span>. </p>\n": "<p>\u5982\u679c\u4e0d<span translate=no>_^_0_^_</span>\u662f 2 \u7684\u5e42\uff0c\u90a3\u4e48\u6211\u4eec\u5c06\u5269\u4f59\u7684\u659c\u7387\u76f8\u52a0\u3002\u6211\u4eec\u8ba1\u7b97\u5269\u4f59\u7684\u659c\u7387<span translate=no>_^_1_^_</span>\uff08\u907f\u514d\u4e4b\u524d\u6dfb\u52a0\u7684\u659c\u7387\uff09\u3002\u7136\u540e\u9009\u62e9\u659c\u5761<span translate=no>_^_2_^_</span>\u3002</p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4e58\u4ee5\u503c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Multiply them pair-wise to get the AliBi bias matrix </p>\n": "<p>\u5c06\u5b83\u4eec\u6210\u5bf9\u4e58\u4ee5\u5f97\u5230 AliBi \u504f\u5dee\u77e9\u9635</p>\n",
 "<p>Output layer </p>\n": "<p>\u8f93\u51fa\u5c42</p>\n",
 "<p>Prepare <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> for attention computation. These will then have shape <span translate=no>_^_3_^_</span>. </p>\n": "<p>\u51c6\u5907<span translate=no>_^_0_^_</span>\uff0c<span translate=no>_^_1_^_</span>\u5e76<span translate=no>_^_2_^_</span>\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u7136\u540e\u8fd9\u4e9b\u5c31\u4f1a\u6709\u5f62\u72b6<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u97f3\u9636\u5206\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>To cache AliBi the biases </p>\n": "<p>\u7f13\u5b58 AliBi \u7684\u504f\u89c1</p>\n",
 "Attention with Linear Biases (ALiBi)": "\u6ce8\u610f\u7ebf\u6027\u504f\u5dee (AliBI)",
 "Documented implementation with explanations of Attention with Linear Biases (ALiBi)": "\u8bb0\u5f55\u5b9e\u73b0\uff0c\u5e76\u89e3\u91ca\u7ebf\u6027\u504f\u5dee\u6ce8\u610f\u529b (AliBi)"
}