{
 "<h1>Attention with Linear Biases (ALiBi)</h1>\n<p>This is an implementation of Attention with Linear Biases (ALiBi) from the paper <a href=\"https://papers.labml.ai/paper/2108.12409\">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>.</p>\n<p>This replaces positional encodings with biases added to attention scores (attention logits, before the softmax). This is a relative scheme tested on autoregressive tasks, and the bias is higher for closeby tokens and lower for far-away tokens. The biases decrease linearly in the log scale (because it&#x27;s before the softmax) and each head has a different slope.</p>\n<p>Here&#x27;s the attention formula for <span translate=no>_^_0_^_</span>-th token,</p>\n<span translate=no>_^_1_^_</span><p>where <span translate=no>_^_2_^_</span> is the query of the <span translate=no>_^_3_^_</span>-th token, <span translate=no>_^_4_^_</span> are the keys up to <span translate=no>_^_5_^_</span>, and <span translate=no>_^_6_^_</span> the number of features per head. Note that the above equality halts because <span translate=no>_^_7_^_</span> is invariant to translations  (you can add any constant to all elements without changing the result).</p>\n<p>Here is <a href=\"experiment.html\">the training code</a> for a ALiBi model.</p>\n<p><a href=\"https://app.labml.ai/run/1454f9ba044a11ed8364e5e321a405ac\"><span translate=no>_^_8_^_</span></a></p>\n": "<h1>\u0dbb\u0dda\u0d9b\u0dd3\u0dba\u0db4\u0d9a\u0dca\u0dc2\u0d9c\u0dca\u0dbb\u0dcf\u0dc4\u0dd3 (\u0d85\u0dbd\u0dd2\u0db6\u0dd3) \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1</h1>\n<p>\u0db8\u0dd9\u0dba\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dd2 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba (\u0d85\u0dbd\u0dd2\u0db6\u0dd3) \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc0\u0dbd\u0dd2\u0db1\u0dca <a href=\"https://papers.labml.ai/paper/2108.12409\">\u0daf\u0dd4\u0db8\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd9\u0da7\u0dd2, \u0daf\u0dd2\u0d9c\u0dd4 \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dab\u0dba: \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d86\u0daf\u0dcf\u0db1 \u0daf\u0dd2\u0d9c \u0db1\u0dd2\u0dc3\u0dca\u0dc3\u0dcf\u0dbb\u0dab\u0dba \u0dc3\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0d9a\u0dbb\u0dba\u0dd2</a>. </p>\n<p>\u0db8\u0dd9\u0dba\u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dd3\u0dba \u0d9a\u0dda\u0dad\u0dd3\u0d9a\u0dbb\u0dab \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0dc0\u0dbd\u0da7 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dbb\u0dba\u0dd2 (\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca, \u0dc3\u0ddc\u0dc6\u0dca\u0da7\u0dca\u0db8\u0dd0\u0d9a\u0dca\u0dc3\u0dca \u0dc0\u0dbd\u0da7 \u0db4\u0dd9\u0dbb). \u0db8\u0dd9\u0dba \u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0d9c\u0dcf\u0db8\u0dd3 \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba\u0db1\u0dca \u0db8\u0dad \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0dc3\u0dcf\u0db4\u0dda\u0d9a\u0dca\u0dc2 \u0dba\u0ddd\u0da2\u0db1\u0dcf \u0d9a\u0dca\u0dbb\u0db8\u0dba\u0d9a\u0dca \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0dc3\u0db8\u0dd3\u0db4 \u0da7\u0ddd\u0d9a\u0db1 \u0dc3\u0db3\u0dc4\u0dcf \u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0 \u0d89\u0dc4\u0dc5 \u0d85\u0d9c\u0dba\u0d9a\u0dca \u0d9c\u0db1\u0dca\u0db1\u0dcf \u0d85\u0dad\u0dbb \u0daf\u0dd4\u0dbb \u0db6\u0dd0\u0dc4\u0dd0\u0dbb \u0da7\u0ddd\u0d9a\u0db1 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0da9\u0dd4 \u0dc0\u0dda. \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dbd\u0ddc\u0d9c\u0dca \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba\u0dda \u0dbb\u0dda\u0d9b\u0dd3\u0dba\u0dc0 \u0d85\u0da9\u0dd4 \u0dc0\u0dda (\u0d91\u0dba \u0dc3\u0ddc\u0dc6\u0dca\u0da7\u0dca\u0db8\u0dd0\u0d9a\u0dca\u0dc3\u0dca \u0dc0\u0dbd\u0da7 \u0db4\u0dd9\u0dbb \u0db1\u0dd2\u0dc3\u0dcf) \u0dc3\u0dc4 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0d9a\u0dca \u0d87\u0dad. </p>\n<p>\u0db8\u0dd9\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>-th \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dd6\u0dad\u0dca\u0dbb\u0dba,</p>\n<span translate=no>_^_1_^_</span><p><span translate=no>_^_3_^_</span>-th \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dda \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8 <span translate=no>_^_2_^_</span> <span translate=no>_^_4_^_</span> \u0d9a\u0ddc\u0dc4\u0dda\u0daf, \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0d87\u0dad <span translate=no>_^_5_^_</span>, \u0dc3\u0dc4 <span translate=no>_^_6_^_</span> \u0dc4\u0dd2\u0dc3\u0d9a\u0da7 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0d9c\u0dab\u0db1. \u0d89\u0dc4\u0dad \u0dc3\u0db8\u0dcf\u0db1\u0dcf\u0dad\u0dca\u0db8\u0dad\u0dcf\u0dc0\u0dba \u0db1\u0dad\u0dbb <span translate=no>_^_7_^_</span> \u0dc0\u0db1 \u0db6\u0dc0 \u0dc3\u0dbd\u0d9a\u0db1\u0dca\u0db1 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0dc0\u0dbd\u0da7 \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db1\u0ddc\u0dc0\u0dda (\u0db4\u0dca\u0dbb\u0dad\u0dd2 result \u0dbd\u0dba \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db1\u0ddc\u0d9a\u0dbb \u0d94\u0db6\u0da7 \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0d85\u0d82\u0d9c \u0dc3\u0db3\u0dc4\u0dcf \u0db1\u0dd2\u0dba\u0dad \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba). </p>\n<p>\u0d85\u0dbd\u0dd2\u0db6\u0dd3\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0dc3\u0db3\u0dc4\u0dcf <a href=\"experiment.html\">\u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0d9a\u0dda\u0dad\u0dba</a> \u0db8\u0dd9\u0db1\u0dca\u0db1. </p>\n<p><a href=\"https://app.labml.ai/run/1454f9ba044a11ed8364e5e321a405ac\"><span translate=no>_^_8_^_</span></a></p>\n",
 "<h2>Attention with Linear Biases (ALiBi)</h2>\n<p>We override <a href=\"../mha.html\">Multi-Head Attention</a>.</p>\n": "<h2>\u0dbb\u0dda\u0d9b\u0dd3\u0dba\u0db4\u0d9a\u0dca\u0dc2\u0d9c\u0dca\u0dbb\u0dcf\u0dc4\u0dd3 (\u0d85\u0dbd\u0dd2\u0db6\u0dd3) \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1</h2>\n<p>\u0d85\u0db4\u0dd2 <a href=\"../mha.html\">\u0db6\u0dc4\u0dd4-\u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</a>\u0d85\u0db7\u0dd2\u0db6\u0dc0\u0dcf \u0dba\u0db8\u0dd4. </p>\n",
 "<h2>Calculate the attention biases matrix</h2>\n<ul><li><span translate=no>_^_0_^_</span> is the number of heads in the attention layer </li>\n<li><span translate=no>_^_1_^_</span> is the attention mask of shape <span translate=no>_^_2_^_</span></li></ul>\n<p>This returns a matrix of shape <span translate=no>_^_3_^_</span> with ALiBi attention biases.</p>\n": "<h2>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0 \u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba \u0d9c\u0dab\u0db1\u0dba</h2>\n<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0dc4\u0dd2\u0dc3\u0dca \u0d9c\u0dab\u0db1 </li>\n<li><span translate=no>_^_1_^_</span> \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d86\u0dc0\u0dbb\u0dab\u0dba\u0dba\u0dd2 <span translate=no>_^_2_^_</span></li></ul>\n<p>\u0db8\u0dd9\u0db8AliBi \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca <span translate=no>_^_3_^_</span> \u0dc3\u0db8\u0d9c \u0dc4\u0dd0\u0da9\u0dba \u0d85\u0db1\u0dd4\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db1\u0dd0\u0dc0\u0dad. </p>\n",
 "<h2>Get head-specific slope <span translate=no>_^_0_^_</span> for each head</h2>\n<ul><li><span translate=no>_^_1_^_</span> is the number of heads in the attention layer <span translate=no>_^_2_^_</span></li></ul>\n<p>The slope for first head is</p>\n<p><span translate=no>_^_3_^_</span></p>\n<p>The slopes for the rest of the heads are in a geometric series with a ratio same as above.</p>\n<p>For instance when the number of heads is <span translate=no>_^_4_^_</span> the slopes are <span translate=no>_^_5_^_</span></p>\n": "<h2>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 <span translate=no>_^_0_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dd2\u0dc3 \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dd2\u0dad \u0db6\u0dd1\u0dc0\u0dd4\u0db8 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1</h2>\n<ul><li><span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0dc4\u0dd2\u0dc3\u0dca \u0d9c\u0dab\u0db1 <span translate=no>_^_2_^_</span></li></ul>\n<p>\u0db4\u0dc5\u0db8\u0dd4\u0dc4\u0dd2\u0dc3 \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dd1\u0dc0\u0dd4\u0db8</p>\n<p><span translate=no>_^_3_^_</span></p>\n<p>\u0dc3\u0dd9\u0dc3\u0dd4\u0dc4\u0dd2\u0dc3\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d89\u0dc4\u0dad \u0dc3\u0db8\u0dcf\u0db1 \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0da2\u0dca\u0dba\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d9a \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0d9a\u0dca \u0dc0\u0dda. </p>\n<p>\u0d8b\u0daf\u0dcf\u0dc4\u0dbb\u0dab\u0dba\u0d9a\u0dca\u0dbd\u0dd9\u0dc3 \u0dc4\u0dd2\u0dc3\u0dca \u0d9c\u0dab\u0db1 \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca <span translate=no>_^_4_^_</span> \u0dc0\u0db1 \u0dc0\u0dd2\u0da7 <span translate=no>_^_5_^_</span></p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> are the tensors that store collection of <em>query</em>, <em>key</em> and <em>value</em> vectors. They have shape <span translate=no>_^_3_^_</span>.</p>\n<p><span translate=no>_^_4_^_</span> has shape <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span> indicates whether for batch <span translate=no>_^_7_^_</span>, query at position <span translate=no>_^_8_^_</span> has access to key-value at position <span translate=no>_^_9_^_</span>.</p>\n": "<p> <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> \u0dc3\u0dc4 <span translate=no>_^_2_^_</span> <em>\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8</em>, <em>\u0dba\u0dad\u0dd4\u0dbb</em>\u0dc3\u0dc4 <em>\u0d85\u0d9c\u0dba</em>\u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d9c\u0db6\u0da9\u0dcf \u0d9a\u0dbb\u0db1 \u0d86\u0dad\u0db1\u0dca\u0dba \u0dc0\u0dda. \u0d92\u0dc0\u0dcf\u0dba\u0dda \u0dc4\u0dd0\u0da9\u0dba \u0d87\u0dad <span translate=no>_^_3_^_</span>. </p>\n<p><span translate=no>_^_4_^_</span> \u0dc4\u0dd0\u0da9\u0dba \u0d87\u0dad\u0dd2 <span translate=no>_^_5_^_</span> \u0d85\u0dad\u0dbb \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8 \u0dc3\u0db3\u0dc4\u0dcf <span translate=no>_^_7_^_</span>, \u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dba\u0dda \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0da7 \u0db4\u0dca\u0dbb\u0dc0\u0dda\u0dc1\u0dba <span translate=no>_^_8_^_</span> \u0dad\u0dd2\u0db6\u0dda\u0daf \u0dba\u0db1\u0dca\u0db1 <span translate=no>_^_6_^_</span> \u0daf\u0d9a\u0dca\u0dc0\u0dba\u0dd2 \u0dc3\u0dca\u0dae\u0dcf\u0db1\u0dba\u0dda \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1-\u0d85\u0d9c\u0dba <span translate=no>_^_9_^_</span>. </p>\n",
 "<p> Simple test function to see the slopes.</p>\n": "<p> \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca\u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0da7 \u0dc3\u0dbb\u0dbd \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dab \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba. </p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p><span translate=no>_^_0_^_</span> Note that we take steps by <span translate=no>_^_1_^_</span> to avoid slopes added previously. </p>\n": "<p><span translate=no>_^_0_^_</span> \u0d9a\u0dbd\u0dd2\u0db1\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0dc0\u0dc5\u0d9a\u0dca\u0dc0\u0dcf <span translate=no>_^_1_^_</span> \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0dd2 \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0db1\u0dca\u0db1\u0dcf \u0db6\u0dc0 \u0dc3\u0dbd\u0d9a\u0db1\u0dca\u0db1. </p>\n",
 "<p><span translate=no>_^_0_^_</span> attention along the key sequence dimension <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1 \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0d94\u0dc3\u0dca\u0dc3\u0dda \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba <span translate=no>_^_1_^_</span> </p>\n",
 "<p><span translate=no>_^_0_^_</span> has shape <a href=\"seq_len, seq_len, 1, 1\">seq_len, seq_len, 1, 1</a> </p>\n": "<p><span translate=no>_^_0_^_</span> <a href=\"seq_len, seq_len, 1, 1\">seq_len, seq_len, 1, 1</a> \u0dc4\u0dd0\u0da9\u0dba \u0d87\u0dad </p>\n",
 "<p><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> have shape <span translate=no>_^_3_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> <span translate=no>_^_2_^_</span> \u0dc3\u0dc4 \u0dc4\u0dd0\u0da9\u0dba <span translate=no>_^_3_^_</span> </p>\n",
 "<p>ALiBi only works with causal masks. </p>\n": "<p>\u0d85\u0dbd\u0dd2\u0db6\u0dd3\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0dc4\u0dda\u0dad\u0dd4 \u0dc0\u0dd9\u0dc3\u0dca \u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0dc3\u0db8\u0d9f \u0db4\u0db8\u0dab\u0dd2. </p>\n",
 "<p>Add AliBi biases to attention scores. ALiBi biases has shape <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> has shape <span translate=no>_^_2_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 AliBi \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0d85\u0dbd\u0dd2\u0db6\u0dd3 \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc4\u0dd0\u0da9\u0dba\u0d9a\u0dca <span translate=no>_^_1_^_</span> \u0d87\u0dad\u0dd2 <span translate=no>_^_0_^_</span> \u0d85\u0dad\u0dbb \u0dc4\u0dd0\u0da9\u0dba\u0d9a\u0dca </p>\u0d87\u0dad <span translate=no>_^_2_^_</span>\n",
 "<p>Add head dimension to mask and check its shape. </p>\n": "<p>\u0d86\u0dc0\u0dbb\u0dab\u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dd2\u0dc3 \u0db8\u0dcf\u0db1\u0dba\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0d9a\u0dbb \u0d91\u0dc4\u0dd2 \u0dc4\u0dd0\u0da9\u0dba \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n",
 "<p>Apply dropout </p>\n": "<p>\u0d85\u0dad\u0dc4\u0dd0\u0dbb\u0daf\u0dd0\u0db8\u0dd3\u0db8 \u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Apply mask </p>\n": "<p>\u0dc0\u0dd9\u0dc3\u0dca\u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate distances <span translate=no>_^_0_^_</span> Here we calculate the distances using the mask.</p>\n<p>Since it&#x27;s causal mask we can just use <span translate=no>_^_1_^_</span> too. <span translate=no>_^_2_^_</span> </p>\n": "<p>\u0daf\u0dd4\u0dbb\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> \u0db8\u0dd9\u0db1\u0dca\u0db1 \u0d85\u0db4\u0dd2 \u0d86\u0dc0\u0dbb\u0dab \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0daf\u0dd4\u0dbb \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4. </p>\n<p>\u0d91\u0dba\u0dc4\u0dda\u0dad\u0dd4 \u0dc0\u0dd9\u0dc3\u0dca \u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0d9a\u0dca \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca <span translate=no>_^_1_^_</span> \u0d85\u0db4\u0da7\u0daf \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. <span translate=no>_^_2_^_</span> </p>\n",
 "<p>Compute attention scores <span translate=no>_^_0_^_</span>. This gives a tensor of shape <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>. \u0db8\u0dd9\u0dba \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2 <span translate=no>_^_1_^_</span>. </p>\n",
 "<p>Concatenate multiple heads </p>\n": "<p>\u0db6\u0dc4\u0dd4\u0dc4\u0dd2\u0dc3\u0dca \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Concatenate the slopes with the remaining slopes. </p>\n": "<p>\u0d89\u0dad\u0dd2\u0dbb\u0dd2\u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n",
 "<p>Create AliBi biases if it&#x27;s not cached </p>\n": "<p>\u0d91\u0dba\u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2 \u0db1\u0ddc\u0dc0\u0dda \u0db1\u0db8\u0dca AliBi \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba </p>\n",
 "<p>Get slopes <span translate=no>_^_0_^_</span> for each head </p>\n": "<p>\u0d91\u0d9a\u0dca\u0d91\u0d9a\u0dca \u0dc4\u0dd2\u0dc3 <span translate=no>_^_0_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the closest power of 2 to <span translate=no>_^_0_^_</span>. If <span translate=no>_^_1_^_</span> is not a power of 2, then we first calculate slopes to the closest (smaller) power of 2, and then add the remaining slopes. </p>\n": "<p>2\u0da7 \u0d86\u0dc3\u0db1\u0dca\u0db1\u0dad\u0db8 \u0db6\u0dbd\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>. 2 \u0db6\u0dbd\u0dba\u0d9a\u0dca <span translate=no>_^_1_^_</span> \u0db1\u0ddc\u0dc0\u0dda \u0db1\u0db8\u0dca, \u0d85\u0db4\u0dd2 \u0db8\u0dd4\u0dbd\u0dd2\u0db1\u0dca\u0db8 2 \u0dc4\u0dd2 \u0dc3\u0db8\u0dd3\u0db4\u0dad\u0db8 (\u0d9a\u0dd4\u0da9\u0dcf) \u0db6\u0dbd\u0dba\u0da7 \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb \u0d89\u0dad\u0dd2\u0dbb\u0dd2 \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db8\u0dd4. </p>\n",
 "<p>If <span translate=no>_^_0_^_</span> is not a power of 2, then we add the remaining slopes. We calculate the remaining slopes for <span translate=no>_^_1_^_</span> (avoiding slopes added previously). And pick the slopes upto <span translate=no>_^_2_^_</span>. </p>\n": "<p>2\u0db6\u0dbd\u0dba\u0d9a\u0dca <span translate=no>_^_0_^_</span> \u0db1\u0ddc\u0dc0\u0dda \u0db1\u0db8\u0dca, \u0d85\u0db4\u0dd2 \u0d89\u0dad\u0dd2\u0dbb\u0dd2 \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db8\u0dd4. \u0d85\u0db4\u0dd2 \u0d89\u0dad\u0dd2\u0dbb\u0dd2 \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4 <span translate=no>_^_1_^_</span> (\u0d9a\u0dbd\u0dd2\u0db1\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0db8\u0d9c \u0dc4\u0dd0\u0dbb\u0dd3\u0db8). \u0db6\u0dd1\u0dc0\u0dd4\u0db8\u0dca \u0d89\u0dc4\u0dc5\u0da7 \u0d9c\u0db1\u0dca\u0db1 <span translate=no>_^_2_^_</span>. </p>\n",
 "<p>Multiply by values <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0d9c\u0dba\u0db1\u0dca\u0d85\u0db1\u0dd4\u0dc0 \u0d9c\u0dd4\u0dab \u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Multiply them pair-wise to get the AliBi bias matrix </p>\n": "<p>\u0d85\u0dbd\u0dd2\u0db6\u0dd3\u0db1\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc0 \u0db1\u0dca\u0dba\u0dcf\u0dc3\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d92\u0dc0\u0dcf \u0dba\u0dd4\u0d9c\u0dbd \u0dc0\u0dc1\u0dba\u0dd9\u0db1\u0dca \u0d9c\u0dd4\u0dab \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Output layer </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dc3\u0dca\u0dae\u0dbb\u0dba </p>\n",
 "<p>Prepare <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> for attention computation. These will then have shape <span translate=no>_^_3_^_</span>. </p>\n": "<p>\u0dc3\u0dd6\u0daf\u0dcf\u0db1\u0db8\u0dca\u0dc0\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> \u0dc3\u0dc4 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 <span translate=no>_^_2_^_</span> \u0dc3\u0db3\u0dc4\u0dcf. \u0db8\u0dda\u0dc0\u0dcf\u0da7 \u0db4\u0dc3\u0dd4\u0dc0 \u0dc4\u0dd0\u0da9\u0dba \u0d87\u0dad <span translate=no>_^_3_^_</span>. </p>\n",
 "<p>Scale scores <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dbd\u0d9a\u0dd4\u0dab\u0dd4 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>To cache AliBi the biases </p>\n": "<p>\u0d85\u0dbd\u0dd2\u0db6\u0dd3\u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 </p>\n",
 "Attention with Linear Biases (ALiBi)": "\u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0d9a\u0dca\u0dc2\u0d9c\u0dca\u0dbb\u0dcf\u0dc4\u0dd3 (\u0d85\u0dbd\u0dd2\u0db6\u0dd3) \u0dc3\u0db8\u0d9f \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1",
 "Documented implementation with explanations of Attention with Linear Biases (ALiBi)": "\u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0d85\u0d9c\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9c \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dd0\u0dc4\u0dd0\u0daf\u0dd2\u0dbd\u0dd2 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0dc3\u0db8\u0d9c \u0dbd\u0dda\u0d9b\u0db1\u0d9c\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 (AliBi)"
}