{
 "<h1>Long Short-Term Memory (LSTM)</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of Long Short-Term Memory.</p>\n": "<h1>\u9577\u671f\u77ed\u671f\u8a18\u61b6 (LSTM)</h1>\n<p><a href=\"https://pytorch.org\">\u3053\u308c\u306f\u9577\u77ed\u671f\u8a18\u61b6\u306ePyTorch\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n",
 "<h2>Long Short-Term Memory Cell</h2>\n<p>LSTM Cell computes <span translate=no>_^_0_^_</span>, and <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is like the long-term memory, and <span translate=no>_^_3_^_</span> is like the short term memory. We use the input <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span> to update the long term memory. In the update, some features of <span translate=no>_^_6_^_</span> are cleared with a forget gate <span translate=no>_^_7_^_</span>, and some features <span translate=no>_^_8_^_</span> are added through a gate <span translate=no>_^_9_^_</span>.</p>\n<p>The new short term memory is the <span translate=no>_^_10_^_</span> of the long-term memory multiplied by the output gate <span translate=no>_^_11_^_</span>.</p>\n<p>Note that the cell doesn&#x27;t look at long term memory <span translate=no>_^_12_^_</span> when doing the update. It only modifies it. Also <span translate=no>_^_13_^_</span> never goes through a linear transformation. This is what solves vanishing and exploding gradients.</p>\n<p>Here&#x27;s the update rule.</p>\n<span translate=no>_^_14_^_</span><p><span translate=no>_^_15_^_</span> stands for element-wise multiplication.</p>\n<p>Intermediate values and gates are computed as linear transformations of the hidden state and input.</p>\n<span translate=no>_^_16_^_</span>": "<h2>\u9577\u77ed\u671f\u30e1\u30e2\u30ea\u30fc\u30bb\u30eb</h2>\n<p>LSTM \u30bb\u30eb\u304c\u8a08\u7b97\u3057\u3001<span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u9577\u671f\u8a18\u61b6\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3001\u77ed\u671f\u8a18\u61b6\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u5165\u529b\u3068\u3092\u4f7f\u7528\u3057\u3066\u9577\u671f\u8a18\u61b6\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u4eca\u56de\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3067\u306f\u3001<span translate=no>_^_6_^_</span>\u4e00\u90e8\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306f\u30d5\u30a9\u30fc\u30b2\u30fc\u30c8\u30b2\u30fc\u30c8\u3067\u30af\u30ea\u30a2\u3055\u308c<span translate=no>_^_7_^_</span>\u3001<span translate=no>_^_8_^_</span><span translate=no>_^_9_^_</span>\u4e00\u90e8\u306e\u30d5\u30a3\u30fc\u30c1\u30e3\u306f\u30b2\u30fc\u30c8\u7d4c\u7531\u3067\u8ffd\u52a0\u3055\u308c\u307e\u3059</p>\u3002\n<p>\u65b0\u3057\u3044\u77ed\u671f\u8a18\u61b6\u306f\u3001<span translate=no>_^_10_^_</span>\u9577\u671f\u8a18\u61b6\u306b\u51fa\u529b\u30b2\u30fc\u30c8\u3092\u639b\u3051\u305f\u3082\u306e\u3067\u3059\u3002<span translate=no>_^_11_^_</span></p>\n<p><span translate=no>_^_12_^_</span>\u66f4\u65b0\u3092\u884c\u3046\u3068\u304d\u3001\u30bb\u30eb\u306f\u9577\u671f\u8a18\u61b6\u3092\u898b\u306a\u3044\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5909\u66f4\u3059\u308b\u3060\u3051\u3067\u3059\u3002\u307e\u305f\u3001<span translate=no>_^_13_^_</span>\u7dda\u5f62\u5909\u63db\u3092\u884c\u3046\u3053\u3068\u3082\u3042\u308a\u307e\u305b\u3093\u3002\u3053\u308c\u304c\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u6d88\u5931\u3068\u7206\u767a\u3092\u89e3\u6d88\u3059\u308b\u3082\u306e\u3067\u3059</p>\u3002\n<p>\u66f4\u65b0\u30eb\u30fc\u30eb\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002</p>\n<span translate=no>_^_14_^_</span><p><span translate=no>_^_15_^_</span>\u8981\u7d20\u3054\u3068\u306e\u4e57\u7b97\u306e\u7565\u3067\u3059\u3002</p>\n<p>\u4e2d\u9593\u5024\u3068\u30b2\u30fc\u30c8\u306f\u3001\u96a0\u308c\u72b6\u614b\u3068\u5165\u529b\u306e\u7dda\u5f62\u5909\u63db\u3068\u3057\u3066\u8a08\u7b97\u3055\u308c\u307e\u3059\u3002</p>\n<span translate=no>_^_16_^_</span>",
 "<h2>Multilayer LSTM</h2>\n": "<h2>\u30de\u30eb\u30c1\u30ec\u30a4\u30e4\u30fcLSTM</h2>\n",
 "<p> <span translate=no>_^_0_^_</span> has shape <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> is a tuple of <span translate=no>_^_3_^_</span> and <span translate=no>_^_4_^_</span>, each with a shape of <span translate=no>_^_5_^_</span>.</p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u306e\u5f62\u72b6\u306f\u3068\u304c\u7d44\u307f\u5408\u308f\u3055\u3063\u305f\u3082\u306e\u3067<span translate=no>_^_4_^_</span>\u3001<span translate=no>_^_5_^_</span>\u305d\u308c\u305e\u308c\u306e\u5f62\u72b6\u306f\u3067\u3059\u3002</p>\n",
 "<p> Create a network of <span translate=no>_^_0_^_</span> of LSTM.</p>\n": "<p>LSTM <span translate=no>_^_0_^_</span> \u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> Optionally, apply layer norm to <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u30ec\u30a4\u30e4\u30fc\u30ce\u30eb\u30e0\u3092\u9069\u7528 <span translate=no>_^_1_^_</span></p>\n",
 "<p>Apply layer normalization (not in original paper, but gives better results) </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u6b63\u898f\u5316\u3092\u9069\u7528\uff08\u5143\u306e\u7528\u7d19\u306b\u306f\u4f7f\u7528\u3057\u306a\u3044\u304c\u3001\u3088\u308a\u826f\u3044\u7d50\u679c\u306b\u306a\u308b\uff09</p>\n",
 "<p>Array to collect the outputs of the final layer at each time step. </p>\n": "<p>\u5404\u30bf\u30a4\u30e0\u30b9\u30c6\u30c3\u30d7\u3067\u6700\u7d42\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\u3092\u53ce\u96c6\u3059\u308b\u914d\u5217\u3002</p>\n",
 "<p>Collect the output <span translate=no>_^_0_^_</span> of the final layer </p>\n": "<p><span translate=no>_^_0_^_</span>\u6700\u7d42\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\u3092\u96c6\u3081\u308b</p>\n",
 "<p>Create cells for each layer. Note that only the first layer gets the input directly. Rest of the layers get the input from the layer below </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u3054\u3068\u306b\u30bb\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u6700\u521d\u306e\u30ec\u30a4\u30e4\u30fc\u3060\u3051\u304c\u76f4\u63a5\u5165\u529b\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u6b8b\u308a\u306e\u30ec\u30a4\u30e4\u30fc\u306f\u3001\u4e0b\u306e\u30ec\u30a4\u30e4\u30fc\u304b\u3089\u5165\u529b\u3092\u53d6\u5f97\u3057\u307e\u3059</p>\u3002\n",
 "<p>Each layer produces an output of 4 times the <span translate=no>_^_0_^_</span> and we split them </p>\n": "<p>\u5404\u30ec\u30a4\u30e4\u30fc\u306f 4 <span translate=no>_^_0_^_</span> \u500d\u306e\u51fa\u529b\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u3089\u3092\u5206\u5272\u3057\u307e\u3059</p>\n",
 "<p>Get the state of the layer </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u306e\u72b6\u614b\u3092\u53d6\u5f97</p>\n",
 "<p>Initialize the state if <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6b21\u306e\u5834\u5408\u306b\u30b9\u30c6\u30fc\u30c8\u3092\u521d\u671f\u5316\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Input to the first layer is the input itself </p>\n": "<p>\u6700\u521d\u306e\u30ec\u30a4\u30e4\u30fc\u3078\u306e\u5165\u529b\u306f\u5165\u529b\u305d\u306e\u3082\u306e\u3067\u3059</p>\n",
 "<p>Input to the next layer is the state of this layer </p>\n": "<p>\u6b21\u306e\u30ec\u30a4\u30e4\u30fc\u3078\u306e\u5165\u529b\u306f\u3001\u3053\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u72b6\u614b\u3067\u3059</p>\n",
 "<p>Loop through the layers </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u3092\u30eb\u30fc\u30d7\u3059\u308b</p>\n",
 "<p>Reverse stack the tensors to get the states of each layer</p>\n<p>\ud83d\udcdd You can just work with the tensor itself but this is easier to debug </p>\n": "<p>\u30c6\u30f3\u30bd\u30eb\u3092\u9006\u306b\u7a4d\u307f\u91cd\u306d\u3066\u5404\u30ec\u30a4\u30e4\u30fc\u306e\u72b6\u614b\u3092\u53d6\u5f97\u3057\u307e\u3059</p>\n<p>\ud83d\udcdd \u30c6\u30f3\u30bd\u30eb\u81ea\u4f53\u3067\u4f5c\u696d\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u30c7\u30d0\u30c3\u30b0\u306f\u7c21\u5358\u3067\u3059</p>\n",
 "<p>Stack the outputs and states </p>\n": "<p>\u51fa\u529b\u3068\u30b9\u30c6\u30fc\u30c8\u3092\u7a4d\u307f\u91cd\u306d\u308b</p>\n",
 "<p>These are the linear layer to transform the <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> vectors. One of them doesn&#x27;t need a bias since we add the transformations. </p>\n": "<p>\u3053\u308c\u3089\u306f\u3001<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u304a\u3088\u3073\u30d9\u30af\u30c8\u30eb\u3092\u5909\u63db\u3059\u308b\u7dda\u5f62\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002\u305d\u306e\u3046\u3061\u306e 1 \u3064\u306f\u3001\u5909\u63db\u3092\u8ffd\u52a0\u3059\u308b\u306e\u3067\u30d0\u30a4\u30a2\u30b9\u3092\u5fc5\u8981\u3068\u3057\u307e\u305b\u3093</p>\u3002\n",
 "<p>This combines <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span>, and <span translate=no>_^_3_^_</span> transformations. </p>\n": "<p>\u3053\u308c\u306b\u3088\u308a<span translate=no>_^_0_^_</span>\u3001\u3001<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u3001<span translate=no>_^_3_^_</span>\u5909\u63db\u304c\u7d44\u307f\u5408\u308f\u3055\u308c\u307e\u3059\u3002</p>\n",
 "<p>We compute the linear transformations for <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> using the same linear layers. </p>\n": "<p>\u3001\u3001\u306e\u7dda\u5f62\u5909\u63db\u306f<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u3001<span translate=no>_^_3_^_</span>\u540c\u3058\u7dda\u5f62\u5c64\u3092\u4f7f\u7528\u3057\u3066\u8a08\u7b97\u3057\u307e\u3059\u3002</p>\n",
 "<p>Whether to apply layer normalizations.</p>\n<p>Applying layer normalization gives better results. <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> and <span translate=no>_^_3_^_</span> embeddings are normalized and <span translate=no>_^_4_^_</span> is normalized in <span translate=no>_^_5_^_</span> </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u3092\u9069\u7528\u3059\u308b\u304b\u3069\u3046\u304b\u3002</p>\n<p>\u30ec\u30a4\u30e4\u30fc\u306e\u6b63\u898f\u5316\u3092\u9069\u7528\u3059\u308b\u3068\u3001\u3088\u308a\u826f\u3044\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002<span translate=no>_^_0_^_</span>\u3001<span translate=no>_^_1_^_</span>\u3001<span translate=no>_^_2_^_</span><span translate=no>_^_3_^_</span>\u304a\u3088\u3073\u57cb\u3081\u8fbc\u307f\u306f\u6b63\u898f\u5316\u3055\u308c\u3001\u6b21\u306e\u5f62\u5f0f\u3067\u6b63\u898f\u5316\u3055\u308c\u307e\u3059 <span translate=no>_^_4_^_</span> <span translate=no>_^_5_^_</span></p>\n",
 "A simple PyTorch implementation/tutorial of Long Short-Term Memory (LSTM) modules.": "\u9577\u77ed\u671f\u8a18\u61b6 (LSTM) \u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7c21\u5358\u306a PyTorch \u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3002",
 "Long Short-Term Memory (LSTM)": "\u9577\u671f\u77ed\u671f\u8a18\u61b6 (LSTM)"
}