{
 "<h1>Trying out Sampling Techniques for Language Models</h1>\n<ul><li><a href=\"greedy.html\">Greedy Sampling</a> </li>\n<li><a href=\"temperature.html\">Temperature Sampling</a> </li>\n<li><a href=\"top_k.html\">Top-k Sampling</a> </li>\n<li><a href=\"nucleus.html\">Nucleus Sampling</a></li></ul>\n<p>This experiment uses the above sampling techniques, on HuggingFace&#x27;s GPT2 model.</p>\n": "<h1>\u5c1d\u8bd5\u8bed\u8a00\u6a21\u578b\u7684\u91c7\u6837\u6280\u672f</h1>\n<ul><li><a href=\"greedy.html\">\u8d2a\u5a6a\u91c7\u6837</a></li>\n<li><a href=\"temperature.html\">\u6e29\u5ea6\u91c7\u6837</a></li>\n<li><a href=\"top_k.html\">\u524d k \u4e2a\u91c7\u6837</a></li>\n<li><a href=\"nucleus.html\">\u539f\u5b50\u6838\u91c7\u6837</a></li></ul>\n<p>\u672c\u5b9e\u9a8c\u5728HuggingFace\u7684GPT2\u6a21\u578b\u4e0a\u4f7f\u7528\u4e86\u4e0a\u8ff0\u91c7\u6837\u6280\u672f\u3002</p>\n",
 "<h2>Sample from model</h2>\n<ul><li><span translate=no>_^_0_^_</span>  is the model to sample from </li>\n<li><span translate=no>_^_1_^_</span>  is the tokenizer to use </li>\n<li><span translate=no>_^_2_^_</span>  is the sampler to use </li>\n<li><span translate=no>_^_3_^_</span>  is the number of samples to generate </li>\n<li><span translate=no>_^_4_^_</span>  is the number of tokens to generate </li>\n<li><span translate=no>_^_5_^_</span>  is the maximum sequence length for the model </li>\n<li><span translate=no>_^_6_^_</span>  is the starting prompt</li></ul>\n": "<h2>\u6765\u81ea\u6a21\u578b\u7684\u6837\u672c</h2>\n<ul><li><span translate=no>_^_0_^_</span>\u662f\u8981\u91c7\u6837\u7684\u6a21\u578b</li>\n<li><span translate=no>_^_1_^_</span>\u662f\u8981\u4f7f\u7528\u7684\u5206\u8bcd\u5668</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8981\u4f7f\u7528\u7684\u91c7\u6837\u5668</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u8981\u751f\u6210\u7684\u6837\u672c\u6570</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u8981\u751f\u6210\u7684\u4ee4\u724c\u6570\u91cf</li>\n<li><span translate=no>_^_5_^_</span>\u662f\u6a21\u578b\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6</li>\n<li><span translate=no>_^_6_^_</span>\u662f\u8d77\u59cb\u63d0\u793a</li></ul>\n",
 "<h3>Try different sampling techniques</h3>\n": "<h3>\u5c1d\u8bd5\u4e0d\u540c\u7684\u91c7\u6837\u6280\u5de7</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><a href=\"greedy.html\">Greedy Sampling</a> </p>\n": "<p><a href=\"greedy.html\">\u8d2a\u5a6a\u91c7\u6837</a></p>\n",
 "<p><a href=\"nucleus.html\">Nucleus Sampling</a> </p>\n": "<p><a href=\"nucleus.html\">\u539f\u5b50\u6838\u91c7\u6837</a></p>\n",
 "<p><a href=\"temperature.html\">Temperature Sampling</a> </p>\n": "<p><a href=\"temperature.html\">\u6e29\u5ea6\u91c7\u6837</a></p>\n",
 "<p><a href=\"top_k.html\">Top-k Sampling</a> </p>\n": "<p><a href=\"top_k.html\">\u524d k \u4e2a\u91c7\u6837</a></p>\n",
 "<p>Add the sampled token to the data </p>\n": "<p>\u5c06\u91c7\u6837\u4ee4\u724c\u6dfb\u52a0\u5230\u6570\u636e\u4e2d</p>\n",
 "<p>Collect output for printing </p>\n": "<p>\u6536\u96c6\u8f93\u51fa\u4ee5\u8fdb\u884c\u6253\u5370</p>\n",
 "<p>Decode and add the sampled token for logging </p>\n": "<p>\u89e3\u7801\u5e76\u6dfb\u52a0\u7528\u4e8e\u65e5\u5fd7\u8bb0\u5f55\u7684\u91c7\u6837\u4ee4\u724c</p>\n",
 "<p>Get the <span translate=no>_^_0_^_</span> of the last token </p>\n": "<p>\u83b7\u53d6\u6700\u540e<span translate=no>_^_0_^_</span>\u4e00\u4e2a\u4ee4\u724c\u7684</p>\n",
 "<p>Get the model output. The &#x27;logits&#x27; has shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u83b7\u53d6\u6a21\u578b\u8f93\u51fa\u3002\u201clogits\u201d \u6709\u5f62\u72b6<span translate=no>_^_0_^_</span></p>\n",
 "<p>Load the model and tokenizer </p>\n": "<p>\u52a0\u8f7d\u6a21\u578b\u548c\u5206\u8bcd\u5668</p>\n",
 "<p>Print the sampled outputs </p>\n": "<p>\u6253\u5370\u91c7\u6837\u8f93\u51fa</p>\n",
 "<p>Prompts to use for sampling </p>\n": "<p>\u91c7\u6837\u65f6\u4f7f\u7528\u7684\u63d0\u793a</p>\n",
 "<p>Sample <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6837\u672c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Sample from the <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6837\u672c\u6765\u81ea<span translate=no>_^_0_^_</span></p>\n",
 "<p>Set the model to eval mode </p>\n": "<p>\u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f</p>\n",
 "<p>Tokenize the <span translate=no>_^_0_^_</span> and make <span translate=no>_^_1_^_</span> copies of it </p>\n": "<p>\u6807\u8bb0\u5316<span translate=no>_^_0_^_</span>\u5e76\u5236\u4f5c\u5176<span translate=no>_^_1_^_</span>\u526f\u672c</p>\n",
 "<p>Truncate the data to the maximum sequence length </p>\n": "<p>\u5c06\u6570\u636e\u622a\u65ad\u4e3a\u6700\u5927\u5e8f\u5217\u957f\u5ea6</p>\n",
 "Trying out Sampling Techniques for Language Models": "\u5c1d\u8bd5\u8bed\u8a00\u6a21\u578b\u7684\u91c7\u6837\u6280\u672f",
 "We try out different sampling techniques for language models on HuggingFace's GPT2 model.": "\u6211\u4eec\u5728HuggingFace\u7684GPT2\u6a21\u578b\u4e0a\u4e3a\u8bed\u8a00\u6a21\u578b\u5c1d\u8bd5\u4e86\u4e0d\u540c\u7684\u91c7\u6837\u6280\u672f\u3002"
}