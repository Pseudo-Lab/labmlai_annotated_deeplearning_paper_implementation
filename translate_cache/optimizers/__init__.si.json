{
 "<h1>Optimizers</h1>\n<h2>Optimizer Implementations</h2>\n<ul><li><a href=\"adam.html\">Adam Optimizer</a> </li>\n<li><a href=\"amsgrad.html\">AMSGrad Optimizer</a> </li>\n<li><a href=\"adam_warmup.html\">Adam Optimizer with warmup</a> </li>\n<li><a href=\"noam.html\">Noam Optimizer</a> </li>\n<li><a href=\"radam.html\">Rectified Adam Optimizer</a> </li>\n<li><a href=\"ada_belief.html\">AdaBelief Optimizer</a></li></ul>\n<p>This <a href=\"mnist_experiment.html\">MNIST example</a> uses these optimizers.</p>\n<h2>Generic Adaptive Optimizer Base class and Weight Decay</h2>\n<p>This file defines a common base class for <em>Adam</em> and extensions of it. The base class helps use implement other optimizers with minimal code because of re-usability.</p>\n<p>We also define a special class for L2 weight decay, so that we don&#x27;t have to implement it inside each of the optimizers, and can easily extend to other weight decays like L1 without changing the optimizers.</p>\n<p>Here are some concepts on PyTorch optimizers:</p>\n<h3>Parameter groups</h3>\n<p>PyTorch optimizers group parameters into sets called groups. Each group can have its own hyper-parameters like learning rates.</p>\n<p>In most common cases there will be only one group. This is when you initialize your optimizer with,</p>\n<span translate=no>_^_0_^_</span><p>You can define multiple parameter groups when initializing the optimizer:</p>\n<span translate=no>_^_1_^_</span><p>Here we pass a list of groups. Each group is a dictionary with its parameters under the key &#x27;params&#x27;. You specify any hyper-parameters as well. If the hyper parameters are not defined they will default to the optimizer level defaults.</p>\n<p>You can access (and even change) these groups, and their hyper-parameters with <span translate=no>_^_2_^_</span>. Most learning rate schedule implementations I&#x27;ve come across do access this and change &#x27;lr&#x27;.</p>\n<h3>States</h3>\n<p>Optimizer maintains states (a dictionary) for each parameter (a tensor), in a dictionary <span translate=no>_^_3_^_</span>. This is where the optimizer maintains things like exponential averages.</p>\n": "<h1>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</h1>\n<h2>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca</h2>\n<ul><li><a href=\"adam.html\">\u0d86\u0daf\u0db8\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a> </li>\n<li><a href=\"amsgrad.html\">AMSGrad \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a> </li>\n<li><a href=\"adam_warmup.html\">\u0d8b\u0dab\u0dd4\u0dc3\u0dd4\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db8\u0d9f \u0d86\u0daf\u0db8\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a> </li>\n<li><a href=\"noam.html\">\u0db1\u0dc0 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a> </li>\n<li><a href=\"radam.html\">\u0db1\u0dd2\u0dc0\u0dd0\u0dbb\u0daf\u0dd2 \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d86\u0daf\u0db8\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a> </li>\n<li><a href=\"ada_belief.html\">ADABelief \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</a></li></ul>\n<p>\u0db8\u0dd9\u0db8 <a href=\"mnist_experiment.html\">MNIST \u0d8b\u0daf\u0dcf\u0dc4\u0dbb\u0dab\u0dba</a> \u0db8\u0dd9\u0db8 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<h2>Generic\u0db6\u0dbd\u0db4\u0dad\u0dca\u0dbb\u0dba \u0dba\u0da7\u0dad\u0dda \u0d85\u0dc0\u0dc3\u0dbb \u0dbd\u0db6\u0dcf \u0d87\u0dad \u0d85\u0db1\u0dd4\u0dc0\u0dbb\u0dca\u0dad\u0dd3 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0da8\u0dcf\u0db4\u0db1\u0dba \u0db8\u0dd6\u0dbd\u0dd2\u0d9a \u0db4\u0db1\u0dca\u0dad\u0dd2\u0dba</h2>\n<p>\u0db8\u0dd9\u0db8\u0d9c\u0ddc\u0db1\u0dd4\u0dc0 <em>\u0d86\u0daf\u0db8\u0dca</em> \u0dc3\u0dc4 \u0d91\u0dc4\u0dd2 \u0daf\u0dd2\u0d9c\u0dd4 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0ddc\u0daf\u0dd4 \u0db4\u0dcf\u0daf\u0d9a \u0db4\u0db1\u0dca\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0dba\u0dd2. \u0db8\u0dd6\u0dbd\u0dd2\u0d9a \u0db4\u0db1\u0dca\u0dad\u0dd2\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d8b\u0db4\u0d9a\u0dcf\u0dbb\u0dd3 \u0dc0\u0dda \u0db1\u0dd0\u0dc0\u0dad \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dc4\u0dd0\u0d9a\u0dd2\u0dba\u0dcf\u0dc0 \u0db1\u0dd2\u0dc3\u0dcf \u0d85\u0dc0\u0db8 \u0d9a\u0dda\u0dad\u0dba\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0dc0\u0dd9\u0db1\u0dad\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dcf\u0dbb\u0d9a \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n<p>\u0d91\u0dbd\u0dca2 \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dd2\u0dc1\u0dda\u0dc2 \u0db4\u0db1\u0dca\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0daf \u0d85\u0db4\u0dd2 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0db1\u0dca\u0db1\u0dd9\u0db8\u0dd4, \u0d91\u0dc0\u0dd2\u0da7 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0dad\u0dd4\u0dc5 \u0d91\u0dba \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d85\u0db4\u0da7 \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0db1\u0ddc\u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dcf\u0dbb\u0d9a \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db1\u0ddc\u0d9a\u0dbb L1 \u0dc0\u0dd0\u0db1\u0dd2 \u0dc0\u0dd9\u0db1\u0dad\u0dca \u0db6\u0dbb \u0daf\u0dd2\u0dbb\u0dcf\u0db4\u0dad\u0dca \u0dc0\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n<p>PyTorch\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0dc3\u0d82\u0d9a\u0dbd\u0dca\u0db4 \u0d9a\u0dd2\u0dc4\u0dd2\u0db4\u0dba\u0d9a\u0dca \u0db8\u0dd9\u0db1\u0dca\u0db1:</p>\n<h3>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca</h3>\n<p>PyTorch\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dbd\u0dd9\u0dc3 \u0dc4\u0dd0\u0db3\u0dd2\u0db1\u0dca\u0dc0\u0dd9\u0db1 \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dc0\u0dbd\u0da7 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0dba\u0dd2. \u0dc3\u0dd1\u0db8 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0d9a\u0da7\u0db8 \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad \u0dc0\u0dd0\u0db1\u0dd2 \u0dad\u0db8\u0db1\u0dca\u0d9c\u0dda\u0db8 \u0d85\u0db0\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dad\u0dd2\u0db6\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2\u0dba. </p>\n<p>\u0db6\u0ddc\u0dc4\u0ddd\u0db4\u0ddc\u0daf\u0dd4 \u0d85\u0dc0\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0db1\u0dca\u0dc4\u0dd2\u0daf\u0dd3 \u0d91\u0d9a\u0dca \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0d9a\u0dca \u0db4\u0db8\u0dab\u0d9a\u0dca \u0dc0\u0db1\u0dd4 \u0d87\u0dad. \u0db8\u0dd9\u0dba \u0d94\u0db6 \u0d94\u0db6\u0dda \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1 \u0dc0\u0dd2\u0da7,</p>\n<span translate=no>_^_0_^_</span><p>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba\u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda\u0daf\u0dd3 \u0d94\u0db6\u0da7 \u0db6\u0dc4\u0dd4 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0db1\u0dd2\u0dbb\u0dca\u0dc0\u0da0\u0db1\u0dba \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba:</p>\n<span translate=no>_^_1_^_</span><p>\u0db8\u0dd9\u0db1\u0dca\u0db1\u0d85\u0db4\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0d9a\u0dca \u0dc3\u0db8\u0dca\u0db8\u0dad \u0d9a\u0dbb\u0db8\u0dd4. \u0dc3\u0dd1\u0db8 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0d9a\u0dca\u0db8 '\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2' \u0dba\u0db1 \u0dba\u0dad\u0dd4\u0dbb \u0dba\u0da7\u0dad\u0dda \u0d91\u0dc4\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0dc1\u0db6\u0dca\u0daf\u0d9a\u0ddd\u0dc2\u0dba\u0d9a\u0dd2. \u0d94\u0db6 \u0d95\u0db1\u0dd1\u0db8 \u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0daf \u0dc3\u0db3\u0dc4\u0db1\u0dca \u0d9a\u0dbb\u0dba\u0dd2. \u0d85\u0db0\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0d92\u0dc0\u0dcf \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0db8\u0da7\u0dca\u0da7\u0db8\u0dda \u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2 \u0dc0\u0dd9\u0dad \u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2\u0dba \u0dc0\u0db1\u0dd4 \u0d87\u0dad. </p>\n<p>\u0d94\u0db6\u0da7\u0db8\u0dd9\u0db8 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc0\u0dbd\u0da7 \u0db4\u0dca\u0dbb\u0dc0\u0dda\u0dc1 \u0dc0\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2\u0dba (\u0dc3\u0dc4 \u0db4\u0dc0\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1), \u0dc3\u0dc4 \u0d92\u0dc0\u0dcf\u0dba\u0dda \u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0db8\u0d9f <span translate=no>_^_2_^_</span>. \u0db8\u0da7 \u0dc4\u0db8\u0dd4 \u0dc0\u0dd3 \u0d87\u0dad\u0dd2 \u0db6\u0ddc\u0dc4\u0ddd \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad \u0d9a\u0dcf\u0dbd\u0dc3\u0da7\u0dc4\u0db1\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0db8\u0dd9\u0dba\u0da7 \u0db4\u0dca\u0dbb\u0dc0\u0dda\u0dc1 \u0dc0\u0dd3 'lr' \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0d9a\u0dbb\u0dba\u0dd2. </p>\n<h3>\u0da2\u0db1\u0db4\u0daf\u0dba</h3>\n<p>\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba\u0dc1\u0db6\u0dca\u0daf\u0d9a\u0ddd\u0dc2\u0dba\u0d9a \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba \u0dc3\u0db3\u0dc4\u0dcf (\u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dca) \u0db4\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dad (\u0dc1\u0db6\u0dca\u0daf\u0d9a\u0ddd\u0dc2\u0dba\u0d9a\u0dca) \u0db4\u0dc0\u0dad\u0dca\u0dc0\u0dcf \u0d9c\u0db1\u0dd3 <span translate=no>_^_3_^_</span>. \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba on \u0dcf\u0dad\u0dd3\u0dba \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dba \u0dc0\u0dd0\u0db1\u0dd2 \u0daf\u0dda\u0dc0\u0dbd\u0dca \u0db4\u0dc0\u0dad\u0dca\u0dc0\u0dcf \u0d9c\u0dd9\u0db1 \u0dba\u0db1\u0dca\u0db1\u0dda \u0db8\u0dd9\u0dc4\u0dd2\u0daf\u0dd3\u0dba. </p>\n",
 "<h2>Base class for <em>Adam</em> and extensions</h2>\n": "<h2><em>\u0d86\u0daf\u0db8\u0dca</em> \u0dc3\u0dc4 \u0daf\u0dd2\u0d9c\u0dd4 \u0dc3\u0db3\u0dc4\u0dcf \u0db8\u0dd6\u0dbd\u0dd2\u0d9a \u0db4\u0db1\u0dca\u0dad\u0dd2\u0dba</h2>\n",
 "<h2>L2 Weight decay</h2>\n": "<h2>L2\u0dc3\u0dd2\u0dbb\u0dd4\u0dbb\u0dda \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba</h2>\n",
 "<h3>Initialize state for a given parameter tensor</h3>\n<p>This should be overridden with code to initialize <span translate=no>_^_0_^_</span> for parameters <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is the parameter group dictionary to which <span translate=no>_^_3_^_</span> belongs.</p>\n": "<h3>\u0daf\u0dd3\u0d87\u0dad\u0dd2 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba tensor \u0dc3\u0db3\u0dc4\u0dcf \u0dbb\u0dcf\u0da2\u0dca\u0dba \u0d86\u0dbb\u0db8\u0dca\u0db7</h3>\n<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca <span translate=no>_^_0_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db8\u0dd9\u0dba \u0d9a\u0dda\u0dad\u0dba \u0dc3\u0db8\u0d9f \u0d89\u0d9a\u0dca\u0db8\u0dc0\u0dcf \u0dba\u0dcf \u0dba\u0dd4\u0dad\u0dd4\u0dba <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc1\u0db6\u0dca\u0daf \u0d9a\u0ddd\u0dc2\u0dba <span translate=no>_^_3_^_</span> \u0d85\u0dba\u0dad\u0dca \u0dc0\u0dda. </p>\n",
 "<h3>Initialize weight decay</h3>\n<ul><li><span translate=no>_^_0_^_</span> is the decay coefficient </li>\n<li><span translate=no>_^_1_^_</span> is a flag indicating whether to add the weight decay to the gradient or directly decay from the parameter. If added to the gradient it will go through the normal optimizer update. </li>\n<li><span translate=no>_^_2_^_</span> this flag indicates whether the weight decay coefficient is absolute. This is applicable when the decay is performed directly on the parameter. If this is false the actual decay is <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>.</li></ul>\n": "<h3>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0d9a\u0dca\u0dc2\u0dba \u0dc3\u0d82\u0d9c\u0dd4\u0dab\u0d9a\u0dba \u0dc0\u0dda </li>\n<li><span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0da7 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4\u0daf \u0dba\u0db1\u0dca\u0db1 \u0dc4\u0ddd \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0dd9\u0db1\u0dca \u0d9a\u0dd9\u0dbd\u0dd2\u0db1\u0dca\u0db8 \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0db4\u0dd9\u0db1\u0dca\u0db1\u0dd4\u0db8\u0dca \u0d9a\u0dbb\u0db1 \u0db0\u0da2\u0dba\u0d9a\u0dd2. \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0da7 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dc5\u0dc4\u0ddc\u0dad\u0dca \u0d91\u0dba \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0dba\u0dcf\u0dc0\u0dad\u0dca\u0d9a\u0dcf\u0dbd\u0dd3\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d9c\u0db8\u0db1\u0dca \u0d9a\u0dbb\u0dba\u0dd2. </li>\n<li><span translate=no>_^_2_^_</span> \u0db8\u0dd9\u0db8 \u0db0\u0da2\u0dba \u0db8\u0d9f\u0dd2\u0db1\u0dca \u0db6\u0dbb \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8\u0dda \u0dc3\u0d82\u0d9c\u0dd4\u0dab\u0d9a\u0dba \u0db1\u0dd2\u0dbb\u0db4\u0dda\u0d9a\u0dca\u0dc2 \u0daf \u0dba\u0db1\u0dca\u0db1 \u0db4\u0dd9\u0db1\u0dca\u0db1\u0dd4\u0db8\u0dca \u0d9a\u0dbb\u0dba\u0dd2. \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba \u0db8\u0dad \u0dc3\u0dd8\u0da2\u0dd4\u0dc0\u0db8 \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb\u0db1 \u0dc0\u0dd2\u0da7 \u0db8\u0dd9\u0dba \u0d85\u0daf\u0dcf\u0dc5 \u0dc0\u0dda. \u0db8\u0dd9\u0dba \u0d85\u0dc3\u0dad\u0dca\u0dba\u0dba\u0d9a\u0dca \u0db1\u0db8\u0dca \u0dc3\u0dd0\u0db6\u0dd1 \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8\u0dba\u0dd2 <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>. </li></ul>\n",
 "<h3>Initialize</h3>\n<ul><li><span translate=no>_^_0_^_</span> is the collection of parameters or set of parameter groups. </li>\n<li><span translate=no>_^_1_^_</span> a dictionary of default hyper-parameters </li>\n<li><span translate=no>_^_2_^_</span> is the learning rate, <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is the tuple <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is <span translate=no>_^_7_^_</span></li></ul>\n": "<h3>\u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc4\u0ddd \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0db8\u0dd6\u0dc4\u0dba\u0d9a\u0dd2. </li>\n<li><span translate=no>_^_1_^_</span> \u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2 \u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca\u0d9c\u0dda \u0dc1\u0db6\u0dca\u0daf \u0d9a\u0ddd\u0dc2\u0dba\u0d9a\u0dca </li>\n<li><span translate=no>_^_2_^_</span> \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba, <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> \u0db8\u0dd9\u0db8 tuple \u0dc0\u0dda <span translate=no>_^_5_^_</span> </li>\n</ul><li><span translate=no>_^_6_^_</span> \u0dc0\u0dda <span translate=no>_^_7_^_</span></li>\n",
 "<h3>Optimizer step</h3>\n<p>We have created a template method that does the common stuff every <em>Adam</em> based optimizer needs.</p>\n": "<h3>\u0db4\u0dd2\u0dba\u0dc0\u0dbb\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba</h3>\n<p><em>\u0d86\u0daf\u0db8\u0dca</em> \u0db8\u0dad \u0db4\u0daf\u0db1\u0db8\u0dca \u0dc0\u0dd6 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0d85\u0dc0\u0dc1\u0dca\u0dba\u0dad\u0dcf \u0dc3\u0dd1\u0db8 \u0db4\u0ddc\u0daf\u0dd4 \u0daf\u0dda\u0dc0\u0dbd\u0dca \u0d9a\u0dbb\u0db1 \u0d85\u0da0\u0dca\u0da0\u0dd4 \u0d9a\u0dca\u0dbb\u0db8\u0dba\u0d9a\u0dca \u0d85\u0db4\u0dd2 \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dbb \u0d87\u0dad\u0dca\u0dad\u0dd9\u0db8\u0dd4. </p>\n",
 "<h3>Perform weight decay and return the gradient</h3>\n": "<h3>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dbb \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba \u0db1\u0dd0\u0dc0\u0dad \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1</h3>\n",
 "<h3>Take optimizer step on a parameter tensor</h3>\n<p>This should be overridden and take the optimization step on <span translate=no>_^_0_^_</span> tensor <span translate=no>_^_1_^_</span>, where <span translate=no>_^_2_^_</span> is the gradient for that parameter, <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span> is the optimizer state dictionary for that parameter, and <span translate=no>_^_5_^_</span> is the parameter group dictionary <span translate=no>_^_6_^_</span> belongs to.</p>\n": "<h3>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d86\u0dad\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0db8\u0dad \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0db1\u0dca\u0db1</h3>\n<p>\u0db8\u0dd9\u0db8overridden \u0dc4\u0dcf <span translate=no>_^_0_^_</span> tensor \u0db8\u0dad \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0dad \u0dba\u0dd4\u0dad\u0dd4 <span translate=no>_^_1_^_</span>, \u0d91\u0db8 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db5\u0dbd\u0dba \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0d9a\u0ddc\u0dc4\u0dd9\u0daf <span translate=no>_^_2_^_</span> , <span translate=no>_^_3_^_</span>, <span translate=no>_^_4_^_</span> \u0dba\u0db1\u0dd4 \u0d91\u0db8 \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0dbb\u0dcf\u0da2\u0dca\u0dba \u0dc1\u0db6\u0dca\u0daf \u0d9a\u0ddd\u0dc2\u0dba <span translate=no>_^_5_^_</span> \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca <span translate=no>_^_6_^_</span> \u0dc1\u0db6\u0dca\u0daf\u0d9a\u0ddd\u0dc2\u0dba\u0da7 \u0d85\u0dba\u0dad\u0dca \u0dc0\u0dda. </p>\n",
 "<p> Return defaults for parameter groups</p>\n": "<p> \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0d86\u0db4\u0dc3\u0dd4 \u0db4\u0dd0\u0dc4\u0dd0\u0dbb \u0dc4\u0dd0\u0dbb\u0dd3\u0db8\u0dca</p>\n",
 "<p>Add the hyper-parameters to the defaults </p>\n": "<p>\u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2\u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Add the weight decay to the gradient and return the modified gradient </p>\n": "<p>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0da7 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb \u0db1\u0dc0\u0dd3\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba \u0db1\u0dd0\u0dc0\u0dad \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1 </p>\n",
 "<p>Calculate loss.</p>\n<p>\ud83e\udd14 I&#x27;m not sure when you need this. I guess it&#x27;s if you define a function that calculates the loss, does <span translate=no>_^_0_^_</span> and return the loss, instead of calling it on your own you could pass it to <span translate=no>_^_1_^_</span>. \ud83e\udd37\u200d\u2642\ufe0f </p>\n": "<p>\u0d85\u0dbd\u0dcf\u0db7\u0dba\u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. </p>\n<p>\ud83e\udd14\u0d94\u0db6\u0da7 \u0db8\u0dd9\u0dba \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0dd2\u0da7 \u0db8\u0da7 \u0dc0\u0dd2\u0dc1\u0dca\u0dc0\u0dcf\u0dc3 \u0db1\u0dd0\u0dad. \u0db8\u0db8 \u0dc4\u0dd2\u0dad\u0db1\u0dca\u0db1\u0dda \u0d91\u0dba \u0d94\u0db6 \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1, \u0d9a\u0dbb\u0db1 <span translate=no>_^_0_^_</span> \u0dc3\u0dc4 \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba\u0d9a\u0dca \u0d85\u0dbb\u0dca\u0dae \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dd4\u0dc0\u0dc4\u0ddc\u0dad\u0dca, \u0d91\u0dba \u0dad\u0db1\u0dd2\u0dc0\u0db8 \u0d85\u0db8\u0dad\u0db1\u0dc0\u0dcf \u0dc0\u0dd9\u0db1\u0dd4\u0dc0\u0da7 \u0d94\u0db6\u0da7 \u0d91\u0dba \u0dc3\u0db8\u0dca\u0db8\u0dad \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba <span translate=no>_^_1_^_</span>. \ud83e\udd37\u200d\u2642\ufe0f </p>\n",
 "<p>Check hyper-parameters </p>\n": "<p>\u0d85\u0db0\u0dd2\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Check the hyper-parameters </p>\n": "<p>\u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca\u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the gradient tensor </p>\n": "<p>\u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0dda\u0d86\u0dad\u0dad\u0dd2\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the state for the parameter </p>\n": "<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0dc3\u0db3\u0dc4\u0dcf \u0dbb\u0dcf\u0da2\u0dca\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>If the weight decay coefficient is absolute </p>\n": "<p>\u0db6\u0dbb\u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8\u0dda \u0dc3\u0d82\u0d9c\u0dd4\u0dab\u0d9a\u0dba \u0db1\u0dd2\u0dbb\u0db4\u0dda\u0d9a\u0dca\u0dc2 \u0db1\u0db8\u0dca </p>\n",
 "<p>If we are doing the decay on the parameter directly </p>\n": "<p>\u0d85\u0db4\u0dd2\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba \u0db8\u0dad \u0d9a\u0dca\u0dc2\u0dba \u0dc0\u0dd3\u0db8 \u0d9a\u0dd9\u0dbd\u0dd2\u0db1\u0dca\u0db8 \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0db8\u0dca </p>\n",
 "<p>Initialize the PyTorch optimizer. This will create parameter groups with the default hyper-parameters </p>\n": "<p>PyTorch\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0db8\u0dd9\u0dba \u0db4\u0dd9\u0dbb\u0db1\u0dd2\u0db8\u0dd2 \u0d85\u0db0\u0dd2-\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2 \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0db1\u0dd2\u0dbb\u0dca\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dd4 \u0d87\u0dad </p>\n",
 "<p>Initialize the state if state is uninitialized </p>\n": "<p>\u0dbb\u0dcf\u0da2\u0dca\u0dbauninitialized \u0db1\u0db8\u0dca \u0dbb\u0dcf\u0da2\u0dca\u0dba \u0d86\u0dbb\u0db8\u0dca\u0db7 </p>\n",
 "<p>Iterate through the parameter groups </p>\n": "<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc4\u0dbb\u0dc4\u0dcf \u0db1\u0dd0\u0dc0\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Iterate through the parameters in the parameter group </p>\n": "<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dda \u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd3\u0db1\u0dca \u0dc4\u0dbb\u0dc4\u0dcf \u0db1\u0dd0\u0dc0\u0dad \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Otherwise, </p>\n": "<p>\u0d91\u0dc3\u0dda\u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca </p>\n",
 "<p>Return the loss, calculated from closure </p>\n": "<p>\u0dc0\u0dc3\u0dcf\u0daf\u0dd0\u0db8\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d85\u0dbd\u0dcf\u0db7\u0dba \u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1 </p>\n",
 "<p>Return the unmodified gradient </p>\n": "<p>\u0db1\u0dc0\u0dd3\u0d9a\u0dbb\u0dab\u0dba\u0db1\u0ddc\u0d9a\u0dc5 \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba \u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1\u0dca\u0db1 </p>\n",
 "<p>Skip if the parameter has no gradient </p>\n": "<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0da7\u0d9a\u0dd2\u0dc3\u0dd2\u0daf\u0dd4 \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0db8\u0d9f \u0dc4\u0dbb\u0dd2\u0db1\u0dca\u0db1 </p>\n",
 "<p>Take the optimization step on the parameter </p>\n": "<p>\u0db4\u0dbb\u0dcf\u0db8\u0dd2\u0dad\u0dd2\u0dba\u0db8\u0dad \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>We don&#x27;t handle sparse gradients </p>\n": "<p>\u0d85\u0db4\u0dd2\u0dc0\u0dd2\u0dbb\u0dbd \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0dc4\u0dd0\u0dc3\u0dd2\u0dbb\u0dc0\u0dd2\u0dba \u0db1\u0dd0\u0dc4\u0dd0 </p>\n",
 "A set of PyTorch implementations/tutorials of popular gradient descent based optimizers. Currently includes Adam, AMSGrad and RAdam optimizers.": "PyTorch \u0da2\u0db1\u0db4\u0dca\u0dbb\u0dd2\u0dba \u0dc1\u0dca\u0dbb\u0dda\u0dab\u0dd2\u0dba\u0dda \u0dc3\u0db8\u0dca\u0db7\u0dc0\u0dba \u0db4\u0daf\u0db1\u0db8\u0dca \u0d9a\u0dbb\u0d9c\u0dad\u0dca \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dc0\u0dbd PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1 \u0dc3\u0db8\u0dd6\u0dc4\u0dba\u0d9a\u0dca. \u0daf\u0dd0\u0db1\u0da7 \u0d86\u0daf\u0db8\u0dca, AMSGrad \u0dc3\u0dc4 RaDam \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba \u0d87\u0dad\u0dd4\u0dc5\u0dad\u0dca \u0dc0\u0dda.",
 "Optimizers": "\u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0d9a\u0dbb\u0dab\u0dba"
}