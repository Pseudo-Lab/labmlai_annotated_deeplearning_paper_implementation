{
 "<h1><a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">Batch Normalization</a></h1>\n": "<h1><a href=\"https://nn.labml.ai/normalization/batch_norm/index.html\">\u6279\u91cf\u6807\u51c6\u5316</a></h1>\n",
 "<h2>Inference</h2>\n": "<h2>\u63a8\u65ad</h2>\n",
 "<h2>Normalization</h2>\n": "<h2>\u89c4\u8303\u5316</h2>\n",
 "<h3>Batch Normalization</h3>\n": "<h3>\u6279\u91cf\u6807\u51c6\u5316</h3>\n",
 "<h3>Internal Covariate Shift</h3>\n": "<h3>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</h3>\n",
 "<h3>Normalizing outside gradient computation doesn&#x27;t work</h3>\n": "<h3>\u5bf9\u5916\u90e8\u68af\u5ea6\u8ba1\u7b97\u8fdb\u884c\u5f52\u4e00\u5316\u4e0d\u8d77\u4f5c\u7528</h3>\n",
 "<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/batch_norm/mnist.ipynb\"><span translate=no>_^_0_^_</span></a> </p>\n": "<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/batch_norm/mnist.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n",
 "<p>Batch normalization also makes the back propagation invariant to the scale of the weights and empirically it improves generalization, so it has regularization effects too.</p>\n": "<p>\u6279\u91cf\u5f52\u4e00\u5316\u8fd8\u4f7f\u53cd\u5411\u4f20\u64ad\u4e0e\u6743\u91cd\u7684\u6bd4\u4f8b\u4fdd\u6301\u4e0d\u53d8\uff0c\u4ece\u7ecf\u9a8c\u4e0a\u8bb2\uff0c\u5b83\u6539\u5584\u4e86\u6cdb\u5316\uff0c\u56e0\u6b64\u5b83\u4e5f\u5177\u6709\u6b63\u5219\u5316\u6548\u679c\u3002</p>\n",
 "<p>By stabilizing the distribution, batch normalization minimizes the internal covariate shift.</p>\n": "<p>\u901a\u8fc7\u7a33\u5b9a\u5206\u5e03\uff0c\u6279\u91cf\u5f52\u4e00\u5316\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u3002</p>\n",
 "<p>Here&#x27;s <a href=\"mnist.html\">the training code</a> and a notebook for training a CNN classifier that uses batch normalization for MNIST dataset.</p>\n": "<p><a href=\"mnist.html\">\u4ee5\u4e0b\u662f\u8bad\u7ec3\u4ee3\u7801</a>\u548c\u7528\u4e8e\u8bad\u7ec3 CNN \u5206\u7c7b\u5668\u7684\u7b14\u8bb0\u672c\uff0c\u8be5\u5206\u7c7b\u5668\u4f7f\u7528 MNIST \u6570\u636e\u96c6\u7684\u6279\u91cf\u5f52\u4e00\u5316\u3002</p>\n",
 "<p>Internal covariate shift will adversely affect training speed because the later layers (<span translate=no>_^_0_^_</span> in the above example) have to adapt to this shifted distribution.</p>\n": "<p>\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u5c06\u5bf9\u8bad\u7ec3\u901f\u5ea6\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u56e0\u4e3a\u540e\u9762\u7684\u56fe\u5c42\uff08\u5728\u4e0a\u9762\u7684\u4f8b\u5b50<span translate=no>_^_0_^_</span>\u4e2d\uff09\u5fc5\u987b\u9002\u5e94\u8fd9\u79cd\u504f\u79fb\u5206\u5e03\u3002</p>\n",
 "<p>It is known that whitening improves training speed and convergence. <em>Whitening</em> is linearly transforming inputs to have zero mean, unit variance, and be uncorrelated.</p>\n": "<p>\u4f17\u6240\u5468\u77e5\uff0c\u7f8e\u767d\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u548c\u6536\u655b\u6027\u3002<em>\u7f8e\u767d</em>\u662f\u5c06\u8f93\u5165\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u4f7f\u5176\u5747\u503c\u4e3a\u96f6\u3001\u5355\u4f4d\u65b9\u5dee\u4e14\u4e0d\u76f8\u5173\u3002</p>\n",
 "<p>Normalizing each feature to zero mean and unit variance could affect what the layer can represent. As an example paper illustrates that, if the inputs to a sigmoid are normalized most of it will be within <span translate=no>_^_0_^_</span> range where the sigmoid is linear. To overcome this each feature is scaled and shifted by two trained parameters <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span>. <span translate=no>_^_3_^_</span> where <span translate=no>_^_4_^_</span> is the output of the batch normalization layer.</p>\n": "<p>\u5c06\u6bcf\u4e2a\u8981\u7d20\u5f52\u4e00\u5316\u4e3a\u96f6\u5747\u503c\u548c\u5355\u4f4d\u65b9\u5dee\u53ef\u80fd\u4f1a\u5f71\u54cd\u56fe\u5c42\u53ef\u4ee5\u8868\u793a\u7684\u5185\u5bb9\u3002\u4f5c\u4e3a\u793a\u4f8b\u8bba\u6587\u8bf4\u660e\uff0c\u5982\u679csigmoid\u7684\u8f93\u5165\u88ab\u5f52\u4e00\u5316\uff0c\u5219\u5927\u90e8\u5206\u5c06\u5728sigmoid\u4e3a\u7ebf\u6027\u7684<span translate=no>_^_0_^_</span>\u8303\u56f4\u5185\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6bcf\u4e2a\u7279\u5f81\u90fd\u901a\u8fc7\u4e24\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u7f29\u653e<span translate=no>_^_1_^_</span>\u548c\u79fb\u52a8<span translate=no>_^_2_^_</span>\u3002<span translate=no>_^_3_^_</span>\u5176\u4e2d<span translate=no>_^_4_^_</span>\u662f\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u7684\u8f93\u51fa\u3002</p>\n",
 "<p>Normalizing outside the gradient computation using pre-computed (detached) means and variances doesn&#x27;t work. For instance. (ignoring variance), let <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> is a trained bias and <span translate=no>_^_3_^_</span> is an outside gradient computation (pre-computed constant).</p>\n": "<p>\u4f7f\u7528\u9884\u5148\u8ba1\u7b97\uff08\u5206\u79bb\uff09\u5747\u503c\u548c\u65b9\u5dee\u5728\u68af\u5ea6\u8ba1\u7b97\u4e4b\u5916\u8fdb\u884c\u5f52\u4e00\u5316\u4e0d\u8d77\u4f5c\u7528\u3002\u4f8b\u5982\u3002\uff08\u5ffd\u7565\u65b9\u5dee\uff09<span translate=no>_^_0_^_</span>\uff0c\u8ba9 wher<span translate=no>_^_1_^_</span> e an<span translate=no>_^_2_^_</span> d \u662f\u4e00\u4e2a\u8bad\u7ec3\u8fc7\u7684\u504f\u5dee\uff0c<span translate=no>_^_3_^_</span>\u662f\u5916\u90e8\u68af\u5ea6\u8ba1\u7b97\uff08\u9884\u5148\u8ba1\u7b97\u7684\u5e38\u91cf\uff09\u3002</p>\n",
 "<p>Note that <span translate=no>_^_0_^_</span> has no effect on <span translate=no>_^_1_^_</span>. Therefore, <span translate=no>_^_2_^_</span> will increase or decrease based <span translate=no>_^_3_^_</span>, and keep on growing indefinitely in each training update. The paper notes that similar explosions happen with variances.</p>\n": "<p>\u8bf7\u6ce8\u610f<span translate=no>_^_0_^_</span>\uff0c\u8fd9\u5bf9<span translate=no>_^_1_^_</span>\u3002\u56e0\u6b64\uff0c<span translate=no>_^_2_^_</span>\u5c06\u5728\u6bcf\u6b21\u8bad\u7ec3\u66f4\u65b0\u4e2d\u589e\u52a0\u6216\u51cf\u5c11<span translate=no>_^_3_^_</span>\uff0c\u5e76\u4e14\u4f1a\u65e0\u9650\u671f\u5730\u589e\u957f\u3002\u8be5\u62a5\u6307\u51fa\uff0c\u7c7b\u4f3c\u7684\u7206\u70b8\u4f1a\u53d1\u751f\u5dee\u5f02\u3002</p>\n",
 "<p>Note that when applying batch normalization after a linear transform like <span translate=no>_^_0_^_</span> the bias parameter <span translate=no>_^_1_^_</span> gets cancelled due to normalization. So you can and should omit bias parameter in linear transforms right before the batch normalization.</p>\n": "<p>\u8bf7\u6ce8\u610f\uff0c\u5728\u7ebf\u6027\u53d8\u6362\u4e4b\u540e\u5e94\u7528\u6279\u91cf\u5f52\u4e00\u5316\u65f6\uff0c\u6bd4\u5982<span translate=no>_^_0_^_</span>\u504f\u7f6e\u53c2\u6570<span translate=no>_^_1_^_</span>\u4f1a\u56e0\u5f52\u4e00\u5316\u800c\u88ab\u53d6\u6d88\u3002\u56e0\u6b64\uff0c\u4f60\u53ef\u4ee5\u800c\u4e14\u5e94\u8be5\u5728\u6279\u91cf\u5f52\u4e00\u5316\u4e4b\u524d\u7701\u7565\u7ebf\u6027\u53d8\u6362\u4e2d\u7684\u504f\u7f6e\u53c2\u6570\u3002</p>\n",
 "<p>The paper defines <em>Internal Covariate Shift</em> as the change in the distribution of network activations due to the change in network parameters during training. For example, let&#x27;s say there are two layers <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span>. During the beginning of the training <span translate=no>_^_2_^_</span> outputs (inputs to <span translate=no>_^_3_^_</span>) could be in distribution <span translate=no>_^_4_^_</span>. Then, after some training steps, it could move to <span translate=no>_^_5_^_</span>. This is <em>internal covariate shift</em>.</p>\n": "<p>\u672c\u6587\u5c06<em>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</em>\u5b9a\u4e49\u4e3a\u8bad\u7ec3\u671f\u95f4\u7531\u4e8e\u7f51\u7edc\u53c2\u6570\u7684\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u7f51\u7edc\u6fc0\u6d3b\u5206\u5e03\u7684\u53d8\u5316\u3002\u4f8b\u5982\uff0c\u5047\u8bbe\u6709\u4e24\u5c42<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span>\u3002\u5728\u57f9\u8bad\u5f00\u59cb\u65f6\uff0c\u53ef\u4ee5\u5206\u53d1<span translate=no>_^_2_^_</span>\u8f93\u51fa\uff08\u8f93\u5165<span translate=no>_^_3_^_</span>\uff09<span translate=no>_^_4_^_</span>\u3002\u7136\u540e\uff0c\u7ecf\u8fc7\u4e00\u4e9b\u8bad\u7ec3\u6b65\u9aa4\u540e\uff0c\u5b83\u53ef\u80fd\u4f1a\u79fb\u81f3<span translate=no>_^_5_^_</span>\u3002\u8fd9\u662f<em>\u5185\u90e8\u534f\u53d8\u91cf\u79fb\u4f4d</em>\u3002</p>\n",
 "<p>The paper introduces a simplified version which they call <em>Batch Normalization</em>. First simplification is that it normalizes each feature independently to have zero mean and unit variance: <span translate=no>_^_0_^_</span> where <span translate=no>_^_1_^_</span> is the <span translate=no>_^_2_^_</span>-dimensional input.</p>\n": "<p>\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7b80\u5316\u7248\u672c\uff0c\u4ed6\u4eec\u79f0\u4e4b\u4e3a<em>\u6279\u91cf\u89c4\u8303\u5316</em>\u3002\u9996\u5148\u7b80\u5316\u7684\u662f\uff0c\u5b83\u5c06\u6bcf\u4e2a\u8981\u7d20\u72ec\u7acb\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u548c\u5355\u4f4d\u65b9\u5dee\u4e3a\u96f6\uff1a<span translate=no>_^_0_^_</span>\u5176\u4e2d<span translate=no>_^_1_^_</span>\u662f<span translate=no>_^_2_^_</span>\u7ef4\u5ea6\u8f93\u5165\u3002</p>\n",
 "<p>The second simplification is to use estimates of mean <span translate=no>_^_0_^_</span> and variance <span translate=no>_^_1_^_</span> from the mini-batch for normalization; instead of calculating the mean and variance across the whole dataset.</p>\n": "<p>\u7b2c\u4e8c\u79cd\u7b80\u5316\u65b9\u6cd5\u662f\u4f7f\u7528<span translate=no>_^_1_^_</span>\u6765\u81ea\u5fae\u578b\u6279\u6b21\u7684\u5747\u503c<span translate=no>_^_0_^_</span>\u548c\u65b9\u5dee\u7684\u4f30\u8ba1\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff1b\u800c\u4e0d\u662f\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002</p>\n",
 "<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of Batch Normalization from paper  <a href=\"https://papers.labml.ai/paper/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</p>\n": "<p>\u8fd9\u662f <a href=\"https://pytorch.org\">PyTorch</a> \u4ece\u7eb8\u8d28\u6279\u91cf\u89c4\u8303\u5316\u4e2d<a href=\"https://papers.labml.ai/paper/1502.03167\">\u5b9e\u73b0\u6279\u91cf\u5f52\u4e00\u5316\uff1a\u901a\u8fc7\u51cf\u5c11\u5185\u90e8\u534f\u53d8\u91cf\u504f\u79fb\u52a0\u901f\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3</a>\u3002</p>\n",
 "<p>We need to know <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> in order to perform the normalization. So during inference, you either need to go through the whole (or part of) dataset and find the mean and variance, or you can use an estimate calculated during training. The usual practice is to calculate an exponential moving average of mean and variance during the training phase and use that for inference.</p>\n": "<p>\u6211\u4eec\u9700\u8981\u77e5\u9053 an<span translate=no>_^_0_^_</span> d<span translate=no>_^_1_^_</span> \u624d\u80fd\u6267\u884c\u89c4\u8303\u5316\u3002\u56e0\u6b64\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u60a8\u8981\u4e48\u9700\u8981\u904d\u5386\u6574\u4e2a\uff08\u6216\u90e8\u5206\uff09\u6570\u636e\u96c6\u5e76\u627e\u5230\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8981\u4e48\u53ef\u4ee5\u4f7f\u7528\u8bad\u7ec3\u671f\u95f4\u8ba1\u7b97\u7684\u4f30\u8ba1\u503c\u3002\u901a\u5e38\u7684\u505a\u6cd5\u662f\u5728\u8bad\u7ec3\u9636\u6bb5\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\uff0c\u7136\u540e\u5c06\u5176\u7528\u4e8e\u63a8\u65ad\u3002</p>\n",
 "<p>Whitening is computationally expensive because you need to de-correlate and the gradients must flow through the full whitening calculation.</p>\n": "<p>\u7f8e\u767d\u5728\u8ba1\u7b97\u4e0a\u5f88\u6602\u8d35\uff0c\u56e0\u4e3a\u4f60\u9700\u8981\u53bb\u5173\u8054\uff0c\u800c\u4e14\u68af\u5ea6\u5fc5\u987b\u901a\u8fc7\u5b8c\u6574\u7684\u7f8e\u767d\u8ba1\u7b97\u3002</p>\n",
 "Batch Normalization": "\u6279\u91cf\u6807\u51c6\u5316"
}