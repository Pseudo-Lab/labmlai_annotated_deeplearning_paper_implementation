{
 "<h1>DeepNorm</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the DeepNorm from the paper <a href=\"https://papers.labml.ai/paper/2203.00555\">DeepNet: Scaling Transformers to 1,000 Layers</a>.</p>\n<p>The paper proposes a method to stabilize extremely deep transformers through a new normalizing function to replace LayerNorm and a weight initialization scheme. This combines the performance of Post-LayerNorm and the stability of Pre-LayerNorm. Transformers with DeepNorms are supposed to be stable even without a learning rate warm-up.</p>\n<p>The paper first shows that the changes to layer outputs (for the same input)  change gradually during stable training; when unstable it changes rapidly during the initial training steps. This happens with initializing weights to small values, and learning rate warm-ups where the training is stable. They use the idea of keeping the changes to layer outputs small to derive the new  normalization and weight initialization mechanism.</p>\n<h2>Weight Initializations</h2>\n<p>Usually, the weights are initialized with Xavier or Kaiming initializations. This paper scales (sets the gain) the weights by a constant <span translate=no>_^_1_^_</span> depending on the size of the  transformer.</p>\n<p>DeepNorm suggests scaling the weights of the two linear transforms in the <a href=\"../../transformers/feed_forward.html\">Feed-Forward Network</a>, the value projection transform, and the output projection transform of the attention layer. Weights of these transforms are scaled by (has a gain equal to) <span translate=no>_^_2_^_</span>.</p>\n<p>The scaling is implemented in the</p>\n<h2>Normalization Function</h2>\n<p><span translate=no>_^_3_^_</span></p>\n<p>where <span translate=no>_^_4_^_</span> is a constant that depends on the depth of the transformer,  <span translate=no>_^_5_^_</span> is <a href=\"../layer_norm/index.html\">Layer Normalization</a>, and  <span translate=no>_^_6_^_</span> is the function of the <span translate=no>_^_7_^_</span>-th transformer sub-layer (FFN or attention).</p>\n<p>This function is used to replace Post-LayerNorm.</p>\n<h2><span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span> constants</h2>\n<span translate=no>_^_10_^_</span><p>Where <span translate=no>_^_11_^_</span> is the number of layers in the encoder and <span translate=no>_^_12_^_</span> is the number of layers in the decoder.</p>\n<p>Refer to <a href=\"https://papers.labml.ai/paper/2203.00555\">the paper</a> for derivation.</p>\n<p><a href=\"experiment.html\">Here is an experiment implementation</a> that uses DeepNorm.</p>\n": "<h1>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/normalization/deep_norm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://papers.labml.ai/paper/2203.00555\">\u0da9\u0dd3\u0db4\u0dca\u0db1\u0dd9\u0da7\u0dca \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0da9\u0dd3\u0db4\u0dca \u0db1\u0ddd\u0db8\u0dca \u0dc4\u0dd2 <a href=\"https://pytorch.org\">\u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca</a> \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8: \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca 1,000 \u0dc3\u0dca\u0dae\u0dbb \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8</a>.</p>\n<p>\u0dbd\u0dda\u0dba\u0dbb\u0dca \u0db1\u0ddd\u0db8\u0dca \u0dc3\u0dc4 \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dba\u0ddd\u0da2\u0db1\u0dcf \u0d9a\u0dca\u0dbb\u0db8\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db1\u0dc0 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba\u0d9a\u0dca \u0dc4\u0dbb\u0dc4\u0dcf \u0d85\u0dad\u0dd2\u0dc1\u0dba \u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d9a\u0dca\u0dbb\u0db8\u0dba\u0d9a\u0dca \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dba\u0ddd\u0da2\u0db1\u0dcf \u0d9a\u0dbb\u0dba\u0dd2. \u0db8\u0dd9\u0dba \u0db4\u0dc1\u0dca\u0da0\u0dcf\u0dad\u0dca-\u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba\u0dda \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0d9a\u0dcf\u0dbb\u0dd2\u0dad\u0dca\u0dc0\u0dba \u0dc3\u0dc4 \u0db4\u0dd6\u0dbb\u0dca\u0dc0 \u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba\u0dda \u0dc3\u0dca\u0dae\u0dcf\u0dba\u0dd2\u0dad\u0dcf\u0dc0 \u0d92\u0d9a\u0dcf\u0db6\u0daf\u0dca\u0db0 \u0d9a\u0dbb\u0dba\u0dd2. DeepNorms \u0dc3\u0db8\u0d9f \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d89\u0d9c\u0dd9\u0db1\u0dd3\u0db8\u0dda \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba\u0d9a\u0dca \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2\u0dc0 \u0db4\u0dc0\u0dcf \u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0dba \u0dba\u0dd4\u0dad\u0dd4\u0dba.</p>\n<p>\u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0db4\u0dc5\u0db8\u0dd4 \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0d9a\u0db8\u0dca (\u0d91\u0db8 \u0d86\u0daf\u0dcf\u0db1 \u0dc3\u0db3\u0dc4\u0dcf) \u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0dad\u0dd4\u0dc5 \u0d9a\u0dca\u0dbb\u0db8\u0dba\u0dd9\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0db6\u0dc0 \u0db4\u0dd9\u0db1\u0dca\u0db1\u0dd4\u0db8\u0dca; \u0d85\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0da7 \u0d91\u0dba \u0db8\u0dd6\u0dbd\u0dd2\u0d9a \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0db4\u0dd2\u0dba\u0dc0\u0dbb \u0dad\u0dd4\u0dc5 \u0dc0\u0dda\u0d9c\u0dba\u0dd9\u0db1\u0dca \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0dc0\u0dda. \u0db8\u0dd9\u0dba \u0dc3\u0dd2\u0daf\u0dd4\u0dc0\u0db1\u0dca\u0db1\u0dda \u0d9a\u0dd4\u0da9\u0dcf \u0d85\u0d9c\u0dba\u0db1\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4\u0dc0 \u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0db1 \u0d89\u0d9c\u0dd9\u0db1\u0dd4\u0db8\u0dca \u0d85\u0db1\u0dd4\u0db4\u0dcf\u0dad\u0dba \u0d8b\u0dab\u0dd4\u0dc3\u0dd4\u0db8\u0dca-\u0d89\u0dc4\u0dc5 \u0dba\u0dcf\u0db8\u0dba\u0dd2. \u0db1\u0dc0 \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba \u0dc3\u0dc4 \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0dba\u0dcf\u0db1\u0dca\u0dad\u0dca\u0dbb\u0dab\u0dba \u0dc0\u0dca\u0dba\u0dd4\u0dad\u0dca\u0db4\u0db1\u0dca\u0db1 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0dc3\u0dca\u0dae\u0dbb \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dc0\u0dbd \u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0d9a\u0db8\u0dca \u0d9a\u0dd4\u0da9\u0dcf \u0d9a\u0dbb \u0dad\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0dda \u0d85\u0daf\u0dc4\u0dc3 \u0d94\u0dc0\u0dd4\u0db1\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2.</p>\n<h2>\u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a\u0d9a\u0dbb\u0dab\u0dba</h2>\n<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0dba\u0dd9\u0db1\u0dca, \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dd4 \u0dbd\u0db6\u0db1\u0dca\u0db1\u0dda \u0dc3\u0dda\u0dc0\u0dd2\u0dba\u0dbb\u0dca \u0dc4\u0ddd \u0d9a\u0dba\u0dd2\u0db8\u0dd2\u0d82 \u0db8\u0dd4\u0dbd\u0db4\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0dc3\u0db8\u0d9f \u0dba. \u0db8\u0dd9\u0db8 \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba\u0db1\u0dca (\u0dc0\u0dcf\u0dc3\u0dd2 \u0dc3\u0d9a\u0dc3\u0dba\u0dd2) \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba<span translate=no>_^_1_^_</span> \u0d85\u0db1\u0dd4\u0dc0 \u0db1\u0dd2\u0dba\u0dad \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dca \u0db6\u0dbb.</p>\n<p>DeepNorm \u0dba\u0ddd\u0da2\u0db1\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda <a href=\"../../transformers/feed_forward.html\">Feed-Forward \u0da2\u0dcf\u0dbd\u0dba\u0dda</a> \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1 \u0daf\u0dd9\u0d9a\u0dd9\u0dc4\u0dd2 \u0db6\u0dbb \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8, \u0d85\u0d9c\u0dba \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0dc4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1 \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab\u0dba \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dba\u0dd2. \u0db8\u0dd9\u0db8 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0dda \u0db6\u0dbb \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dbb \u0d87\u0dad (\u0dc3\u0db8\u0dcf\u0db1 \u0dc0\u0dcf\u0dc3\u0dd2\u0dba\u0d9a\u0dca<span translate=no>_^_2_^_</span> \u0d87\u0dad).</p>\n<p>\u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0dc0\u0dda</p>\n<h2>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba</h2>\n<p><span translate=no>_^_3_^_</span></p>\n<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba\u0dda \u0d9c\u0dd0\u0db9\u0dd4\u0dbb \u0db8\u0dad \u0dbb\u0db3\u0dcf \u0db4\u0dc0\u0dad\u0dd2\u0db1 \u0db1\u0dd2\u0dba\u0dad\u0dba\u0d9a\u0dca<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span> \u0dba\u0db1\u0dd4 <a href=\"../layer_norm/index.html\">\u0dc3\u0dca\u0dae\u0dbb \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba</a> \u0dc0\u0db1 \u0d85\u0dad\u0dbb<span translate=no>_^_6_^_</span> \u0d91\u0dba<span translate=no>_^_7_^_</span> -th \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0d8b\u0db4-\u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda (FFN \u0dc4\u0ddd \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba) \u0dc0\u0dda.</p>\n<p>\u0db8\u0dd9\u0db8 \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba \u0db4\u0dc1\u0dca\u0da0\u0dcf\u0dad\u0dca-\u0dc3\u0dca\u0dae\u0dbb \u0db1\u0dd2\u0dba\u0db8\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0dba\u0dd2.</p>\n<h2><span translate=no>_^_8_^_</span>\u0dc3\u0dc4<span translate=no>_^_9_^_</span> \u0db1\u0dd2\u0dba\u0dad\u0dba\u0db1\u0dca</h2>\n<span translate=no>_^_10_^_</span><p>\u0d91\u0db1\u0dca\u0d9a\u0ddd\u0da9\u0dbb\u0dba\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0d9c\u0dab\u0db1<span translate=no>_^_11_^_</span> \u0d9a\u0ddc\u0dad\u0dd0\u0db1\u0daf \u0dc3\u0dc4<span translate=no>_^_12_^_</span> \u0dc0\u0dd2\u0d9a\u0dda\u0dad\u0d9a\u0dba\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0d9c\u0dab\u0db1 \u0dc0\u0dda.</p>\n<p>\u0dc0\u0dca\u0dba\u0dd4\u0dad\u0dca\u0db4\u0db1\u0dca\u0db1 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf <a href=\"https://papers.labml.ai/paper/2203.00555\">\u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2</a> \u0dc0\u0dd9\u0dad \u0dba\u0ddc\u0db8\u0dd4 \u0dc0\u0db1\u0dca\u0db1.</p>\n<p>DeepNorm \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 <a href=\"experiment.html\">\u0d85\u0dad\u0dca\u0dc4\u0daf\u0dcf \u0db6\u0dd0\u0dbd\u0dd3\u0db8\u0dda \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0d9a\u0dca \u0db8\u0dd9\u0db1\u0dca\u0db1</a>.</p>\n",
 "<h2>DeepNorm Normalization</h2>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h2>\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba</h2>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<h2>Transformer Decoder Layer with DeepNorm</h2>\n<p>This implements a transformer decoder layer with DeepNorm. Encoder layers will have a similar form.</p>\n": "<h2>\u0da9\u0dd3\u0db4\u0dca\u0db1\u0ddd\u0db8\u0dca \u0dc3\u0db8\u0d9f \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc0\u0dd2\u0d9a\u0dda\u0dad\u0d9a \u0dc3\u0dca\u0dae\u0dbb\u0dba</h2>\n<p>\u0db8\u0dd9\u0dbaDeepNorm \u0dc3\u0db8\u0d9f \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc0\u0dd2\u0d9a\u0dda\u0dad\u0d9a \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0dba\u0dd2. \u0d91\u0db1\u0dca\u0d9a\u0ddd\u0da9\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0dc0\u0dbd\u0da7 \u0dc3\u0db8\u0dcf\u0db1 \u0dc3\u0dca\u0dc0\u0dbb\u0dd6\u0db4\u0dba\u0d9a\u0dca \u0d87\u0dad. </p>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span> </p>\n",
 "<p>Attention output project </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc0\u0dca\u0dba\u0dcf\u0db4\u0dd8\u0dad\u0dd2\u0dba </p>\n",
 "<p>Attention value projection </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0d85\u0d9c\u0dba \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0db1\u0dba </p>\n",
 "<p>Create causal mask </p>\n": "<p>\u0dc4\u0dda\u0dad\u0dd4\u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>DeepNorms after attention and feed forward network </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0dd9\u0db1\u0dca\u0db4\u0dc3\u0dd4 DeepNorms \u0dc3\u0dc4 \u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0da2\u0dcf\u0dbd\u0dba \u0db4\u0ddd\u0dc2\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Feed forward network linear transformations </p>\n": "<p>\u0da2\u0dcf\u0dbd\u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0db1\u0dca \u0d89\u0daf\u0dd2\u0dbb\u0dd2\u0dba\u0da7 \u0db4\u0ddd\u0dc2\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Initialize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a\u0dbb\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Pass through the feed-forward network </p>\n": "<p>Feed-forward\u0da2\u0dcf\u0dbd\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0d9c\u0db8\u0db1\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Run through self attention, i.e. keys and values are from self </p>\n": "<p>\u0dc3\u0dca\u0dc0\u0dba\u0d82\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc4\u0dbb\u0dc4\u0dcf \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1, i.e. \u0dba\u0dad\u0dd4\u0dbb\u0dd4 \u0dc3\u0dc4 \u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0dca \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0dc3\u0dd2\u0da7 </p>\n",
 "<p>Scale weights after initialization </p>\n": "<p>\u0d86\u0dbb\u0db8\u0dca\u0db7\u0dba\u0dd9\u0db1\u0dca\u0db4\u0dc3\u0dd4 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab \u0db6\u0dbb </p>\n",
 "<p>Subsequent mask, will mask out tokens from seeing future tokens </p>\n": "<p>\u0db4\u0dc3\u0dd4\u0d9a\u0dcf\u0dbd\u0dd3\u0db1\u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab, \u0d85\u0db1\u0dcf\u0d9c\u0dad \u0da7\u0ddd\u0d9a\u0db1 \u0daf\u0dd0\u0d9a\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0da7\u0ddd\u0d9a\u0db1 \u0dc0\u0dc3\u0d82 \u0d9a\u0dbb\u0db1\u0dd4 \u0d87\u0dad </p>\n",
 "<p>The mask will be initialized on the first call </p>\n": "<p>\u0db4\u0dc5\u0db8\u0dd4\u0d87\u0db8\u0dad\u0dd4\u0db8\u0dd9\u0db1\u0dca \u0dc0\u0dd9\u0dc3\u0dca\u0db8\u0dd4\u0dc4\u0dd4\u0dab \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0db1\u0dd4 \u0d87\u0dad </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  are the embeddings of shape <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dda <span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the shape for LayerNorm <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  is <span translate=no>_^_5_^_</span> for LayerNorm </li>\n<li><span translate=no>_^_6_^_</span>  is a flag indicating whether to do an elementwise transformation in LayerNorm</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dc0\u0dda <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> \u0dc3\u0dca\u0dae\u0dbb \u0db1\u0ddd\u0db8\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dd0\u0da9\u0dba\u0dba\u0dd2 <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> \u0dc3\u0dca\u0dae\u0dbb \u0db1\u0ddd\u0db8\u0dca <span translate=no>_^_5_^_</span> \u0dc3\u0db3\u0dc4\u0dcf \u0dc0\u0dda </li>\n<li><span translate=no>_^_6_^_</span> LayerNorm \u0dc4\u0dd2 \u0db8\u0dd6\u0dbd\u0dd2\u0d9a \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba\u0d9a\u0dca \u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4\u0daf \u0dba\u0db1\u0dca\u0db1 \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dd9\u0db1 \u0db0\u0da2\u0dba\u0d9a\u0dd2</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the output from the previous layer <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the output of the current sub-layer <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0db4\u0dd9\u0dbb \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc0\u0dda <span translate=no>_^_1_^_</span> </li>\n</ul><li><span translate=no>_^_2_^_</span> \u0dc0\u0dad\u0dca\u0db8\u0db1\u0dca \u0d8b\u0db4 \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0dda \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0dc0\u0dda <span translate=no>_^_3_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the token embedding size </li>\n<li><span translate=no>_^_1_^_</span>  is the self attention module </li>\n<li><span translate=no>_^_2_^_</span>  is the feed forward module </li>\n<li><span translate=no>_^_3_^_</span>  is <span translate=no>_^_4_^_</span> coefficient in DeepNorm </li>\n<li><span translate=no>_^_5_^_</span>  is <span translate=no>_^_6_^_</span> constant for scaling weights initialization</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2 </li>\n<li><span translate=no>_^_1_^_</span> \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0d86\u0dc4\u0dcf\u0dbb \u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba\u0dba\u0dd2 </li>\n<li><span translate=no>_^_3_^_</span> DeepNorm \u0dc4\u0dd2 <span translate=no>_^_4_^_</span> \u0dc3\u0d82\u0d9c\u0dd4\u0dab\u0d9a\u0dba </li>\n<li><span translate=no>_^_5_^_</span> \u0db6\u0dbb \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf <span translate=no>_^_6_^_</span> \u0db1\u0dd2\u0dba\u0dad \u0dc0\u0dda</li></ul>\n",
 "A PyTorch implementation/tutorial of DeepNorm from paper DeepNet: Scaling Transformers to 1,000 Layers.": "\u0da9\u0dd3\u0db4\u0dca\u0db1\u0dd9\u0da7\u0dca \u0d9a\u0da9\u0daf\u0dcf\u0dc3\u0dd2 \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0da9\u0dd3\u0db4\u0dca\u0db1\u0ddd\u0db8\u0dca \u0dc4\u0dd2 \u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba: \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca 1,000 \u0dc3\u0dca\u0dae\u0dbb \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8.",
 "DeepNorm": "\u0d9c\u0dd0\u0db9\u0dd4\u0dbb\u0dd4 \u0dc3\u0db8\u0dca\u0db8\u0dad\u0dba"
}