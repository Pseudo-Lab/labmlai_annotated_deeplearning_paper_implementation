{
 "<h1>Layer Normalization</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of <a href=\"https://papers.labml.ai/paper/1607.06450\">Layer Normalization</a>.</p>\n<h3>Limitations of <a href=\"../batch_norm/index.html\">Batch Normalization</a></h3>\n<ul><li>You need to maintain running means. </li>\n<li>Tricky for RNNs. Do you need different normalizations for each step? </li>\n<li>Doesn&#x27;t work with small batch sizes; large NLP models are usually trained with small batch sizes. </li>\n<li>Need to compute means and variances across devices in distributed training.</li></ul>\n<h2>Layer Normalization</h2>\n<p>Layer normalization is a simpler normalization method that works on a wider range of settings. Layer normalization transforms the inputs to have zero mean and unit variance across the features. <em>Note that batch normalization fixes the zero mean and unit variance for each element.</em> Layer normalization does it for each batch across all elements.</p>\n<p>Layer normalization is generally used for NLP tasks.</p>\n<p>We have used layer normalization in most of the <a href=\"../../transformers/gpt/index.html\">transformer implementations</a>.</p>\n": "<h1>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</h1>\n<p><a href=\"https://papers.labml.ai/paper/1607.06450\">\u3053\u308c\u306f\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306e</a> <a href=\"https://pytorch.org\">PyTorch</a> \u5b9f\u88c5\u3067\u3059\u3002</p>\n<h3><a href=\"../batch_norm/index.html\">\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u5236\u9650\u4e8b\u9805</a></h3>\n<ul><li>\u30e9\u30f3\u30cb\u30f3\u30b0\u624b\u6bb5\u3092\u7dad\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li>\n<li>RNN\u306b\u3068\u3063\u3066\u306f\u6271\u3044\u306b\u304f\u3044\u3002\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7570\u306a\u308b\u6b63\u898f\u5316\u304c\u5fc5\u8981\u3067\u3059\u304b</li>?\n<li>\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u306f\u6a5f\u80fd\u3057\u307e\u305b\u3093\u3002\u5927\u898f\u6a21\u306aNLP\u30e2\u30c7\u30eb\u306f\u901a\u5e38\u3001\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u307e\u3059\u3002</li>\n</ul><li>\u5206\u6563\u578b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u306f\u3001\u30c7\u30d0\u30a4\u30b9\u9593\u306e\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li>\n<h2>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</h2>\n<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306f\u3001\u3088\u308a\u5e45\u5e83\u3044\u8a2d\u5b9a\u306b\u9069\u7528\u3067\u304d\u308b\u3001\u3088\u308a\u5358\u7d14\u306a\u6b63\u898f\u5316\u65b9\u6cd5\u3067\u3059\u3002\u5c64\u306e\u6b63\u898f\u5316\u306b\u3088\u308a\u3001\u5165\u529b\u306f\u7279\u5fb4\u5168\u4f53\u3067\u5e73\u5747\u304c\u30bc\u30ed\u3067\u5358\u4f4d\u5206\u6563\u304c\u306a\u304f\u306a\u308b\u3088\u3046\u306b\u5909\u63db\u3055\u308c\u307e\u3059\u3002<em>\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u3001\u5404\u8981\u7d20\u306e\u30bc\u30ed\u5e73\u5747\u3068\u5358\u4f4d\u5206\u6563\u304c\u56fa\u5b9a\u3055\u308c\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002</em>\u30ec\u30a4\u30e4\u30fc\u306e\u6b63\u898f\u5316\u306f\u3001\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u30d0\u30c3\u30c1\u3054\u3068\u306b\u6b63\u898f\u5316\u3092\u884c\u3044\u307e\u3059</p>\u3002\n<p>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306f\u901a\u5e38\u3001NLP \u30bf\u30b9\u30af\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p>\n<p><a href=\"../../transformers/gpt/index.html\">\u307b\u3068\u3093\u3069\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5b9f\u88c5\u3067\u5c64\u306e\u6b63\u898f\u5316\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059</a>\u3002</p>\n",
 "<h2>Layer Normalization</h2>\n<p>Layer normalization <span translate=no>_^_0_^_</span> normalizes the input <span translate=no>_^_1_^_</span> as follows:</p>\n<p>When input <span translate=no>_^_2_^_</span> is a batch of embeddings, where <span translate=no>_^_3_^_</span> is the batch size and <span translate=no>_^_4_^_</span> is the number of features. <span translate=no>_^_5_^_</span> and <span translate=no>_^_6_^_</span>. <span translate=no>_^_7_^_</span></p>\n<p>When input <span translate=no>_^_8_^_</span> is a batch of a sequence of embeddings, where <span translate=no>_^_9_^_</span> is the batch size, <span translate=no>_^_10_^_</span> is the number of channels, <span translate=no>_^_11_^_</span> is the length of the sequence. <span translate=no>_^_12_^_</span> and <span translate=no>_^_13_^_</span>. <span translate=no>_^_14_^_</span></p>\n<p>When input <span translate=no>_^_15_^_</span> is a batch of image representations, where <span translate=no>_^_16_^_</span> is the batch size, <span translate=no>_^_17_^_</span> is the number of channels, <span translate=no>_^_18_^_</span> is the height and <span translate=no>_^_19_^_</span> is the width. This is not a widely used scenario. <span translate=no>_^_20_^_</span> and <span translate=no>_^_21_^_</span>. <span translate=no>_^_22_^_</span></p>\n": "<h2>\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316</h2>\n<p><span translate=no>_^_0_^_</span>\u5c64\u306e\u6b63\u898f\u5316\u306f\u3001<span translate=no>_^_1_^_</span>\u5165\u529b\u3092\u6b21\u306e\u3088\u3046\u306b\u6b63\u898f\u5316\u3057\u307e\u3059\u3002</p>\n<p>\u5165\u529b\u304c\u57cb\u3081\u8fbc\u307f\u306e\u30d0\u30c3\u30c1\u306e\u5834\u5408\u3001<span translate=no>_^_2_^_</span>\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001<span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u306f\u30d5\u30a3\u30fc\u30c1\u30e3\u306e\u6570\u3067\u3059\u3002<span translate=no>_^_5_^_</span>\u3068<span translate=no>_^_6_^_</span>\u3002<span translate=no>_^_7_^_</span></p>\n<p><span translate=no>_^_8_^_</span>\u5165\u529b\u304c\u57cb\u3081\u8fbc\u307f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u30d0\u30c3\u30c1\u3067\u3042\u308b\u5834\u5408\u3001<span translate=no>_^_9_^_</span>\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001<span translate=no>_^_10_^_</span>\u306f\u30c1\u30e3\u30cd\u30eb\u6570\u3001<span translate=no>_^_11_^_</span>\u306f\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055\u3067\u3059\u3002<span translate=no>_^_12_^_</span>\u3068<span translate=no>_^_13_^_</span>\u3002<span translate=no>_^_14_^_</span></p>\n<p><span translate=no>_^_15_^_</span>\u5165\u529b\u304c\u30a4\u30e1\u30fc\u30b8\u8868\u73fe\u306e\u30d0\u30c3\u30c1\u306e\u5834\u5408\u3001<span translate=no>_^_16_^_</span>\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001<span translate=no>_^_17_^_</span>\u306f\u30c1\u30e3\u30cd\u30eb\u6570\u3001<span translate=no>_^_18_^_</span>\u306f\u9ad8\u3055\u3001<span translate=no>_^_19_^_</span>\u306f\u5e45\u3067\u3059\u3002\u3053\u308c\u306f\u3042\u307e\u308a\u4f7f\u308f\u308c\u3066\u3044\u306a\u3044\u30b7\u30ca\u30ea\u30aa\u3067\u3059\u3002<span translate=no>_^_20_^_</span>\u3068<span translate=no>_^_21_^_</span>\u3002<span translate=no>_^_22_^_</span></p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> <span translate=no>_^_0_^_</span> is a tensor of shape <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> could be any number of dimensions.  For example, in an NLP task this will be <span translate=no>_^_3_^_</span></p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5f62\u72b6\u306e\u30c6\u30f3\u30bd\u30eb\u3067\u3059\u3002<span translate=no>_^_2_^_</span>\u6b21\u5143\u306f\u3044\u304f\u3064\u3067\u3082\u304b\u307e\u3044\u307e\u305b\u3093\u3002\u305f\u3068\u3048\u3070\u3001NLP \u30bf\u30b9\u30af\u3067\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059</p>\u3002<span translate=no>_^_3_^_</span>\n",
 "<p> Simple test</p>\n": "<p>\u7c21\u5358\u306a\u30c6\u30b9\u30c8</p>\n",
 "<p>Calculate the mean of all elements; i.e. the means for each element <span translate=no>_^_0_^_</span> </p>\n": "<p>\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u5e73\u5747\u3001\u3064\u307e\u308a\u5404\u8981\u7d20\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Calculate the squared mean of all elements; i.e. the means for each element <span translate=no>_^_0_^_</span> </p>\n": "<p>\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u4e8c\u4e57\u5e73\u5747\u3001\u3064\u307e\u308a\u5404\u8981\u7d20\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u307e\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Convert <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u306b\u5909\u63db <span translate=no>_^_1_^_</span></p>\n",
 "<p>Create parameters for <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> for gain and bias </p>\n": "<p><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u30b2\u30a4\u30f3\u3068\u30d0\u30a4\u30a2\u30b9\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u4f5c\u6210</p>\n",
 "<p>Normalize <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30fc\u30de\u30e9\u30a4\u30ba <span translate=no>_^_0_^_</span></p>\n",
 "<p>Sanity check to make sure the shapes match </p>\n": "<p>\u5f62\u72b6\u304c\u5408\u3063\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u30b5\u30cb\u30c6\u30a3\u30c1\u30a7\u30c3\u30af</p>\n",
 "<p>Scale and shift <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b9\u30b1\u30fc\u30eb\u3068\u30b7\u30d5\u30c8 <span translate=no>_^_0_^_</span></p>\n",
 "<p>The dimensions to calculate the mean and variance on </p>\n": "<p>\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\u3059\u308b\u5bfe\u8c61\u306e\u30c7\u30a3\u30e1\u30f3\u30b7\u30e7\u30f3</p>\n",
 "<p>Variance of all element <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5168\u8981\u7d20\u306e\u5dee\u7570 <span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> <span translate=no>_^_1_^_</span> is the shape of the elements (except the batch).  The input should then be  <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is <span translate=no>_^_4_^_</span>, used in <span translate=no>_^_5_^_</span> for numerical stability </li>\n<li><span translate=no>_^_6_^_</span> is whether to scale and shift the normalized value</li></ul>\n<p>We&#x27;ve tried to use the same names for arguments as PyTorch <span translate=no>_^_7_^_</span> implementation.</p>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u8981\u7d20\u306e\u5f62\u72b6\u3067\u3059 (\u30d0\u30c3\u30c1\u306f\u9664\u304f)\u3002\u305d\u306e\u5834\u5408\u3001\u5165\u529b\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002<span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span><span translate=no>_^_5_^_</span>\u6570\u5024\u306e\u5b89\u5b9a\u6027\u306e\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059 <span translate=no>_^_4_^_</span></li>\n<li><span translate=no>_^_6_^_</span>\u6b63\u898f\u5316\u3055\u308c\u305f\u5024\u3092\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3057\u3066\u30b7\u30d5\u30c8\u3059\u308b\u304b\u3069\u3046\u304b\u3067\u3059</li></ul>\n<p>\u5f15\u6570\u306b\u306f PyTorch <span translate=no>_^_7_^_</span> \u5b9f\u88c5\u3068\u540c\u3058\u540d\u524d\u3092\u4f7f\u7528\u3057\u3088\u3046\u3068\u3057\u307e\u3057\u305f\u3002</p>\n",
 "A PyTorch implementation/tutorial of layer normalization.": "\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316\u306ePyTorch\u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3002",
 "Layer Normalization": "\u30ec\u30a4\u30e4\u30fc\u6b63\u898f\u5316"
}