{
 "<h1>Gradient Penalty for Wasserstein GAN (WGAN-GP)</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1704.00028\">Improved Training of Wasserstein GANs</a>.</p>\n<p><a href=\"../index.html\">WGAN</a> suggests clipping weights to enforce Lipschitz constraint on the discriminator network (critic). This and other weight constraints like L2 norm clipping, weight normalization, L1, L2 weight decay have problems:</p>\n<p>1. Limiting the capacity of the discriminator 2. Exploding and vanishing gradients (without <a href=\"../../../normalization/batch_norm/index.html\">Batch Normalization</a>).</p>\n<p>The paper <a href=\"https://papers.labml.ai/paper/1704.00028\">Improved Training of Wasserstein GANs</a> proposal a better way to improve Lipschitz constraint, a gradient penalty.</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the penalty weight and</p>\n<span translate=no>_^_2_^_</span><p>That is we try to keep the gradient norm <span translate=no>_^_3_^_</span> close to <span translate=no>_^_4_^_</span>.</p>\n<p>In this implementation we set <span translate=no>_^_5_^_</span>.</p>\n<p>Here is the <a href=\"experiment.html\">code for an experiment</a> that uses gradient penalty.</p>\n": "<h1>\u30ef\u30c3\u30b5\u30fc\u30b9\u30bf\u30a4\u30f3 GAN (WGAN-GP) \u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u30da\u30ca\u30eb\u30c6\u30a3</h1>\n<p>\u3053\u308c\u306f\u3001<a href=\"https://papers.labml.ai/paper/1704.00028\">\u30f4\u30a1\u30c3\u30b5\u30fc\u30b9\u30bf\u30a4\u30f3GAN\u306e\u6539\u826f\u578b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u88c5\u3067\u3059</a>\u3002</p>\n<p><a href=\"../index.html\">WGAN\u306f</a>\u3001\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u30fb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u30ea\u30c3\u30d7\u30b7\u30c3\u30c4\u5236\u7d04\u3092\u9069\u7528\u3059\u308b\u305f\u3081\u306b\u30a6\u30a7\u30a4\u30c8\u3092\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u3059\u308b\u3053\u3068\u3092\u63d0\u6848\u3057\u3066\u3044\u308b\uff08\u8a55\u8ad6\u5bb6\uff09\u3002\u3053\u308c\u306b\u52a0\u3048\u3066\u3001L2 \u30ce\u30eb\u30e0\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u3001\u30a6\u30a7\u30a4\u30c8\u6b63\u898f\u5316\u3001L1\u3001L2 \u30a6\u30a7\u30a4\u30c8\u6e1b\u8870\u306a\u3069\u306e\u4ed6\u306e\u30a6\u30a7\u30a4\u30c8\u5236\u7d04\u306b\u306f\u554f\u984c\u304c\u3042\u308a\u307e\u3059</p>\u3002\n<p>1\u3002\u30c7\u30a3\u30b9\u30af\u30ea\u30df\u30cd\u30fc\u30bf\u30fc\u306e\u5bb9\u91cf\u5236\u9650 2.<a href=\"../../../normalization/batch_norm/index.html\">\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u304c\u7206\u767a\u3057\u305f\u308a\u6d88\u3048\u305f\u308a\u3059\u308b (\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306a\u3057)</a></p>\n<p>\u8ad6\u6587\u300c<a href=\"https://papers.labml.ai/paper/1704.00028\">Wasserstein GAN\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u6539\u5584</a>\u300d\u306f\u3001\u52fe\u914d\u30da\u30ca\u30eb\u30c6\u30a3\u3067\u3042\u308b\u30ea\u30c3\u30d7\u30b7\u30c3\u30c4\u5236\u7d04\u3092\u6539\u5584\u3059\u308b\u3088\u308a\u826f\u3044\u65b9\u6cd5\u3092\u63d0\u6848\u3057\u3066\u3044\u307e\u3059\u3002</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span>\u30da\u30ca\u30eb\u30c6\u30a3\u30a6\u30a7\u30a4\u30c8\u306f\u3069\u3053\u3067</p>\n<span translate=no>_^_2_^_</span><p>\u3064\u307e\u308a\u3001<span translate=no>_^_3_^_</span>\u52fe\u914d\u306e\u30ce\u30eb\u30e0\u3092\u8fd1\u304f\u306b\u4fdd\u3064\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\u3002<span translate=no>_^_4_^_</span></p>\n<p><span translate=no>_^_5_^_</span>\u3053\u306e\u5b9f\u88c5\u3067\u306f\u8a2d\u5b9a\u3057\u307e\u3057\u305f\u3002</p>\n<p><a href=\"experiment.html\">\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u4f7f\u7528\u3059\u308b\u5b9f\u9a13\u306e\u30b3\u30fc\u30c9\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059</a>\u3002</p>\n",
 "<h2>Gradient Penalty</h2>\n": "<h2>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u30da\u30ca\u30eb\u30c6\u30a3</h2>\n",
 "<p>Calculate gradients of <span translate=no>_^_0_^_</span> with respect to <span translate=no>_^_1_^_</span>. <span translate=no>_^_2_^_</span> is set to ones since we want the gradients of <span translate=no>_^_3_^_</span>, and we need to create and retain graph since we have to compute gradients with respect to weight on this loss. </p>\n": "<p><span translate=no>_^_0_^_</span>\u3092\u57fa\u6e96\u3068\u3057\u305f\u52fe\u914d\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002<span translate=no>_^_1_^_</span><span translate=no>_^_2_^_</span>\u306e\u52fe\u914d\u3092\u6c42\u3081\u3066\u3044\u308b\u306e\u30671\u306b\u8a2d\u5b9a\u3057<span translate=no>_^_3_^_</span>\u3001\u3053\u306e\u640d\u5931\u306e\u91cd\u307f\u306b\u5bfe\u3059\u308b\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u6301\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</p>\u3002\n",
 "<p>Calculate the norm <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30ce\u30eb\u30e0\u306e\u8a08\u7b97 <span translate=no>_^_0_^_</span></p>\n",
 "<p>Get batch size </p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u53d6\u5f97</p>\n",
 "<p>Reshape gradients to calculate the norm </p>\n": "<p>\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5f62\u3092\u5909\u3048\u3066\u30ce\u30eb\u30e0\u3092\u8a08\u7b97\u3057\u3088\u3046</p>\n",
 "<p>Return the loss <span translate=no>_^_0_^_</span> </p>\n": "<p>\u640d\u5931\u3092\u8fd4\u3059 <span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span></li></ul>\n<p><span translate=no>_^_4_^_</span> since we set <span translate=no>_^_5_^_</span> for this implementation.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u306f <span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u306f <span translate=no>_^_3_^_</span></li>\n<p><span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\u3053\u306e\u5b9f\u88c5\u306b\u7740\u624b\u3057\u305f\u304b\u3089\u3067\u3059\u3002</p>\n",
 "An annotated PyTorch implementation/tutorial of\n Improved Training of Wasserstein GANs.": "\u6ce8\u91c8\u4ed8\u304d\u306e PyTorch \u5b9f\u88c5/\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\n \u30ef\u30c3\u30b5\u30fc\u30b9\u30bf\u30a4\u30f3GAN\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304c\u6539\u5584\u3055\u308c\u307e\u3057\u305f\u3002",
 "Gradient Penalty for Wasserstein GAN (WGAN-GP)": "\u30ef\u30c3\u30b5\u30fc\u30b9\u30bf\u30a4\u30f3 GAN (WGAN-GP) \u306e\u30b0\u30e9\u30c7\u30fc\u30b7\u30e7\u30f3\u30da\u30ca\u30eb\u30c6\u30a3"
}