{
 "<h1>Wasserstein GAN (WGAN)</h1>\n<p>This is an implementation of <a href=\"https://papers.labml.ai/paper/1701.07875\">Wasserstein GAN</a>.</p>\n<p>The original GAN loss is based on Jensen-Shannon (JS) divergence between the real distribution <span translate=no>_^_0_^_</span> and generated distribution <span translate=no>_^_1_^_</span>. The Wasserstein GAN is based on Earth Mover distance between these distributions.</p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span> is the set of all joint distributions, whose marginal probabilities are <span translate=no>_^_4_^_</span>.</p>\n<p><span translate=no>_^_5_^_</span> is the earth mover distance for a given joint distribution (<span translate=no>_^_6_^_</span> and <span translate=no>_^_7_^_</span> are probabilities).</p>\n<p>So <span translate=no>_^_8_^_</span> is equal to the least earth mover distance for any joint distribution between the real distribution <span translate=no>_^_9_^_</span> and generated distribution <span translate=no>_^_10_^_</span>.</p>\n<p>The paper shows that Jensen-Shannon (JS) divergence and other measures for the difference between two probability distributions are not smooth. And therefore if we are doing gradient descent on one of the probability distributions (parameterized) it will not converge.</p>\n<p>Based on Kantorovich-Rubinstein duality, <span translate=no>_^_11_^_</span></p>\n<p>where <span translate=no>_^_12_^_</span> are all 1-Lipschitz functions.</p>\n<p>That is, it is equal to the greatest difference <span translate=no>_^_13_^_</span> among all 1-Lipschitz functions.</p>\n<p>For <span translate=no>_^_14_^_</span>-Lipschitz functions, <span translate=no>_^_15_^_</span></p>\n<p>If all <span translate=no>_^_16_^_</span>-Lipschitz functions can be represented as <span translate=no>_^_17_^_</span> where <span translate=no>_^_18_^_</span> is parameterized by <span translate=no>_^_19_^_</span>,</p>\n<p><span translate=no>_^_20_^_</span></p>\n<p>If <span translate=no>_^_21_^_</span> is represented by a generator <span translate=no>_^_22_^_</span> and <span translate=no>_^_23_^_</span> is from a known distribution <span translate=no>_^_24_^_</span>,</p>\n<p><span translate=no>_^_25_^_</span></p>\n<p>Now to converge <span translate=no>_^_26_^_</span> with <span translate=no>_^_27_^_</span> we can gradient descent on <span translate=no>_^_28_^_</span> to minimize above formula.</p>\n<p>Similarly we can find <span translate=no>_^_29_^_</span> by ascending on <span translate=no>_^_30_^_</span>, while keeping <span translate=no>_^_31_^_</span> bounded. <em>One way to keep <span translate=no>_^_32_^_</span> bounded is to clip all weights in the neural network that defines <span translate=no>_^_33_^_</span> clipped within a range.</em></p>\n<p>Here is the code to try this on a <a href=\"experiment.html\">simple MNIST generation experiment</a>.</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/wasserstein/experiment.ipynb\"><span translate=no>_^_34_^_</span></a></p>\n": "<h1>Wasserstein GAN (WGAN)</h1>\n<p>\u8fd9\u662f <a href=\"https://papers.labml.ai/paper/1701.07875\">Wasserstein GAN</a> \u7684\u5b9e\u73b0\u3002</p>\n<p>\u6700\u521d\u7684GAN\u635f\u8017\u57fa\u4e8e\u5b9e\u9645\u5206\u5e03<span translate=no>_^_0_^_</span>\u548c\u751f\u6210\u7684\u5206\u5e03\u4e4b\u95f4\u7684Jensen-Shannon\uff08JS\uff09\u5dee\u5f02<span translate=no>_^_1_^_</span>\u3002Wasserstein GAN \u57fa\u4e8e\u8fd9\u4e9b\u5206\u5e03\u4e4b\u95f4\u7684 Earth Mover \u8ddd\u79bb\u3002</p>\n<p><span translate=no>_^_2_^_</span></p>\n<p><span translate=no>_^_3_^_</span>\u662f\u6240\u6709\u8054\u5408\u5206\u5e03\u7684\u96c6\u5408\uff0c\u5176\u8fb9\u9645\u6982\u7387\u4e3a<span translate=no>_^_4_^_</span>\u3002</p>\n<p><span translate=no>_^_5_^_</span>\u662f\u7ed9\u5b9a\u5173\u8282\u5206\u5e03\u7684\u5730\u7403\u79fb\u52a8\u5668\u8ddd\u79bb\uff08<span translate=no>_^_6_^_</span><span translate=no>_^_7_^_</span>\u4e5f\u662f\u6982\u7387\uff09\u3002</p>\n<p>\u56e0\u6b64\uff0c\u7b49<span translate=no>_^_8_^_</span>\u4e8e\u5b9e\u9645\u5206\u5e03<span translate=no>_^_9_^_</span>\u548c\u751f\u6210\u7684\u5206\u5e03\u4e4b\u95f4\u4efb\u4f55\u5173\u8282\u5206\u5e03\u7684\u6700\u5c0f\u5730\u7403\u79fb\u52a8\u5668\u8ddd\u79bb<span translate=no>_^_10_^_</span>\u3002</p>\n<p>\u672c\u6587\u8868\u660e\uff0cJensen-Shannon\uff08JS\uff09\u80cc\u79bb\u548c\u5176\u4ed6\u8861\u91cf\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u5dee\u5f02\u7684\u5ea6\u91cf\u5e76\u4e0d\u5e73\u6ed1\u3002\u56e0\u6b64\uff0c\u5982\u679c\u6211\u4eec\u5bf9\u5176\u4e2d\u4e00\u4e2a\u6982\u7387\u5206\u5e03\uff08\u53c2\u6570\u5316\uff09\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u5b83\u5c06\u4e0d\u4f1a\u6536\u655b\u3002</p>\n<p>\u57fa\u4e8e\u574e\u6258\u7f57\u7ef4\u5947-\u9c81\u5bbe\u65af\u5766\u4e8c\u5143\u6027\uff0c<span translate=no>_^_11_^_</span></p>\n<p>\u6240\u6709<span translate=no>_^_12_^_</span>\u7684 1-Lipschitz \u51fd\u6570\u90fd\u5728\u54ea\u91cc\u3002</p>\n<p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u7b49\u4e8e\u6240\u6709 1-Lipschitz \u51fd\u6570<span translate=no>_^_13_^_</span>\u4e4b\u95f4\u7684\u6700\u5927\u5dee\u5f02\u3002</p>\n<p>\u5bf9\u4e8e<span translate=no>_^_14_^_</span>-Lipschitz \u51fd\u6570\uff0c<span translate=no>_^_15_^_</span></p>\n<p>\u5982\u679c\u6240\u6709<span translate=no>_^_16_^_</span>-Lipschitz \u51fd\u6570\u90fd\u53ef\u4ee5\u8868\u793a<span translate=no>_^_18_^_</span>\u4e3a\u53c2\u6570\u5316\u4e86<span translate=no>_^_17_^_</span>\u54ea\u91cc<span translate=no>_^_19_^_</span>\uff0c</p>\n<p><span translate=no>_^_20_^_</span></p>\n<p>\u5982\u679c<span translate=no>_^_21_^_</span>\u7531\u751f\u6210\u5668\u8868\u793a<span translate=no>_^_22_^_</span><span translate=no>_^_23_^_</span>\u5e76\u4e14\u6765\u81ea\u5df2\u77e5\u5206\u5e03<span translate=no>_^_24_^_</span>\uff0c</p>\n<p><span translate=no>_^_25_^_</span></p>\n<p>\u73b0\u5728\u4e3a\u4e86\u6536\u655b<span translate=no>_^_26_^_</span>\uff0c<span translate=no>_^_27_^_</span>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d<span translate=no>_^_28_^_</span>\u6765\u6700\u5c0f\u5316\u4e0a\u8ff0\u516c\u5f0f\u3002</p>\n<p>\u540c\u6837\uff0c\u6211\u4eec\u53ef\u4ee5<span translate=no>_^_29_^_</span>\u901a\u8fc7\u4e0a\u5347\u6765\u627e\u5230<span translate=no>_^_30_^_</span>\uff0c\u540c\u65f6\u4fdd\u6301<span translate=no>_^_31_^_</span>\u754c\u9650\u3002<em>\u4fdd\u6301<span translate=no>_^_32_^_</span>\u754c\u9650\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u88c1\u526a\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b9a\u4e49\u8303\u56f4\u5185\u7684<span translate=no>_^_33_^_</span>\u88c1\u526a\u7684\u6240\u6709\u6743\u91cd\u3002</em></p>\n<p>\u4ee5\u4e0b\u662f\u5728\u4e00\u4e2a<a href=\"experiment.html\">\u7b80\u5355\u7684 MNIST \u751f\u6210\u5b9e\u9a8c</a>\u4e2d\u5c1d\u8bd5\u6b64\u64cd\u4f5c\u7684\u4ee3\u7801\u3002</p>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/wasserstein/experiment.ipynb\"><span translate=no>_^_34_^_</span></a></p>\n",
 "<h2>Discriminator Loss</h2>\n<p>We want to find <span translate=no>_^_0_^_</span> to maximize <span translate=no>_^_1_^_</span>, so we minimize, <span translate=no>_^_2_^_</span></p>\n": "<h2>\u9274\u522b\u5668\u4e22\u5931</h2>\n<p>\u6211\u4eec\u60f3\u627e\u5230\u6700\u5927\u5316<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\uff0c\u6240\u4ee5\u6211\u4eec\u6700\u5c0f\u5316\uff0c<span translate=no>_^_2_^_</span></p>\n",
 "<h2>Generator Loss</h2>\n<p>We want to find <span translate=no>_^_0_^_</span> to minimize <span translate=no>_^_1_^_</span> The first component is independent of <span translate=no>_^_2_^_</span>, so we minimize, <span translate=no>_^_3_^_</span></p>\n": "<h2>\u53d1\u7535\u673a\u635f\u5931</h2>\n<p>\u6211\u4eec<span translate=no>_^_0_^_</span>\u60f3\u627e\u5230\u6700\u5c0f\u5316<span translate=no>_^_1_^_</span>\u7b2c\u4e00\u4e2a\u7ec4\u4ef6\u662f\u72ec\u7acb\u7684<span translate=no>_^_2_^_</span>\uff0c\u6240\u4ee5\u6211\u4eec\u6700\u5c0f\u5316\uff0c<span translate=no>_^_3_^_</span></p>\n",
 "<p>We use ReLUs to clip the loss to keep <span translate=no>_^_0_^_</span> range. </p>\n": "<p>\u6211\u4eec\u4f7f\u7528 RELUs \u6765\u524a\u51cf\u635f\u5931\u4ee5\u4fdd\u6301<span translate=no>_^_0_^_</span>\u5c04\u7a0b\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span></li></ul>\n<p>This returns the a tuple with losses for <span translate=no>_^_4_^_</span> and <span translate=no>_^_5_^_</span>, which are later added. They are kept separate for logging.</p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u662f<span translate=no>_^_3_^_</span></li>\n<p>\u8fd9\u5c06\u8fd4\u56de\u5e26\u6709 and \u4e8f\u635f\u7684 a \u5143\u7ec4<span translate=no>_^_4_^_</span><span translate=no>_^_5_^_</span>\uff0c\u7a0d\u540e\u4f1a\u6dfb\u52a0\u8fd9\u4e9b\u5143\u7ec4\u3002\u5b83\u4eec\u5206\u5f00\u5b58\u653e\u4ee5\u8fdb\u884c\u65e5\u5fd7\u8bb0\u5f55\u3002</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span></li></ul>\n",
 "A simple PyTorch implementation/tutorial of Wasserstein Generative Adversarial Networks (WGAN) loss functions.": "Wasserstein \u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08WGAN\uff09\u635f\u5931\u51fd\u6570\u7684\u7b80\u5355\u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\u3002",
 "Wasserstein GAN (WGAN)": "Wasserstein GAN (WGAN)"
}