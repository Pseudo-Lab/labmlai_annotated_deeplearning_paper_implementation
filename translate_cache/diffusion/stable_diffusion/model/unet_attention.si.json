{
 "<h1>Transformer for Stable Diffusion <a href=\"unet.html\">U-Net</a></h1>\n<p>This implements the transformer module used in <a href=\"unet.html\">U-Net</a> that  gives <span translate=no>_^_0_^_</span></p>\n<p>We have kept to the model definition and naming unchanged from <a href=\"https://github.com/CompVis/stable-diffusion\">CompVis/stable-diffusion</a> so that we can load the checkpoints directly.</p>\n": "<h1>\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0dc3\u0dbb\u0dab\u0dba <a href=\"unet.html\">\u0dba\u0dd6-\u0db1\u0dd9\u0da7\u0dca</a> \u0dc3\u0db3\u0dc4\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca</h1>\n<p>\u0db8\u0dd9\u0dba \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0db1 <a href=\"unet.html\">\u0dba\u0dd6-\u0db1\u0dd9\u0da7\u0dca</a> \u0dc4\u0dd2 \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1 \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0db8\u0ddc\u0da9\u0dd2\u0dba\u0dd4\u0dbd\u0dba \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0dba\u0dd2<span translate=no>_^_0_^_</span></p>\n<p>\u0d85\u0db4\u0dd2 \u0d86\u0daf\u0dbb\u0dca\u0dc1 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0dd0\u0d9a\u0dca\u0dc0\u0dd3\u0db8 \u0dad\u0db6\u0dcf \u0d87\u0dad\u0dd2 \u0d85\u0dad\u0dbb <a href=\"https://github.com/CompVis/stable-diffusion\">\u0d9a\u0ddc\u0db8\u0dca\u0dc0\u0dd2\u0dc3\u0dca/\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0dc3\u0dbb\u0dab</a> \u0dc3\u0dd2\u0da7 \u0db1\u0ddc\u0dc0\u0dd9\u0db1\u0dc3\u0dca\u0dc0 \u0db1\u0db8\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0d85\u0db4\u0da7 \u0db8\u0dd4\u0dbb\u0db4\u0ddc\u0dbd\u0dc0\u0dbd\u0dca \u0d9a\u0dd9\u0dbd\u0dd2\u0db1\u0dca\u0db8 \u0db4\u0dd0\u0da7\u0dc0\u0dd2\u0dba \u0dc4\u0dd0\u0d9a\u0dd2 \u0dc0\u0db1 \u0db4\u0dbb\u0dd2\u0daf\u0dd2.</p>\n",
 "<h2>Spatial Transformer</h2>\n": "<h2>\u0d85\u0dc0\u0d9a\u0dcf\u0dc1\u0dd3\u0dba \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca</h2>\n",
 "<h3>Cross Attention Layer</h3>\n<p>This falls-back to self-attention when conditional embeddings are not specified.</p>\n": "<h3>\u0dc4\u0dbb\u0dc3\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba</h3>\n<p>\u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0db1\u0dd2\u0dc1\u0dca\u0da0\u0dd2\u0dad\u0dc0 \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0dc0\u0dd2\u0da7 \u0db8\u0dd9\u0dba \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0da7 \u0dba\u0ddc\u0db8\u0dd4 \u0dc0\u0dda.</p>\n",
 "<h3>Feed-Forward Network</h3>\n": "<h3>Feed-\u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0da2\u0dcf\u0dbd\u0dba</h3>\n",
 "<h3>GeGLU Activation</h3>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h3>Glu \u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8</h3>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<h3>Transformer Layer</h3>\n": "<h3>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb\u0dba</h3>\n",
 "<h4>Flash Attention</h4>\n<ul><li><span translate=no>_^_0_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_5_^_</span></li></ul>\n": "<h4>\u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</h4>\n<ul><li><span translate=no>_^_0_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_5_^_</span></li></ul>\n",
 "<h4>Normal Attention</h4>\n<ul><li><span translate=no>_^_0_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span>  are the query vectors before splitting heads, of shape <span translate=no>_^_5_^_</span></li></ul>\n": "<h4>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</h4>\n<ul><li><span translate=no>_^_0_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_1_^_</span></li>\n<li><span translate=no>_^_2_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0dd3\u0db8\u0da7 \u0db4\u0dd9\u0dbb \u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca \u0daf\u0ddb\u0dc1\u0dd2\u0d9a, \u0dc4\u0dd0\u0da9\u0dba\u0dd9\u0db1\u0dca<span translate=no>_^_5_^_</span></li></ul>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Add residual </p>\n": "<p>\u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p>Apply the transformer layers </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0dba\u0ddc\u0daf\u0db1\u0dca\u0db1</p>\n",
 "<p>Attention scaling factor </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab \u0dc3\u0dcf\u0db0\u0d9a\u0dba</p>\n",
 "<p>Calculate attention <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>Combined linear projections <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0d92\u0d9a\u0dcf\u0db6\u0daf\u0dca\u0db0 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0db4\u0dca\u0dbb\u0d9a\u0dca\u0dc2\u0dda\u0db4\u0dab<span translate=no>_^_0_^_</span> \u0dc3\u0dc4<span translate=no>_^_1_^_</span></p>\n",
 "<p>Compute attention <span translate=no>_^_0_^_</span> This gives a tensor of shape <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span> \u0db8\u0dd9\u0dba \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0dad\u0dad\u0dd2\u0d9a\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2<span translate=no>_^_1_^_</span></p>\n",
 "<p>Compute attention output <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1\u0dba \u0d9c\u0dab\u0db1\u0dba<span translate=no>_^_0_^_</span></p>\n",
 "<p>Compute softmax <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0dc3\u0ddc\u0dc6\u0dca\u0da7\u0dca\u0db8\u0dd0\u0d9a\u0dca\u0dc3\u0dca \u0d9c\u0dab\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>Cross attention layer and pre-norm layer </p>\n": "<p>\u0dc4\u0dbb\u0dc3\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dc3\u0dc4 \u0db4\u0dd9\u0dbb-\u0dc3\u0db8\u0dca\u0db8\u0dad \u0dc3\u0dca\u0dae\u0dbb\u0dba</p>\n",
 "<p>Cross-attention with conditioning </p>\n": "<p>\u0d9a\u0db1\u0dca\u0da9\u0dd2\u0dc2\u0db1\u0dda\u0dc2\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0dc4\u0dbb\u0dc3\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</p>\n",
 "<p>Feed-forward network </p>\n": "<p>Feed-\u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0da2\u0dcf\u0dbd\u0dba</p>\n",
 "<p>Feed-forward network and pre-norm layer </p>\n": "<p>Feed-\u0d89\u0daf\u0dd2\u0dbb\u0dd2 \u0da2\u0dcf\u0dbd\u0dba \u0dc3\u0dc4 \u0db4\u0dd9\u0dbb-\u0dc3\u0db8\u0dca\u0db8\u0dad \u0dc3\u0dca\u0dae\u0dbb\u0dba</p>\n",
 "<p>Final <span translate=no>_^_0_^_</span> convolution </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1<span translate=no>_^_0_^_</span> \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd2\u0db8</p>\n",
 "<p>Final linear layer </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba</p>\n",
 "<p>Flash attention works for head sizes <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span>, so we have to pad the heads to fit this size. </p>\n": "<p>\u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc4\u0dd2\u0dc3\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf \u0d9a\u0dbb\u0dba\u0dd2<span translate=no>_^_0_^_</span><span translate=no>_^_2_^_</span>,<span translate=no>_^_1_^_</span> \u0dc3\u0dc4, \u0d91\u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dca \u0db8\u0dd9\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0da7 \u0dc3\u0dbb\u0dd2\u0dbd\u0db1 \u0db4\u0dbb\u0dd2\u0daf\u0dd2 \u0dc4\u0dd2\u0dc3\u0dca \u0db4\u0dd1\u0da9\u0dca \u0d9a\u0dc5 \u0dba\u0dd4\u0dad\u0dd4\u0dba.</p>\n",
 "<p>For residual connection </p>\n": "<p>\u0d85\u0dc0\u0dc1\u0dda\u0dc2 \u0dc3\u0db8\u0dca\u0db6\u0db1\u0dca\u0db0\u0dad\u0dcf\u0dc0\u0dba \u0dc3\u0db3\u0dc4\u0dcf</p>\n",
 "<p>Get <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0dbd\u0db6\u0dcf<span translate=no>_^_0_^_</span> \u0d9c\u0db1\u0dca\u0db1<span translate=no>_^_1_^_</span></p>\n",
 "<p>Get batch size and number of elements along sequence axis (<span translate=no>_^_0_^_</span>) </p>\n": "<p>\u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dd2\u0d9a \u0d85\u0d9a\u0dca\u0dc2\u0dba \u0d94\u0dc3\u0dca\u0dc3\u0dda \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0dc4 \u0db8\u0dd6\u0dbd\u0daf\u0dca\u0dbb\u0dc0\u0dca\u0dba \u0d9c\u0dab\u0db1 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 (<span translate=no>_^_0_^_</span>)</p>\n",
 "<p>Get query, key and value vectors </p>\n": "<p>\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8, \u0dba\u0dad\u0dd4\u0dbb \u0dc3\u0dc4 \u0d85\u0d9c\u0dba \u0daf\u0ddb\u0dc1\u0dd2\u0d9a \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1</p>\n",
 "<p>Get shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0dc4\u0dd0\u0da9\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>If <span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> we perform self attention </p>\n": "<p><span translate=no>_^_1_^_</span>\u0d85\u0db4\u0dd2 \u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4<span translate=no>_^_0_^_</span> \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0db1\u0db8\u0dca</p>\n",
 "<p>Initial <span translate=no>_^_0_^_</span> convolution </p>\n": "<p>\u0db8\u0dd6\u0dbd\u0dd2\u0d9a<span translate=no>_^_0_^_</span> \u0d9a\u0dd0\u0da7\u0dd2 \u0d9c\u0dd0\u0dc3\u0dd2\u0db8</p>\n",
 "<p>Initial group normalization </p>\n": "<p>\u0d86\u0dbb\u0db8\u0dca\u0db7\u0d9a \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba\u0d9a\u0dbb\u0dab\u0dba</p>\n",
 "<p>Map to <span translate=no>_^_0_^_</span> with a linear layer </p>\n": "<p>\u0dbb\u0dda\u0d9b\u0dd3\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba\u0d9a\u0dca<span translate=no>_^_0_^_</span> \u0dc3\u0db8\u0d9f \u0dc3\u0dd2\u0dad\u0dd2\u0dba\u0db8</p>\n",
 "<p>Normalize </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u200d\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p>Otherwise, fallback to normal attention </p>\n": "<p>\u0d91\u0dc3\u0dda \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca, \u0dc3\u0dcf\u0db8\u0dcf\u0db1\u0dca\u0dba \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba\u0da7 \u0dc0\u0dd0\u0da7\u0dd3\u0db8</p>\n",
 "<p>Pad the heads </p>\n": "<p>\u0dc4\u0dd2\u0dc3\u0dca \u0db4\u0dd1\u0da9\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p>Query, key and value mappings </p>\n": "<p>\u0dc0\u0dd2\u0db8\u0dc3\u0dd4\u0db8\u0dca, \u0dba\u0dad\u0dd4\u0dbb \u0dc3\u0dc4 \u0d85\u0d9c\u0dba \u0dc3\u0dd2\u0dad\u0dd2\u0dba\u0db8\u0dca</p>\n",
 "<p>Reshape and transpose from <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0db1\u0dd0\u0dc0\u0dad \u0dc4\u0dd0\u0da9\u0d9c\u0dc3\u0dca\u0dc0\u0dcf \u0dc3\u0dd2\u0da7 \u0dc3\u0db8\u0dca\u0db4\u0dca\u0dbb\u0dda\u0dc2\u0dab\u0dba<span translate=no>_^_0_^_</span> \u0d9a\u0dbb\u0db1\u0dca\u0db1<span translate=no>_^_1_^_</span></p>\n",
 "<p>Reshape to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db1\u0dd0\u0dc0\u0dad \u0dc4\u0dd0\u0da9\u0d9c\u0dc3\u0dca\u0dc0\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>Self attention </p>\n": "<p>\u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</p>\n",
 "<p>Self-attention layer and pre-norm layer </p>\n": "<p>\u0dc3\u0dca\u0dc0\u0dba\u0d82 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0dca\u0dae\u0dbb\u0dba \u0dc4\u0dcf \u0db4\u0dd9\u0dbb-\u0dc3\u0db8\u0dca\u0db8\u0dad \u0dc3\u0dca\u0dae\u0dbb\u0dba</p>\n",
 "<p>Set the scale for scaled dot-product attention. </p>\n": "<p>\u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab \u0dad\u0dd2\u0dad\u0dca \u0db1\u0dd2\u0dc2\u0dca\u0db4\u0dcf\u0daf\u0db1 \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dbb\u0dd2\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1.</p>\n",
 "<p>Set to <span translate=no>_^_0_^_</span> if it&#x27;s not installed </p>\n": "<p>\u0d91\u0dba \u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dbb \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2<span translate=no>_^_0_^_</span> \u0db1\u0db8\u0dca \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1</p>\n",
 "<p>Setup <a href=\"https://github.com/HazyResearch/flash-attention\">flash attention</a>. Flash attention is only used if it&#x27;s installed and <span translate=no>_^_0_^_</span> is set to <span translate=no>_^_1_^_</span>. </p>\n": "<p>\u0dc3\u0dd0\u0d9a\u0dc3\u0dd4\u0db8 <a href=\"https://github.com/HazyResearch/flash-attention\">\u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba</a>. \u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dd4 \u0dbd\u0db6\u0db1\u0dca\u0db1\u0dda \u0d91\u0dba \u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dbb \u0d87\u0dad\u0dca\u0db1\u0db8\u0dca \u0dc3\u0dc4<span translate=no>_^_0_^_</span> \u0d91\u0dba \u0dc3\u0d9a\u0dc3\u0dcf \u0d87\u0dad\u0dca\u0db1\u0db8\u0dca \u0db4\u0db8\u0dab\u0dd2<span translate=no>_^_1_^_</span>.</p>\n",
 "<p>Split the heads </p>\n": "<p>\u0dc4\u0dd2\u0dc3\u0dca \u0db6\u0dd9\u0daf\u0db1\u0dca\u0db1</p>\n",
 "<p>Split them to heads of shape <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0dc4\u0dd0\u0da9\u0dba\u0dda \u0dc4\u0dd2\u0dc3\u0dca \u0dc0\u0dbd\u0da7 \u0db6\u0dd9\u0daf\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>Stack <span translate=no>_^_0_^_</span>, <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> vectors for flash attention, to get a single tensor of shape <span translate=no>_^_3_^_</span> </p>\n": "<p>\u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf<span translate=no>_^_2_^_</span> \u0daf\u0ddb\u0dc1\u0dd2\u0d9a<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>, \u0dc4\u0dd0\u0da9\u0dba\u0dda \u0dad\u0db1\u0dd2 \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dba\u0d9a\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0dd0\u0db1\u0dd3\u0db8\u0da7<span translate=no>_^_3_^_</span></p>\n",
 "<p>Transformer layers </p>\n": "<p>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb</p>\n",
 "<p>Transpose and reshape from <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p>\u0dc3\u0dd2\u0da7 \u0dc3\u0db8\u0dca\u0db4\u0dca\u0dbb\u0dda\u0dc2\u0dab\u0dba \u0d9a\u0dbb \u0db1\u0dd0\u0dc0\u0dad \u0dc4\u0dd0\u0da9\u0d9c\u0dc3\u0dca\u0dc0\u0dcf<span translate=no>_^_0_^_</span> \u0d9c\u0db1\u0dca\u0db1<span translate=no>_^_1_^_</span></p>\n",
 "<p>Truncate the extra head size </p>\n": "<p>\u0d85\u0db8\u0dad\u0dbb \u0dc4\u0dd2\u0dc3 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0d9a\u0db4\u0dcf</p>\n",
 "<p>Use flash attention if it&#x27;s available and the head size is less than or equal to <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0dad \u0dc4\u0dd0\u0d9a\u0dd2 \u0db1\u0db8\u0dca \u0dc3\u0dc4 \u0dc4\u0dd2\u0dc3 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0d85\u0da9\u0dd4 \u0dc4\u0ddd \u0dc3\u0db8\u0dcf\u0db1 \u0db1\u0db8\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1<span translate=no>_^_0_^_</span></p>\n",
 "<p>You can install flash attention by cloning their Github repo, <a href=\"https://github.com/HazyResearch/flash-attention\">https://github.com/HazyResearch/flash-attention</a> and then running <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0d9a\u0dca\u0dbd\u0ddd\u0db1\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0d94\u0db6\u0da7 \u0dc6\u0dca\u0dbd\u0dd1\u0dc2\u0dca \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dc3\u0dca\u0dae\u0dcf\u0db4\u0db1\u0dba \u0d9a\u0dc5 \u0dc4\u0dd0\u0d9a\u0dd2\u0dba Github repo, <a href=\"https://github.com/HazyResearch/flash-attention\">https://github.com/HazyResearch/flash-attention</a> \u0d89\u0db1\u0dca\u0db4\u0dc3\u0dd4 \u0db0\u0dcf\u0dc0\u0db1\u0dba<span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  are the input embeddings of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the conditional embeddings of shape <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d86\u0daf\u0dcf\u0db1 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dda<span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dda<span translate=no>_^_3_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the feature map of shape <span translate=no>_^_1_^_</span> </li>\n<li><span translate=no>_^_2_^_</span>  is the conditional embeddings of shape <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0dc4\u0dd0\u0da9\u0dba\u0dda \u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0dc3\u0dd2\u0dad\u0dd2\u0dba\u0db8\u0dba\u0dd2<span translate=no>_^_1_^_</span></li>\n</ul><li><span translate=no>_^_2_^_</span>\u0dc4\u0dd0\u0da9\u0dba\u0dda \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dda<span translate=no>_^_3_^_</span></li>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the input embedding size </li>\n<li><span translate=no>_^_1_^_</span>  is multiplicative factor for the hidden layer size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0d86\u0daf\u0dcf\u0db1 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_1_^_</span>\u0dc3\u0dd0\u0d9f\u0dc0\u0dd4\u0dab\u0dd4 \u0dc3\u0dca\u0dae\u0dbb \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0db6\u0dc4\u0dd4\u0d9a\u0dcf\u0dbb\u0dca\u0dba \u0dc3\u0dcf\u0db0\u0d9a\u0dba\u0d9a\u0dd2</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the input embedding size </li>\n<li><span translate=no>_^_1_^_</span>  is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span>  is the size of a attention head </li>\n<li><span translate=no>_^_3_^_</span>  is the size of the conditional embeddings </li>\n<li><span translate=no>_^_4_^_</span>  specifies whether to perform the attention softmax computation inplace to  save memory</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0d86\u0daf\u0dcf\u0db1 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_1_^_</span>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 \u0dc0\u0dda</li>\n<li><span translate=no>_^_2_^_</span>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0dc4\u0dd2\u0dc3\u0dd9\u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_3_^_</span>\u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_4_^_</span>\u0db8\u0dad\u0d9a\u0dba \u0d89\u0dad\u0dd2\u0dbb\u0dd2 \u0d9a\u0dbb \u0d9c\u0dd0\u0db1\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba softmax \u0d9c\u0dab\u0db1\u0dba inplace \u0d89\u0da7\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dba\u0db1\u0dca\u0db1 \u0db1\u0dd2\u0dba\u0db8</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the input embedding size </li>\n<li><span translate=no>_^_1_^_</span>  is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span>  is the size of a attention head </li>\n<li><span translate=no>_^_3_^_</span>  is the size of the conditional embeddings</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0d86\u0daf\u0dcf\u0db1 \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_1_^_</span>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 \u0dc0\u0dda</li>\n<li><span translate=no>_^_2_^_</span>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0dc4\u0dd2\u0dc3\u0dd9\u0dc4\u0dd2 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li>\n<li><span translate=no>_^_3_^_</span>\u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dca \u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the number of channels in the feature map </li>\n<li><span translate=no>_^_1_^_</span>  is the number of attention heads </li>\n<li><span translate=no>_^_2_^_</span>  is the number of transformer layers </li>\n<li><span translate=no>_^_3_^_</span>  is the size of the conditional embedding</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u0dc0\u0dd2\u0dc1\u0dda\u0dc2\u0dcf\u0d82\u0d9c \u0dc3\u0dd2\u0dad\u0dd2\u0dba\u0db8\u0dda \u0db1\u0dcf\u0dbd\u0dd2\u0d9a\u0dcf \u0d9c\u0dab\u0db1</li>\n<li><span translate=no>_^_1_^_</span>\u0d85\u0dc0\u0db0\u0dcf\u0db1\u0dba \u0dba\u0ddc\u0db8\u0dd4 \u0db4\u0dca\u0dbb\u0db0\u0dcf\u0db1\u0dd3\u0db1\u0dca \u0dc3\u0d82\u0d9b\u0dca\u0dba\u0dcf\u0dc0 \u0dc0\u0dda</li>\n<li><span translate=no>_^_2_^_</span>\u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca \u0dc3\u0dca\u0dae\u0dbb \u0d9c\u0dab\u0db1</li>\n<li><span translate=no>_^_3_^_</span>\u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc3\u0dc4\u0dd2\u0dad \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd2\u0dc0\u0dbd \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2</li></ul>\n",
 "Annotated PyTorch implementation/tutorial of the transformer for U-Net in stable diffusion.": "\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0dc3\u0dbb\u0dab\u0dba \u0dad\u0dd4\u0dc5 \u0dba\u0dd6-\u0db1\u0dd9\u0da7\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dba\u0dda PyTorch \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dd2\u0dbb\u0dd3\u0db8/\u0db1\u0dd2\u0db6\u0db1\u0dca\u0db0\u0db1\u0dba.",
 "Transformer for Stable Diffusion U-Net": "\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0dbb \u0dc0\u0dd2\u0dc3\u0dbb\u0dab\u0dba \u0dba\u0dd6-\u0db1\u0dd9\u0da7\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0da7\u0dca\u0dbb\u0dcf\u0db1\u0dca\u0dc3\u0dca\u0dc6\u0ddd\u0db8\u0dbb\u0dca"
}