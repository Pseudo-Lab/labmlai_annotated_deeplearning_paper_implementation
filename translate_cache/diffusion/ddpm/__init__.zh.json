{
 "<h1>Denoising Diffusion Probabilistic Models (DDPM)</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation/tutorial of the paper <a href=\"https://papers.labml.ai/paper/2006.11239\">Denoising Diffusion Probabilistic Models</a>.</p>\n<p>In simple terms, we get an image from data and add noise step by step. Then We train a model to predict that noise at each step and use the model to generate images.</p>\n<p>The following definitions and derivations show how this works. For details please refer to <a href=\"https://papers.labml.ai/paper/2006.11239\">the paper</a>.</p>\n<h2>Forward Process</h2>\n<p>The forward process adds noise to the data <span translate=no>_^_1_^_</span>, for <span translate=no>_^_2_^_</span> timesteps.</p>\n<span translate=no>_^_3_^_</span><p>where <span translate=no>_^_4_^_</span> is the variance schedule.</p>\n<p>We can sample <span translate=no>_^_5_^_</span> at any timestep <span translate=no>_^_6_^_</span> with,</p>\n<span translate=no>_^_7_^_</span><p>where <span translate=no>_^_8_^_</span> and <span translate=no>_^_9_^_</span></p>\n<h2>Reverse Process</h2>\n<p>The reverse process removes noise starting at <span translate=no>_^_10_^_</span> for <span translate=no>_^_11_^_</span> time steps.</p>\n<span translate=no>_^_12_^_</span><p><span translate=no>_^_13_^_</span> are the parameters we train.</p>\n<h2>Loss</h2>\n<p>We optimize the ELBO (from Jenson&#x27;s inequality) on the negative log likelihood.</p>\n<span translate=no>_^_14_^_</span><p>The loss can be rewritten as follows.</p>\n<span translate=no>_^_15_^_</span><p><span translate=no>_^_16_^_</span> is constant since we keep <span translate=no>_^_17_^_</span> constant.</p>\n<h3>Computing <span translate=no>_^_18_^_</span></h3>\n<p>The forward process posterior conditioned by <span translate=no>_^_19_^_</span> is,</p>\n<span translate=no>_^_20_^_</span><p>The paper sets <span translate=no>_^_21_^_</span> where <span translate=no>_^_22_^_</span> is set to constants <span translate=no>_^_23_^_</span> or <span translate=no>_^_24_^_</span>.</p>\n<p>Then, <span translate=no>_^_25_^_</span></p>\n<p>For given noise <span translate=no>_^_26_^_</span> using <span translate=no>_^_27_^_</span></p>\n<span translate=no>_^_28_^_</span><p>This gives,</p>\n<span translate=no>_^_29_^_</span><p>Re-parameterizing with a model to predict noise</p>\n<span translate=no>_^_30_^_</span><p>where <span translate=no>_^_31_^_</span> is a learned function that predicts <span translate=no>_^_32_^_</span> given <span translate=no>_^_33_^_</span>.</p>\n<p>This gives,</p>\n<span translate=no>_^_34_^_</span><p>That is, we are training to predict the noise.</p>\n<h3>Simplified loss</h3>\n<p><span translate=no>_^_35_^_</span></p>\n<p>This minimizes <span translate=no>_^_36_^_</span> when <span translate=no>_^_37_^_</span> and <span translate=no>_^_38_^_</span> for <span translate=no>_^_39_^_</span> discarding the weighting in <span translate=no>_^_40_^_</span>. Discarding the weights <span translate=no>_^_41_^_</span> increase the weight given to higher <span translate=no>_^_42_^_</span> (which have higher noise levels), therefore increasing the sample quality.</p>\n<p>This file implements the loss calculation and a basic sampling method that we use to generate images during training.</p>\n<p>Here is the <a href=\"unet.html\">UNet model</a> that gives <span translate=no>_^_43_^_</span> and <a href=\"experiment.html\">training code</a>. <a href=\"evaluate.html\">This file</a> can generate samples and interpolations from a trained model.</p>\n": "<h1>\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM)</h1>\n<p><a href=\"https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.ipynb\"><span translate=no>_^_0_^_</span></a></p>\n<p>\u8fd9\u662f\u300a<a href=\"https://papers.labml.ai/paper/2006.11239\">\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b</a>\u300b\u8bba\u6587\u7684 <a href=\"https://pytorch.org\">PyTorch</a> \u5b9e\u73b0/\u6559\u7a0b\u3002</p>\n<p>\u7b80\u800c\u8a00\u4e4b\uff0c\u6211\u4eec\u4ece\u6570\u636e\u4e2d\u83b7\u53d6\u56fe\u50cf\u5e76\u9010\u6b65\u6dfb\u52a0\u566a\u70b9\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u6765\u9884\u6d4b\u6bcf\u4e2a\u6b65\u9aa4\u7684\u566a\u58f0\uff0c\u5e76\u4f7f\u7528\u8be5\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002</p>\n<p>\u4ee5\u4e0b\u5b9a\u4e49\u548c\u6d3e\u751f\u8bf4\u660e\u4e86\u5176\u5de5\u4f5c\u539f\u7406\u3002\u8be6\u60c5\u8bf7\u53c2\u9605<a href=\"https://papers.labml.ai/paper/2006.11239\">\u8bba\u6587</a>\u3002</p>\n<h2>\u8f6c\u53d1\u8fdb\u7a0b</h2>\n<p>\u5728<span translate=no>_^_2_^_</span>\u65f6\u95f4\u6b65\u957f\u5185\uff0c\u8f6c\u53d1\u8fc7\u7a0b\u4f1a\u7ed9\u6570\u636e<span translate=no>_^_1_^_</span>\u589e\u52a0\u566a\u97f3\u3002</p>\n<span translate=no>_^_3_^_</span><p>\u65b9\u5dee\u8ba1\u5212\u5728\u54ea\u91cc<span translate=no>_^_4_^_</span>\u3002</p>\n<p>\u6211\u4eec\u53ef\u4ee5<span translate=no>_^_5_^_</span>\u968f\u65f6\u91c7\u6837<span translate=no>_^_6_^_</span>\uff0c</p>\n<span translate=no>_^_7_^_</span><p>\u5728\u54ea\u91cc<span translate=no>_^_8_^_</span>\u548c<span translate=no>_^_9_^_</span></p>\n<h2>\u53cd\u5411\u5904\u7406</h2>\n<p>\u76f8\u53cd\u7684\u8fc7\u7a0b\u4f1a\u4ece\u56db\u4e2a<span translate=no>_^_11_^_</span>\u65f6\u95f4\u6b65<span translate=no>_^_10_^_</span>\u957f\u5f00\u59cb\u6d88\u9664\u566a\u97f3\u3002</p>\n<span translate=no>_^_12_^_</span><p><span translate=no>_^_13_^_</span>\u662f\u6211\u4eec\u8bad\u7ec3\u7684\u53c2\u6570\u3002</p>\n<h2>\u635f\u5931</h2>\n<p>\u6211\u4eec\u6839\u636e\u8d1f\u5bf9\u6570\u6982\u7387\u4f18\u5316 ELBO\uff08\u6765\u81ea\u7b80\u68ee\u4e0d\u7b49\u5f0f\uff09\u3002</p>\n<span translate=no>_^_14_^_</span><p>\u635f\u5931\u53ef\u4ee5\u6539\u5199\u5982\u4e0b\u3002</p>\n<span translate=no>_^_15_^_</span><p><span translate=no>_^_16_^_</span>\u662f\u6052\u5b9a\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u4fdd\u6301<span translate=no>_^_17_^_</span>\u4e0d\u53d8\u3002</p>\n<h3>\u8ba1\u7b97<span translate=no>_^_18_^_</span></h3>\n<p>\u540e\u9a8c\u7684\u524d\u5411\u8fc7\u7a0b<span translate=no>_^_19_^_</span>\u662f\uff0c</p>\n<span translate=no>_^_20_^_</span><p>\u8bba\u6587\u5c06<span translate=no>_^_21_^_</span>\u5176\u4e2d\u8bbe\u7f6e<span translate=no>_^_22_^_</span>\u4e3a\u5e38\u91cf<span translate=no>_^_23_^_</span>\u6216<span translate=no>_^_24_^_</span>.</p>\n<p>\u7136\u540e\uff0c<span translate=no>_^_25_^_</span></p>\n<p>\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u566a\u97f3\uff0c<span translate=no>_^_26_^_</span>\u4f7f\u7528<span translate=no>_^_27_^_</span></p>\n<span translate=no>_^_28_^_</span><p>\u8fd9\u7ed9\u4e86\uff0c</p>\n<span translate=no>_^_29_^_</span><p>\u4f7f\u7528\u6a21\u578b\u91cd\u65b0\u53c2\u6570\u5316\u4ee5\u9884\u6d4b\u566a\u58f0</p>\n<span translate=no>_^_30_^_</span><p>\u5176\u4e2d<span translate=no>_^_31_^_</span>\u662f\u9884\u6d4b<span translate=no>_^_32_^_</span>\u7ed9\u5b9a\u503c\u7684\u5b66\u4e60\u51fd\u6570<span translate=no>_^_33_^_</span>\u3002</p>\n<p>\u8fd9\u7ed9\u4e86\uff0c</p>\n<span translate=no>_^_34_^_</span><p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u6b63\u5728\u8bad\u7ec3\u9884\u6d4b\u566a\u97f3\u3002</p>\n<h3>\u7b80\u5316\u635f\u5931</h3>\n<p><span translate=no>_^_35_^_</span></p>\n<p>\u8fd9\u6837\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11<span translate=no>_^_36_^_</span>\u653e<span translate=no>_^_39_^_</span>\u5f03\u6743\u91cd\u7684\u65f6\u95f4<span translate=no>_^_37_^_</span>\u548c<span translate=no>_^_38_^_</span>\u65f6\u95f4<span translate=no>_^_40_^_</span>\u3002\u4e22\u5f03\u6743\u91cd\u4f1a<span translate=no>_^_41_^_</span>\u589e\u52a0\u7ed9\u51fa\u66f4\u9ad8\u7684\u6743\u91cd<span translate=no>_^_42_^_</span>\uff08\u566a\u58f0\u7b49\u7ea7\u66f4\u9ad8\uff09\uff0c\u4ece\u800c\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u3002</p>\n<p>\u8be5\u6587\u4ef6\u5b9e\u73b0\u4e86\u635f\u5931\u8ba1\u7b97\u548c\u57fa\u672c\u91c7\u6837\u65b9\u6cd5\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u8be5\u65b9\u6cd5\u751f\u6210\u56fe\u50cf\u3002</p>\n<p>\u8fd9\u662f\u63d0\u4f9b<span translate=no>_^_43_^_</span>\u548c<a href=\"experiment.html\">\u8bad\u7ec3\u4ee3\u7801</a>\u7684 <a href=\"unet.html\">UNet \u6a21\u578b</a>\u3002<a href=\"evaluate.html\">\u6b64\u6587\u4ef6</a>\u53ef\u4ee5\u4ece\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6210\u6837\u672c\u548c\u63d2\u503c\u3002</p>\n",
 "<h2>Denoise Diffusion</h2>\n": "<h2>\u964d\u566a\u6269\u6563</h2>\n",
 "<h4>Get <span translate=no>_^_0_^_</span> distribution</h4>\n<span translate=no>_^_1_^_</span>": "<h4>\u83b7\u53d6<span translate=no>_^_0_^_</span>\u5206\u53d1</h4>\n<span translate=no>_^_1_^_</span>",
 "<h4>Sample from <span translate=no>_^_0_^_</span></h4>\n<span translate=no>_^_1_^_</span>": "<h4>\u6837\u672c\u6765\u81ea<span translate=no>_^_0_^_</span></h4>\n<span translate=no>_^_1_^_</span>",
 "<h4>Simplified Loss</h4>\n<p><span translate=no>_^_0_^_</span></p>\n": "<h4>\u7b80\u5316\u635f\u5931</h4>\n<p><span translate=no>_^_0_^_</span></p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><a href=\"utils.html\">gather</a> <span translate=no>_^_0_^_</span> </p>\n": "<p><a href=\"utils.html\">\u6536\u96c6</a><span translate=no>_^_0_^_</span></p>\n",
 "<p><a href=\"utils.html\">gather</a> <span translate=no>_^_0_^_</span> and compute <span translate=no>_^_1_^_</span> </p>\n": "<p><a href=\"utils.html\">\u6536\u96c6</a><span translate=no>_^_0_^_</span>\u548c\u8ba1\u7b97<span translate=no>_^_1_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> linearly increasing variance schedule </p>\n": "<p>\u521b\u5efa<span translate=no>_^_0_^_</span>\u7ebf\u6027\u589e\u52a0\u7684\u5dee\u5f02\u8ba1\u5212</p>\n",
 "<p>Get <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f97\u5230<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get batch size </p>\n": "<p>\u83b7\u53d6\u6279\u6b21\u5927\u5c0f</p>\n",
 "<p>Get random <span translate=no>_^_0_^_</span> for each sample in the batch </p>\n": "<p>\u6279\u6b21\u4e2d<span translate=no>_^_0_^_</span>\u6bcf\u4e2a\u6837\u672c\u7684\u968f\u673a\u83b7\u53d6</p>\n",
 "<p>MSE loss </p>\n": "<p>MSE \u4e8f\u635f</p>\n",
 "<p>Sample </p>\n": "<p>\u6837\u672c</p>\n",
 "<p>Sample <span translate=no>_^_0_^_</span> for <span translate=no>_^_1_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u7684\u6837\u672c<span translate=no>_^_1_^_</span></p>\n",
 "<p>Sample from <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6837\u672c\u6765\u81ea<span translate=no>_^_0_^_</span></p>\n",
 "<p>get <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5f97\u5230<span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> model </li>\n<li><span translate=no>_^_2_^_</span> is <span translate=no>_^_3_^_</span> </li>\n<li><span translate=no>_^_4_^_</span> is the device to place constants on</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f<span translate=no>_^_1_^_</span>\u6a21\u7279</li>\n<li><span translate=no>_^_2_^_</span>\u662f<span translate=no>_^_3_^_</span></li>\n<li><span translate=no>_^_4_^_</span>\u662f\u7528\u6765\u653e\u7f6e\u5e38\u91cf\u7684\u8bbe\u5907</li></ul>\n",
 "Denoising Diffusion Probabilistic Models (DDPM)": "\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM)",
 "PyTorch implementation and tutorial of the paper Denoising Diffusion Probabilistic Models (DDPM).": "PyTorch \u5b9e\u73b0\u548c\u8bba\u6587\u964d\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u7684\u6559\u7a0b\u3002"
}