{
 "<h1>PonderNet: Learning to Ponder</h1>\n<p>This is a <a href=\"https://pytorch.org\">PyTorch</a> implementation of the paper <a href=\"https://papers.labml.ai/paper/2107.05407\">PonderNet: Learning to Ponder</a>.</p>\n<p>PonderNet adapts the computation based on the input. It changes the number of steps to take on a recurrent network based on the input. PonderNet learns this with end-to-end gradient descent.</p>\n<p>PonderNet has a step function of the form</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>where <span translate=no>_^_1_^_</span> is the input, <span translate=no>_^_2_^_</span> is the state, <span translate=no>_^_3_^_</span> is the prediction at step <span translate=no>_^_4_^_</span>, and <span translate=no>_^_5_^_</span> is the probability of halting (stopping) at current step.</p>\n<p><span translate=no>_^_6_^_</span> can be any neural network (e.g. LSTM, MLP, GRU, Attention layer).</p>\n<p>The unconditioned probability of halting at step <span translate=no>_^_7_^_</span> is then,</p>\n<p><span translate=no>_^_8_^_</span></p>\n<p>That is the probability of not being halted at any of the previous steps and halting at step <span translate=no>_^_9_^_</span>.</p>\n<p>During inference, we halt by sampling based on the halting probability <span translate=no>_^_10_^_</span>  and get the prediction at the halting layer <span translate=no>_^_11_^_</span> as the final output.</p>\n<p>During training, we get the predictions from all the layers and calculate the losses for each of them. And then take the weighted average of the losses based on the probabilities of getting halted at each layer <span translate=no>_^_12_^_</span>.</p>\n<p>The step function is applied to a maximum number of steps donated by <span translate=no>_^_13_^_</span>.</p>\n<p>The overall loss of PonderNet is</p>\n<span translate=no>_^_14_^_</span><p><span translate=no>_^_15_^_</span> is the normal loss function between target <span translate=no>_^_16_^_</span> and prediction <span translate=no>_^_17_^_</span>.</p>\n<p><span translate=no>_^_18_^_</span> is the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2013Leibler divergence</a>.</p>\n<p><span translate=no>_^_19_^_</span> is the <a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\">Geometric distribution</a> parameterized by <span translate=no>_^_20_^_</span>. <em><span translate=no>_^_21_^_</span> has nothing to do with <span translate=no>_^_22_^_</span>; we are just sticking to same notation as the paper</em>. <span translate=no>_^_23_^_</span>.</p>\n<p>The regularization loss biases the network towards taking <span translate=no>_^_24_^_</span> steps and incentivizes  non-zero probabilities for all steps; i.e. promotes exploration.</p>\n<p>Here is the <a href=\"experiment.html\">training code <span translate=no>_^_25_^_</span></a> to train a PonderNet on <a href=\"../parity.html\">Parity Task</a>.</p>\n": "<h1>PonderNet\uff1a\u5b66\u4f1a\u601d\u8003</h1>\n<p>\u8fd9\u662f <a href=\"https://pytorch.org\">P <a href=\"https://papers.labml.ai/paper/2107.05407\">onderNet\uff1a\u5b66\u4f1a\u601d\u8003\u8fd9\u7bc7\u8bba\u6587\u7684 PyT</a> orch</a> \u5b9e\u73b0\u3002</p>\n<p>PonderNet \u6839\u636e\u8f93\u5165\u8c03\u6574\u8ba1\u7b97\u3002\u5b83\u6839\u636e\u8f93\u5165\u66f4\u6539\u5728\u5faa\u73af\u7f51\u7edc\u4e0a\u91c7\u53d6\u7684\u6b65\u6570\u3002PonderNet \u901a\u8fc7\u7aef\u5230\u7aef\u68af\u5ea6\u4e0b\u964d\u6765\u4e86\u89e3\u8fd9\u4e00\u70b9\u3002</p>\n<p>PonderNet \u7684\u6b65\u9aa4\u51fd\u6570\u662f\u8fd9\u6837\u7684</p>\n<p><span translate=no>_^_0_^_</span></p>\n<p>\u5176\u4e2d\uff0c<span translate=no>_^_1_^_</span>\u662f\u8f93\u5165\uff0c<span translate=no>_^_2_^_</span>\u662f\u72b6\u6001\uff0c<span translate=no>_^_3_^_</span>\u662f\u6b65\u9aa4\u4e2d\u7684\u9884\u6d4b<span translate=no>_^_4_^_</span>\uff0c<span translate=no>_^_5_^_</span>\u662f\u5f53\u524d\u6b65\u9aa4\u505c\u6b62\uff08\u505c\u6b62\uff09\u7684\u6982\u7387\u3002</p>\n<p><span translate=no>_^_6_^_</span>\u53ef\u4ee5\u662f\u4efb\u4f55\u795e\u7ecf\u7f51\u7edc\uff08\u4f8b\u5982 LSTM\u3001MLP\u3001GRU\u3001\u6ce8\u610f\u529b\u5c42\uff09\u3002</p>\n<p>\u56e0\u6b64\uff0c\u6309\u6b65\u505c\u987f\u7684\u65e0\u6761\u4ef6\u6982\u7387<span translate=no>_^_7_^_</span>\u662f</p>\n<p><span translate=no>_^_8_^_</span></p>\n<p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u4e4b\u524d\u7684\u4efb\u4f55\u6b65\u9aa4\u4e2d\u90fd\u4e0d\u4f1a\u505c\u4e0b\u6765\uff0c\u800c\u662f\u4e00\u6b65\u6b65\u505c\u4e0b\u6765\u7684\u53ef\u80fd\u6027<span translate=no>_^_9_^_</span>\u3002</p>\n<p>\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u6839\u636e\u505c\u6b62\u6982\u7387\u901a\u8fc7\u91c7\u6837\u6765\u505c\u6b62\uff0c<span translate=no>_^_10_^_</span>\u5e76\u5c06\u505c\u987f\u5c42\u7684\u9884\u6d4b<span translate=no>_^_11_^_</span>\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa\u3002</p>\n<p>\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u6211\u4eec\u4f1a\u5f97\u5230\u6240\u6709\u5c42\u7684\u9884\u6d4b\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u5c42\u7684\u635f\u5931\u3002\u7136\u540e\u6839\u636e\u6bcf\u5c42\u505c\u6b62\u7684\u6982\u7387\u5f97\u51fa\u635f\u5931\u7684\u52a0\u6743\u5e73\u5747\u503c<span translate=no>_^_12_^_</span>\u3002</p>\n<p>\u6b65\u957f\u51fd\u6570\u5e94\u7528\u4e8e\u6350\u8d60\u7684\u6700\u5927\u6b65\u6570<span translate=no>_^_13_^_</span>\u3002</p>\n<p>PonderNet \u7684\u603b\u635f\u5931\u662f</p>\n<span translate=no>_^_14_^_</span><p><span translate=no>_^_15_^_</span>\u662f\u76ee\u6807<span translate=no>_^_16_^_</span>\u548c\u9884\u6d4b\u4e4b\u95f4\u7684\u6b63\u6001\u635f\u5931\u51fd\u6570<span translate=no>_^_17_^_</span>\u3002</p>\n<p><span translate=no>_^_18_^_</span>\u662f <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2014Leibler \u7684\u5206\u6b67</a>\u3002</p>\n<p><span translate=no>_^_19_^_</span>\u662f\u53c2\u6570\u5316\u7684<a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\">\u51e0\u4f55\u5206\u5e03</a><span translate=no>_^_20_^_</span>\u3002<em><span translate=no>_^_21_^_</span>\u65e0\u5173<span translate=no>_^_22_^_</span>\uff1b\u6211\u4eec\u53ea\u662f\u575a\u6301\u4f7f\u7528\u4e0e\u8bba\u6587\u76f8\u540c\u7684\u7b26\u53f7</em>\u3002<span translate=no>_^_23_^_</span></p>\u3002\n<p>\u6b63\u5219\u5316\u635f\u5931\u4f7f\u7f51\u7edc\u504f\u5411\u4e8e\u91c7\u53d6<span translate=no>_^_24_^_</span>\u6b65\u9aa4\uff0c\u5e76\u6fc0\u52b1\u6240\u6709\u6b65\u9aa4\u7684\u975e\u96f6\u6982\u7387\uff1b\u5373\u4fc3\u8fdb\u63a2\u7d22\u3002</p>\n<p>\u4ee5\u4e0b\u662f\u5728 PonderNet \u4e0a<a href=\"experiment.html\">\u8bad\u7ec3 PonderNet <a href=\"../parity.html\">\u5b8c\u6210\u5947\u5076\u4efb\u52a1</a>\u7684\u8bad\u7ec3\u4ee3\u7801<span translate=no>_^_25_^_</span></a>\u3002</p>\n",
 "<h2>PonderNet with GRU for Parity Task</h2>\n<p>This is a simple model that uses a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html\">GRU Cell</a> as the step function.</p>\n<p>This model is for the <a href=\"../parity.html\">Parity Task</a> where the input is a vector of <span translate=no>_^_0_^_</span>. Each element of the vector is either <span translate=no>_^_1_^_</span>, <span translate=no>_^_2_^_</span> or <span translate=no>_^_3_^_</span> and the output is the parity - a binary value that is true if the number of <span translate=no>_^_4_^_</span>s is odd and false otherwise.</p>\n<p>The prediction of the model is the log probability of the parity being <span translate=no>_^_5_^_</span>.</p>\n": "<h2>PonderNet \u4e0e GRU \u4e00\u8d77\u6267\u884c\u5947\u5076\u6821\u9a8c\u4efb\u52a1</h2>\n<p>\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528 <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html\">GRU Cell</a> \u4f5c\u4e3a\u6b65\u8fdb\u51fd\u6570\u7684\u7b80\u5355\u6a21\u578b\u3002</p>\n<p>\u6b64\u6a21\u578b\u9002\u7528\u4e8e<a href=\"../parity.html\">\u5947\u5076\u6821\u9a8c\u4efb\u52a1</a>\uff0c\u5176\u4e2d\u8f93\u5165\u7684\u5411\u91cf\u4e3a<span translate=no>_^_0_^_</span>\u3002\u5411\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f<span translate=no>_^_2_^_</span>\u6216<span translate=no>_^_1_^_</span><span translate=no>_^_3_^_</span>\uff0c\u8f93\u51fa\u4e3a\u5947\u5076\u6821\u9a8c\u2014\u2014\u5982\u679c<span translate=no>_^_4_^_</span> s\u7684\u6570\u91cf\u4e3a\u5947\u6570\uff0c\u5219\u4e3atrue\u7684\u4e8c\u8fdb\u5236\u503c\u5426\u5219\u4e3a false\u3002</p>\n<p>\u6a21\u578b\u7684\u9884\u6d4b\u662f\u5947\u5076\u6821\u9a8c\u7684\u5bf9\u6570\u6982\u7387<span translate=no>_^_5_^_</span>\u3002</p>\n",
 "<h2>Reconstruction loss</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span> is the normal loss function between target <span translate=no>_^_2_^_</span> and prediction <span translate=no>_^_3_^_</span>.</p>\n": "<h2>\u91cd\u5efa\u635f\u5931</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span>\u662f\u76ee\u6807<span translate=no>_^_2_^_</span>\u548c\u9884\u6d4b\u4e4b\u95f4\u7684\u6b63\u5e38\u635f\u5931\u51fd\u6570<span translate=no>_^_3_^_</span>\u3002</p>\n",
 "<h2>Regularization loss</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span> is the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2013Leibler divergence</a>.</p>\n<p><span translate=no>_^_2_^_</span> is the <a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\">Geometric distribution</a> parameterized by <span translate=no>_^_3_^_</span>. <em><span translate=no>_^_4_^_</span> has nothing to do with <span translate=no>_^_5_^_</span>; we are just sticking to same notation as the paper</em>. <span translate=no>_^_6_^_</span>.</p>\n<p>The regularization loss biases the network towards taking <span translate=no>_^_7_^_</span> steps and incentivies non-zero probabilities for all steps; i.e. promotes exploration.</p>\n": "<h2>\u6b63\u89c4\u5316\u635f\u5931</h2>\n<p><span translate=no>_^_0_^_</span></p>\n<p><span translate=no>_^_1_^_</span>\u662f <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2014Leibler \u80cc\u79bb</a>\u3002</p>\n<p><span translate=no>_^_2_^_</span>\u662f\u901a\u8fc7\u53c2\u6570\u5316\u7684<a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\">\u51e0\u4f55\u5206\u5e03</a><span translate=no>_^_3_^_</span>\u3002<em><span translate=no>_^_4_^_</span>\u4e0e\u4e4b\u65e0\u5173<span translate=no>_^_5_^_</span>\uff1b\u6211\u4eec\u53ea\u662f\u575a\u6301\u4f7f\u7528\u4e0e\u62a5\u7eb8\u76f8\u540c\u7684\u7b26\u53f7</em>\u3002<span translate=no>_^_6_^_</span>\u3002</p>\n<p>\u6b63\u5219\u5316\u635f\u5931\u4f7f\u7f51\u7edc\u504f\u5411\u4e8e\u91c7\u53d6<span translate=no>_^_7_^_</span>\u63aa\u65bd\uff0c\u5e76\u6fc0\u52b1\u6240\u6709\u6b65\u9aa4\u7684\u975e\u96f6\u6982\u7387\uff1b\u5373\u4fc3\u8fdb\u63a2\u7d22\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p><span translate=no>_^_0_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> We could use a layer that takes the concatenation of <span translate=no>_^_1_^_</span> and <span translate=no>_^_2_^_</span> as input but we went with this for simplicity. </p>\n": "<p><span translate=no>_^_0_^_</span>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u5c06<span translate=no>_^_1_^_</span>\u548c\u7684\u4e32\u8054<span translate=no>_^_2_^_</span>\u4f5c\u4e3a\u8f93\u5165\u7684\u5c42\uff0c\u4f46\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u8fd9\u4e2a\u5c42\u3002</p>\n",
 "<p><span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> where the computation was halted at step <span translate=no>_^_2_^_</span> </p>\n": "<p><span translate=no>_^_0_^_</span>\u4ee5\u53ca\u8ba1\u7b97\u5728<span translate=no>_^_1_^_</span>\u4f55\u5904\u505c\u6b62<span translate=no>_^_2_^_</span></p>\n",
 "<p><span translate=no>_^_0_^_</span> for each sample and the mean of them </p>\n": "<p><span translate=no>_^_0_^_</span>\u6bcf\u4e2a\u6837\u672c\u53ca\u5176\u5747\u503c</p>\n",
 "<p>A vector to maintain which samples has halted computation </p>\n": "<p>\u7528\u4e8e\u7ef4\u62a4\u54ea\u4e9b\u6837\u672c\u5df2\u505c\u6b62\u8ba1\u7b97\u7684\u5411\u91cf</p>\n",
 "<p>Add to total loss </p>\n": "<p>\u518d\u52a0\u4e0a\u603b\u4e8f\u635f</p>\n",
 "<p>An option to set during inference so that computation is actually halted at inference time </p>\n": "<p>\u5728\u63a8\u7406\u671f\u95f4\u8bbe\u7f6e\u7684\u9009\u9879\uff0c\u4ee5\u4fbf\u5728\u63a8\u7406\u65f6\u5b9e\u9645\u505c\u6b62\u8ba1\u7b97</p>\n",
 "<p>Calculate the KL-divergence. <em>The <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\">PyTorch KL-divergence</a> implementation accepts log probabilities.</em> </p>\n": "<p>\u8ba1\u7b97 KL \u80cc\u79bb\u3002<em><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\">PyTorch KL-Divergen</a> ce \u5b9e\u73b0\u63a5\u53d7\u5bf9\u6570\u6982\u7387\u3002</em></p>\n",
 "<p>Collect <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> </p>\n": "<p>\u6536\u96c6<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span></p>\n",
 "<p>Empty vector to calculate <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8981\u8ba1\u7b97\u7684\u7a7a\u5411\u91cf<span translate=no>_^_0_^_</span></p>\n",
 "<p>GRU <span translate=no>_^_0_^_</span> </p>\n": "<p>GRU<span translate=no>_^_0_^_</span></p>\n",
 "<p>Get <span translate=no>_^_0_^_</span> upto <span translate=no>_^_1_^_</span> and expand it across the batch dimension </p>\n": "<p>\u4e86\u89e3<span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5e76\u5c06\u5176\u6269\u5c55\u5230\u6574\u4e2a\u6279\u6b21\u7ef4\u5ea6</p>\n",
 "<p>Get next state <span translate=no>_^_0_^_</span> </p>\n": "<p>\u83b7\u53d6\u4e0b\u4e00\u4e2a\u72b6\u6001<span translate=no>_^_0_^_</span></p>\n",
 "<p>Halt based on halting probability <span translate=no>_^_0_^_</span> </p>\n": "<p>\u57fa\u4e8e\u6682\u505c\u6982\u7387\u6682\u505c<span translate=no>_^_0_^_</span></p>\n",
 "<p>Iterate for <span translate=no>_^_0_^_</span> steps </p>\n": "<p>\u8fed\u4ee3<span translate=no>_^_0_^_</span>\u6b65\u9aa4</p>\n",
 "<p>Iterate upto <span translate=no>_^_0_^_</span> </p>\n": "<p>\u8fed\u4ee3\u5230<span translate=no>_^_0_^_</span></p>\n",
 "<p>KL-divergence loss </p>\n": "<p>KL-\u80cc\u79bb\u635f\u5931</p>\n",
 "<p>Lists to store <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> </p>\n": "<p>\u8981\u5b58\u50a8\u7684\u6e05\u5355<span translate=no>_^_0_^_</span>\u548c<span translate=no>_^_1_^_</span></p>\n",
 "<p>Save <span translate=no>_^_0_^_</span> </p>\n": "<p>\u4fdd\u5b58<span translate=no>_^_0_^_</span></p>\n",
 "<p>Stop the computation if all samples have halted </p>\n": "<p>\u5982\u679c\u6240\u6709\u6837\u672c\u90fd\u5df2\u505c\u6b62\uff0c\u5219\u505c\u6b62\u8ba1\u7b97</p>\n",
 "<p>The halting probability <span translate=no>_^_0_^_</span> for the last step </p>\n": "<p>\u6700\u540e\u4e00\u6b65<span translate=no>_^_0_^_</span>\u7684\u505c\u6b62\u6982\u7387</p>\n",
 "<p>The total <span translate=no>_^_0_^_</span> </p>\n": "<p>\u603b\u6570<span translate=no>_^_0_^_</span></p>\n",
 "<p>Transpose <span translate=no>_^_0_^_</span> to <span translate=no>_^_1_^_</span> </p>\n": "<p>\u79fb\u8c03<span translate=no>_^_0_^_</span>\u5230<span translate=no>_^_1_^_</span></p>\n",
 "<p>Update <span translate=no>_^_0_^_</span> </p>\n": "<p>\u66f4\u65b0<span translate=no>_^_0_^_</span></p>\n",
 "<p>Update <span translate=no>_^_0_^_</span> and <span translate=no>_^_1_^_</span> based on what was halted at current step <span translate=no>_^_2_^_</span> </p>\n": "<p>\u66f4\u65b0<span translate=no>_^_0_^_</span>\u5e76<span translate=no>_^_1_^_</span>\u57fa\u4e8e\u5f53\u524d\u6b65\u9aa4\u4e2d\u6b62\u7684\u5185\u5bb9<span translate=no>_^_2_^_</span></p>\n",
 "<p>Update halted samples </p>\n": "<p>\u66f4\u65b0\u5df2\u6682\u505c\u7684\u6837\u672c</p>\n",
 "<p>We get initial state <span translate=no>_^_0_^_</span> </p>\n": "<p>\u6211\u4eec\u5f97\u5230\u521d\u59cb\u72b6\u6001<span translate=no>_^_0_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> - the success probability of geometric distribution </li>\n<li><span translate=no>_^_2_^_</span> is the highest <span translate=no>_^_3_^_</span>; we use this to pre-compute <span translate=no>_^_4_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>is<span translate=no>_^_1_^_</span>-\u51e0\u4f55\u5206\u5e03\u7684\u6210\u529f\u6982\u7387</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6700\u9ad8\u7684<span translate=no>_^_3_^_</span>\uff1b\u6211\u4eec\u7528\u5b83\u6765\u9884\u5148\u8ba1\u7b97<span translate=no>_^_4_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> in a tensor of shape <span translate=no>_^_2_^_</span> </li>\n<li><span translate=no>_^_3_^_</span> is <span translate=no>_^_4_^_</span> in a tensor of shape <span translate=no>_^_5_^_</span> </li>\n<li><span translate=no>_^_6_^_</span> is the target of shape <span translate=no>_^_7_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5904\u4e8e\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_2_^_</span></li>\n<li><span translate=no>_^_3_^_</span><span translate=no>_^_4_^_</span>\u5904\u4e8e\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_5_^_</span></li>\n<li><span translate=no>_^_6_^_</span>\u662f\u5f62\u72b6\u7684\u76ee\u6807<span translate=no>_^_7_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is <span translate=no>_^_1_^_</span> in a tensor of shape <span translate=no>_^_2_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><span translate=no>_^_1_^_</span>\u5904\u4e8e\u5f62\u72b6\u7684\u5f20\u91cf<span translate=no>_^_2_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the input of shape <span translate=no>_^_1_^_</span></li></ul>\n<p>This outputs a tuple of four tensors:</p>\n<p>1. <span translate=no>_^_2_^_</span> in a tensor of shape <span translate=no>_^_3_^_</span> 2. <span translate=no>_^_4_^_</span> in a tensor of shape <span translate=no>_^_5_^_</span> - the log probabilities of the parity being <span translate=no>_^_6_^_</span> 3. <span translate=no>_^_7_^_</span> of shape <span translate=no>_^_8_^_</span> 4. <span translate=no>_^_9_^_</span> of shape <span translate=no>_^_10_^_</span> where the computation was halted at step <span translate=no>_^_11_^_</span></p>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u5f62\u72b6\u7684\u8f93\u5165<span translate=no>_^_1_^_</span></li></ul>\n<p>\u8fd9\u5c06\u8f93\u51fa\u4e00\u4e2a\u7531\u56db\u4e2a\u5f20\u91cf\u7ec4\u6210\u7684\u5143\u7ec4\uff1a</p>\n<p>1.<span translate=no>_^_2_^_</span>\u5728\u5f62\u72b6\u4e3a<span translate=no>_^_3_^_</span> 2 \u7684\u5f20\u91cf\u4e2d\u3002<span translate=no>_^_4_^_</span>\u5728\u5f62\u72b6\u5f20\u91cf\u4e2d<span translate=no>_^_5_^_</span>-\u5947\u5076\u6821\u9a8c\u7684\u5bf9\u6570\u6982\u7387\u4e3a<span translate=no>_^_6_^_</span> 3\u3002<span translate=no>_^_7_^_</span>\u5f62\u72b6\u4e3a<span translate=no>_^_8_^_</span> 4\u3002<span translate=no>_^_9_^_</span><span translate=no>_^_10_^_</span>\u5728\u6b65\u8fdb\u65f6\u505c\u6b62\u8ba1\u7b97\u7684\u5f62\u72b6<span translate=no>_^_11_^_</span></p>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the loss function <span translate=no>_^_1_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u635f\u5931\u51fd\u6570<span translate=no>_^_1_^_</span></li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span> is the number of elements in the input vector </li>\n<li><span translate=no>_^_1_^_</span> is the state vector size of the GRU </li>\n<li><span translate=no>_^_2_^_</span> is the maximum number of steps <span translate=no>_^_3_^_</span></li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u5143\u7d20\u6570</li>\n<li><span translate=no>_^_1_^_</span>\u662f GRU \u7684\u72b6\u6001\u5411\u91cf\u5927\u5c0f</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6700\u5927\u6b65\u6570<span translate=no>_^_3_^_</span></li></ul>\n",
 "A PyTorch implementation/tutorial of PonderNet: Learning to Ponder.": "PonderNet \u7684 PyTorch \u5b9e\u73b0/\u6559\u7a0b\uff1a\u5b66\u4f1a\u601d\u8003\u3002",
 "PonderNet: Learning to Ponder": "PonderNet\uff1a\u5b66\u4f1a\u601d\u8003"
}