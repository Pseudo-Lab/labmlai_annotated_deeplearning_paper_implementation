{
 "<h3>Basic english tokenizer</h3>\n<p>We use character level tokenizer in this experiment. You can switch by setting,</p>\n<span translate=no>_^_0_^_</span><p>in the configurations dictionary when starting the experiment.</p>\n": "<h3>\u57fa\u7840\u82f1\u8bed\u5206\u8bcd\u5668</h3>\n<p>\u6211\u4eec\u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u89d2\u8272\u7b49\u7ea7\u5206\u8bcd\u5668\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u8fdb\u884c\u5207\u6362\uff0c</p>\n<span translate=no>_^_0_^_</span><p>\u5f00\u59cb\u5b9e\u9a8c\u65f6\u5728\u914d\u7f6e\u5b57\u5178\u4e2d\u3002</p>\n",
 "<h3>Character level tokenizer</h3>\n": "<h3>\u89d2\u8272\u7b49\u7ea7\u5206\u8bcd\u5668</h3>\n",
 "<p> <a id=\"TokenizerConfigs\"></a></p>\n<h2>Tokenizer Configurations</h2>\n": "<p><a id=\"TokenizerConfigs\"></a></p>\n<h2>\u5206\u8bcd\u5668\u914d\u7f6e</h2>\n",
 "<p> Character level tokenizer configuration</p>\n": "<p>\u89d2\u8272\u7ea7\u522b\u5206\u8bcd\u5668\u914d\u7f6e</p>\n",
 "tokenizer.py": "tokenizer.py"
}