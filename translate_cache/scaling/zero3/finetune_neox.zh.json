{
 "<h1>Finetune <a href=\"../../neox/index.html\">GPT-NeoX</a> with <a href=\"index.html\">Zero3 memory optimizer</a></h1>\n<p>This script trains the bias parameters of the <a href=\"../../neox/model.html\">GPT-NeoX model</a>  on multiple devices with Zero-DP Memory Optimization.</p>\n": "<h1>\u914d\u5907 <a href=\"index.html\">Zero3 \u5185\u5b58\u4f18\u5316\u5668\u7684 Finetune GPT-NEO</a> <a href=\"../../neox/index.html\">X</a></h1>\n<p>\u8be5\u811a\u672c\u4f7f\u7528\u96f6 DP \u5185\u5b58\u4f18\u5316\u529f\u80fd\u5728\u591a\u4e2a\u5668\u4ef6\u4e0a\u8bad\u7ec3 <a href=\"../../neox/model.html\">GPT-NEOX \u6a21\u578b</a>\u7684\u504f\u7f6e\u53c2\u6570\u3002</p>\n",
 "<h4>Create the model with Zero3 memory optimizer</h4>\n": "<h4>\u4f7f\u7528 Zero3 \u5185\u5b58\u4f18\u5316\u5668\u521b\u5efa\u6a21\u578b</h4>\n",
 "<h4>Run the training on the node with rank <span translate=no>_^_0_^_</span>.</h4>\n": "<h4>\u5728\u5e26\u6709 rank \u7684\u8282\u70b9\u4e0a\u8fd0\u884c\u8bad\u7ec3<span translate=no>_^_0_^_</span>\u3002</h4>\n",
 "<h4>Set the optimizers for the model</h4>\n<p>Note that we pass the sharded parameters from <span translate=no>_^_0_^_</span>.</p>\n": "<h4>\u8bbe\u7f6e\u6a21\u578b\u7684\u4f18\u5316\u5668</h4>\n<p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u4ece\u4f20\u9012\u5206\u7247\u53c2\u6570<span translate=no>_^_0_^_</span>\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Create a sequential model </p>\n": "<p>\u521b\u5efa\u987a\u5e8f\u6a21\u578b</p>\n",
 "<p>Create configurations </p>\n": "<p>\u521b\u5efa\u914d\u7f6e</p>\n",
 "<p>Create the experiment </p>\n": "<p>\u521b\u5efa\u5b9e\u9a8c</p>\n",
 "<p>Initialize PyTorch distributed process group </p>\n": "<p>\u521d\u59cb\u5316 PyTorch \u5206\u5e03\u5f0f\u8fdb\u7a0b\u7ec4</p>\n",
 "<p>Initialize the model. Do this before the loop for cleaner logs. </p>\n": "<p>\u521d\u59cb\u5316\u6a21\u578b\u3002\u5728\u5faa\u73af\u4e4b\u524d\u6267\u884c\u6b64\u64cd\u4f5c\u4ee5\u83b7\u5f97\u66f4\u6e05\u6670\u7684\u65e5\u5fd7\u3002</p>\n",
 "<p>Load configurations </p>\n": "<p>\u88c5\u8f7d\u914d\u7f6e</p>\n",
 "<p>Log the machine configurations </p>\n": "<p>\u8bb0\u5f55\u8ba1\u7b97\u673a\u914d\u7f6e</p>\n",
 "<p>Set current device </p>\n": "<p>\u8bbe\u7f6e\u5f53\u524d\u8bbe\u5907</p>\n",
 "<p>Start a process for each GPU. You will need a separate launcher if you are using multiple computers. </p>\n": "<p>\u4e3a\u6bcf\u4e2a GPU \u542f\u52a8\u4e00\u4e2a\u8fdb\u7a0b\u3002\u5982\u679c\u60a8\u4f7f\u7528\u591a\u53f0\u8ba1\u7b97\u673a\uff0c\u5219\u9700\u8981\u5355\u72ec\u7684\u542f\u52a8\u5668\u3002</p>\n",
 "<p>Start the experiment </p>\n": "<p>\u5f00\u59cb\u5b9e\u9a8c</p>\n",
 "<p>To make sure the fine tuner sets the trainable parameters </p>\n": "<p>\u786e\u4fdd\u7cbe\u7ec6\u8c03\u8c10\u5668\u8bbe\u7f6e\u4e86\u53ef\u8bad\u7ec3\u7684\u53c2\u6570</p>\n",
 "<p>Train the model </p>\n": "<p>\u8bad\u7ec3\u6a21\u578b</p>\n",
 "<p>Use the <a href=\"../../neox/samples/finetune.html\">Pipeline Parallel Trainer configurations</a> and adapt it for Zero3 memory optimizer. </p>\n": "<p>\u4f7f\u7528 Pi <a href=\"../../neox/samples/finetune.html\">peline Parallel Trainer \u914d\u7f6e\u5e76</a>\u5c06\u5176\u8c03\u6574\u4e3a Zero3 \u5185\u5b58\u4f18\u5316\u5668\u3002</p>\n",
 "<p>Wrap the layers with <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5c06\u56fe\u5c42\u5305\u88f9\u8d77\u6765<span translate=no>_^_0_^_</span></p>\n",
 "Finetune GPT-NeoX with Zero3 memory optimizer": "\u914d\u5907 Zero3 \u5185\u5b58\u4f18\u5316\u5668\u7684 Finetune GPT-NEOX",
 "This script trains the bias parameters of the GPT-NeoX on multiple devices with Zero-DP Memory Optimization.": "\u8be5\u811a\u672c\u4f7f\u7528\u96f6 DP \u5185\u5b58\u4f18\u5316\u529f\u80fd\u5728\u591a\u4e2a\u5668\u4ef6\u4e0a\u8bad\u7ec3 GPT-NEOX \u7684\u504f\u7f6e\u53c2\u6570\u3002"
}