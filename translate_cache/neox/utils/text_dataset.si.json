{
 "<h1>Text Dataset for GPT-NeoX</h1>\n": "<h1>\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca\u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd9\u0dc5 \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba</h1>\n",
 "<h2>Dataset for fine-tuning GPT-NeoX</h2>\n<p>This is not optimized to very large datasets.</p>\n": "<h2>\u0dc4\u0ddc\u0db3\u0dd2\u0db1\u0dca\u0dc3\u0dd4\u0dc3\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba GPT-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca</h2>\n<p>\u0db8\u0dd9\u0dba\u0d89\u0dad\u0dcf \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0daf\u0dad\u0dca\u0dad \u0d9a\u0dcf\u0dab\u0dca\u0da9 \u0dc0\u0dbd\u0da7 \u0db4\u0dca\u0dbb\u0dc1\u0dc3\u0dca\u0dad\u0dd2\u0d9a\u0dbb\u0dab\u0dba \u0d9a\u0dbb \u0db1\u0ddc\u0db8\u0dd0\u0dad. </p>\n",
 "<h3>Get a sample</h3>\n<ul><li><span translate=no>_^_0_^_</span>  is the index of the sample </li>\n<p><em>Returns</em>  the input and the target</p></ul>\n": "<h3>\u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd2\u0dba\u0d9a\u0dca\u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd2\u0dba\u0dda \u0daf\u0dbb\u0dca\u0dc1\u0d9a\u0dba \u0dc0\u0dda </li>\n<p>\u0d86\u0daf\u0dcf\u0db1\u0dba\u0dc3\u0dc4 \u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0dba<em>\u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2</em> </p></ul>\n",
 "<h3>Load Dataset</h3>\n<ul><li><span translate=no>_^_0_^_</span>  is the sequence length of a single training sample </li>\n<li><span translate=no>_^_1_^_</span>  is the name of the dataset </li>\n<p><em>Returns</em>  the dataset</p></ul>\n": "<h3>\u0daf\u0dad\u0dca\u0dad\u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba \u0db4\u0dd6\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0dad\u0db1\u0dd2 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd2\u0dba\u0d9a \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0daf\u0dd2\u0d9c \u0dc0\u0dda </li>\n<li><span translate=no>_^_1_^_</span> \u0daf\u0dad\u0dca\u0dad \u0dc3\u0db8\u0dd4\u0daf\u0dcf\u0dba \u0db1\u0db8 </li>\n<p>\u0daf\u0dad\u0dca\u0dad\u0dc3\u0db8\u0dd4\u0daf\u0dcf\u0dba<em>\u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2</em> </p></ul>\n",
 "<h3>Load text file</h3>\n<ul><li><span translate=no>_^_0_^_</span>  is the location of the text file </li>\n<li><span translate=no>_^_1_^_</span>  is the URL to download the file from </li>\n<li><span translate=no>_^_2_^_</span>  is the number of characters to filter.  Use this during testing when trying large datasets </li>\n<p><em>Returns</em>  the text content</p></ul>\n": "<h3>\u0db4\u0dd9\u0dc5\u0d9c\u0ddc\u0db1\u0dd4\u0dc0 \u0db4\u0dd6\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0db4\u0dd9\u0dc5 \u0d9c\u0ddc\u0db1\u0dd4\u0dc0\u0dda \u0db4\u0dd2\u0dc4\u0dd2\u0da7\u0dd3\u0db8 </li>\n<li><span translate=no>_^_1_^_</span> \u0d9c\u0ddc\u0db1\u0dd4\u0dc0 \u0db6\u0dcf\u0d9c\u0dad \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf URL \u0d91\u0d9a </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0db4\u0dd9\u0dbb\u0dd3\u0db8\u0da7 \u0d85\u0d9a\u0dca\u0dc2\u0dbb \u0d9c\u0dab\u0db1\u0dba\u0dba\u0dd2. \u0dc0\u0dd2\u0dc1\u0dcf\u0dbd \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd \u0d8b\u0dad\u0dca\u0dc3\u0dcf\u0dc4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda\u0daf\u0dd3 \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda\u0daf\u0dd3 \u0db8\u0dd9\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db1\u0dca\u0db1 </li>\n<p>\u0db4\u0dd9\u0dc5\u0d85\u0db1\u0dca\u0dad\u0dbb\u0dca\u0d9c\u0dad\u0dba<em>\u0db1\u0dd0\u0dc0\u0dad \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2</em> </p></ul>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p>Create a PyTorch tensor </p>\n": "<p>\u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca\u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dba\u0d9a\u0dca \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Download if it doesn&#x27;t exist </p>\n": "<p>\u0d91\u0dba\u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0db6\u0dcf\u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Filter </p>\n": "<p>\u0db4\u0dd9\u0dbb\u0dc4\u0db1\u0dca </p>\n",
 "<p>Load data </p>\n": "<p>\u0daf\u0dad\u0dca\u0dad\u0db4\u0dd6\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Load the content </p>\n": "<p>\u0d85\u0db1\u0dca\u0dad\u0dbb\u0dca\u0d9c\u0dad\u0dba\u0db4\u0dd6\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Number of samples </p>\n": "<p>\u0dc3\u0dcf\u0db8\u0dca\u0db4\u0dbd\u0d9c\u0dab\u0db1 </p>\n",
 "<p>Tokenize </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dca\u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Truncate </p>\n": "<p>\u0da7\u0dca\u0dbb\u0db1\u0dca\u0d9a\u0dda\u0da7\u0dca\u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the list of token ids </li>\n<li><span translate=no>_^_1_^_</span>  is the sequence length of a single training sample</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0da7\u0ddd\u0d9a\u0db1\u0dca \u0dc4\u0dd0\u0db3\u0dd4\u0db1\u0dd4\u0db8\u0dca\u0db4\u0dad\u0dca \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0\u0dba\u0dd2 </li>\n<li><span translate=no>_^_1_^_</span> \u0dad\u0db1\u0dd2 \u0db4\u0dd4\u0dc4\u0dd4\u0dab\u0dd4 \u0db1\u0dd2\u0dba\u0dd0\u0daf\u0dd2\u0dba\u0d9a \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0daf\u0dd2\u0d9c \u0dc0\u0dda</li></ul>\n",
 "Loads text datasets to fine-tune GPT-NeoX": "GPT-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0db8\u0db1\u0dcf\u0dc0 \u0dc3\u0d9a\u0dc3\u0dca \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd9\u0dc5 \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd \u0db4\u0da7\u0dc0\u0dba\u0dd2",
 "Text Dataset for GPT-NeoX": "\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd9\u0dc5 \u0daf\u0dad\u0dca\u0dad \u0d9a\u0da7\u0dca\u0da7\u0dbd\u0dba"
}