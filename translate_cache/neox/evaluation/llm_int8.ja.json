{
 "<h1>Evaluate GPT-NeoX using LLM.int8() quantization on test suite</h1>\n<p>This code evaluate <a href=\"../index.html\">GPT-NeoX</a> using <a href=\"../utils/llm_int8.html\">LLM.int8() quantization</a>, on a suite of tasks.</p>\n": "<h1>\u30c6\u30b9\u30c8\u30b9\u30a4\u30fc\u30c8\u3067 llm.int8 () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066 GPT-Neox \u3092\u8a55\u4fa1\u3059\u308b</h1>\n<p>\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u4e00\u9023\u306e\u30bf\u30b9\u30af\u3067 <a href=\"../utils/llm_int8.html\">llm.int8 () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066</a> <a href=\"../index.html\">GPT-Neox</a> \u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> model </p>\n": "<p><span translate=no>_^_0_^_</span>\u30e2\u30c7\u30eb\u4f5c\u6210</p>\n",
 "<p>Device </p>\n": "<p>\u7aef\u672b</p>\n",
 "<p>Load layers </p>\n": "<p>\u30ec\u30a4\u30e4\u30fc\u3092\u30ed\u30fc\u30c9</p>\n",
 "<p>Load layers in float16 into CPU. We convert the layers to int8 later, because doing that on the fly after loading layers to GPU causes CUDA memory fragmentation (about 3GB memory can get lost due to fragmentation). </p>\n": "<p>float16 \u306e\u30ec\u30a4\u30e4\u30fc\u3092 CPU \u306b\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u30ec\u30a4\u30e4\u30fc\u3092GPU\u306b\u30ed\u30fc\u30c9\u3057\u305f\u5f8c\u306b\u305d\u306e\u5834\u3067\u3053\u308c\u3092\u884c\u3046\u3068\u3001CUDA\u30e1\u30e2\u30ea\u306e\u65ad\u7247\u5316\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u3001\u5f8c\u3067\u30ec\u30a4\u30e4\u30fc\u3092int8\u306b\u5909\u63db\u3057\u307e\u3059\uff08\u30d5\u30e9\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308a\u7d043GB\u306e\u30e1\u30e2\u30ea\u304c\u5931\u308f\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059</p>\uff09\u3002\n",
 "<p>Run <a href=\"index.html\">evaluation harness</a> </p>\n": "<p><a href=\"index.html\">\u8a55\u4fa1\u7528\u30cf\u30fc\u30cd\u30b9\u3092\u5b9f\u884c</a></p>\n",
 "<p>This reduces CUDA memory fragmentation </p>\n": "<p>\u3053\u308c\u306b\u3088\u308a\u3001CUDA \u30e1\u30e2\u30ea\u306e\u65ad\u7247\u5316\u304c\u6e1b\u5c11\u3057\u307e\u3059\u3002</p>\n",
 "Evaluate GPT-NeoX using LLM.int8() quantization on test suite": "\u30c6\u30b9\u30c8\u30b9\u30a4\u30fc\u30c8\u3067 llm.int8 () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066 GPT-Neox \u3092\u8a55\u4fa1\u3059\u308b"
}