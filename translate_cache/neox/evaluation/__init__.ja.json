{
 "<h1>Evaluation</h1>\n<p>This is the code to test the model on <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a>.</p>\n<ul><li><a href=\"half_precision.html\">Evaluating half precision model on a single GPU</a></li></ul>\n": "<h1>\u8a55\u4fa1</h1>\n<p><a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">Eleutherai/LM-\u8a55\u4fa1\u30cf\u30fc\u30cd\u30b9\u3067\u30e2\u30c7\u30eb\u3092\u30c6\u30b9\u30c8\u3059\u308b\u305f\u3081\u306e\u30b3\u30fc\u30c9\u3067\u3059</a>\u3002</p>\n<ul><li><a href=\"half_precision.html\">\u5358\u4e00\u306e GPU \u3067\u306e\u534a\u7cbe\u5ea6\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1</a></li></ul>\n",
 "<h2>Evaluation Harness Adapter</h2>\n<p>This is based on the <a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">adapter from EleutherAI/gpt-neox</a></p>\n": "<h2>\u8a55\u4fa1\u7528\u30cf\u30fc\u30cd\u30b9\u30a2\u30c0\u30d7\u30bf\u30fc</h2>\n<p><a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">\u3053\u308c\u306fElutherai/GPT-Neox\u306e\u30a2\u30c0\u30d7\u30bf\u30fc\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3044\u307e\u3059</a></p>\n",
 "<h2>Run evaluation harness with a given model</h2>\n": "<h2>\u7279\u5b9a\u306e\u30e2\u30c7\u30eb\u3067\u8a55\u4fa1\u7528\u30cf\u30fc\u30cd\u30b9\u3092\u5b9f\u884c</h2>\n",
 "<h3>Get log-likelihoods of the next tokens</h3>\n<ul><li><span translate=no>_^_0_^_</span>  List of requests containing the context and the expected continuation. </li>\n<li><span translate=no>_^_1_^_</span>  If True, disable tqdm progress bar.</li></ul>\n": "<h3>\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u306e\u5bfe\u6570\u767a\u751f\u78ba\u7387\u3092\u53d6\u5f97</h3>\n<ul><li><span translate=no>_^_0_^_</span>\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3068\u4e88\u60f3\u3055\u308c\u308b\u7d99\u7d9a\u3092\u542b\u3080\u30ea\u30af\u30a8\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3002</li>\n</ul><li><span translate=no>_^_1_^_</span>True \u306e\u5834\u5408\u3001tqdm \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u3092\u7121\u52b9\u306b\u3057\u307e\u3059\u3002</li>\n",
 "<h3>Run given evaluations</h3>\n": "<h3>\u4e0e\u3048\u3089\u308c\u305f\u8a55\u4fa1\u3092\u5b9f\u884c</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Batch size</p>\n": "<p>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba</p>\n",
 "<p> Call the model</p>\n": "<p>\u30e2\u30c7\u30eb\u306b\u96fb\u8a71\u3059\u308b</p>\n",
 "<p> Decode text from token ids</p>\n": "<p>\u30c8\u30fc\u30af\u30f3 ID \u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3092\u30c7\u30b3\u30fc\u30c9</p>\n",
 "<p> Encode a given text</p>\n": "<p>\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b</p>\n",
 "<p>Add configs </p>\n": "<p>\u69cb\u6210\u3092\u8ffd\u52a0</p>\n",
 "<p>Add padding </p>\n": "<p>\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0</p>\n",
 "<p>Add the total log-likelihoods and whether there was a match to the results </p>\n": "<p>\u5bfe\u6570\u63a8\u5b9a\u5024\u306e\u5408\u8a08\u3068\u3001\u4e00\u81f4\u3057\u305f\u304b\u3069\u3046\u304b\u3092\u7d50\u679c\u306b\u52a0\u7b97\u3057\u307e\u3059\u3002</p>\n",
 "<p>All tasks if nothing is specified </p>\n": "<p>\u4f55\u3082\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3059\u3079\u3066\u306e\u30bf\u30b9\u30af</p>\n",
 "<p>Concatenate the context and continuation </p>\n": "<p>\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3068\u7d9a\u304d\u3092\u9023\u7d50\u3059\u308b</p>\n",
 "<p>Create a tensor </p>\n": "<p>\u30c6\u30f3\u30bd\u30eb\u306e\u4f5c\u6210</p>\n",
 "<p>Create the adapter </p>\n": "<p>\u30a2\u30c0\u30d7\u30bf\u30fc\u306e\u4f5c\u6210</p>\n",
 "<p>Determine the padded length. Shorter sequences will get padded. </p>\n": "<p>\u30d1\u30c3\u30c9\u306e\u9577\u3055\u3092\u6c7a\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u77ed\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306f\u30d1\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u307e\u3059</p>\u3002\n",
 "<p>End-of-text token </p>\n": "<p>\u30c6\u30ad\u30b9\u30c8\u7d42\u4e86\u30c8\u30fc\u30af\u30f3</p>\n",
 "<p>For results </p>\n": "<p>\u7d50\u679c\u306b\u3064\u3044\u3066</p>\n",
 "<p>Get log softmaxes </p>\n": "<p>\u30ed\u30b0\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3092\u53d6\u5f97</p>\n",
 "<p>Get logits of those </p>\n": "<p>\u305d\u308c\u3089\u306e\u30ed\u30b0\u3092\u53d6\u5f97</p>\n",
 "<p>Get model logits </p>\n": "<p>\u30e2\u30c7\u30eb\u30ed\u30b8\u30c3\u30c8\u3092\u53d6\u5f97</p>\n",
 "<p>Get number of predicted tokens </p>\n": "<p>\u4e88\u6e2c\u30c8\u30fc\u30af\u30f3\u306e\u6570\u3092\u53d6\u5f97</p>\n",
 "<p>Get the target tokens </p>\n": "<p>\u5bfe\u8c61\u30c8\u30fc\u30af\u30f3\u3092\u53d6\u5f97</p>\n",
 "<p>Get the tokens with the highest probabilities </p>\n": "<p>\u6700\u3082\u78ba\u7387\u306e\u9ad8\u3044\u30c8\u30fc\u30af\u30f3\u3092\u624b\u306b\u5165\u308c\u3088\u3046</p>\n",
 "<p>Input length </p>\n": "<p>\u5165\u529b\u9577\u3055</p>\n",
 "<p>Lengths of the input sequences </p>\n": "<p>\u5165\u529b\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055</p>\n",
 "<p>Load the tokenizer </p>\n": "<p>\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u30ed\u30fc\u30c9</p>\n",
 "<p>Log-likelihoods of the target tokens </p>\n": "<p>\u5bfe\u8c61\u30c8\u30fc\u30af\u30f3\u306e\u5bfe\u6570\u767a\u751f\u53ef\u80fd\u6027</p>\n",
 "<p>Loop through each request in the chunk and collect them into PyTorch tensors with paddings </p>\n": "<p>\u30c1\u30e3\u30f3\u30af\u5185\u306e\u5404\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u30eb\u30fc\u30d7\u51e6\u7406\u3057\u3001\u30d1\u30c7\u30a3\u30f3\u30b0\u4ed8\u304d\u306e PyTorch \u30c6\u30f3\u30bd\u30eb\u306b\u307e\u3068\u3081\u307e\u3059\u3002</p>\n",
 "<p>Loop through requests with <span translate=no>_^_0_^_</span> number of requests at a time </p>\n": "<p><span translate=no>_^_0_^_</span>\u4e00\u5ea6\u306b\u8907\u6570\u306e\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u3042\u308b\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u30eb\u30fc\u30d7\u30b9\u30eb\u30fc\u3059\u308b</p>\n",
 "<p>Loop through the input/output pairs of the batch </p>\n": "<p>\u30d0\u30c3\u30c1\u306e\u5165\u529b\u3068\u51fa\u529b\u306e\u30da\u30a2\u3092\u30eb\u30fc\u30d7\u51e6\u7406\u3057\u307e\u3059</p>\n",
 "<p>Maximum number of tokens to generate </p>\n": "<p>\u751f\u6210\u3059\u308b\u30c8\u30fc\u30af\u30f3\u306e\u6700\u5927\u6570</p>\n",
 "<p>Maximum sequence length </p>\n": "<p>\u6700\u5927\u30b7\u30fc\u30b1\u30f3\u30b9\u9577</p>\n",
 "<p>Padded length for the batch </p>\n": "<p>\u30d0\u30c3\u30c1\u7528\u306e\u30d1\u30c3\u30c9\u5165\u308a\u9577\u3055</p>\n",
 "<p>Padding </p>\n": "<p>\u30d1\u30c7\u30a3\u30f3\u30b0</p>\n",
 "<p>Re-order and return results </p>\n": "<p>\u4e26\u3079\u66ff\u3048\u3066\u7d50\u679c\u3092\u8fd4\u3059</p>\n",
 "<p>Remove final token </p>\n": "<p>\u6700\u7d42\u30c8\u30fc\u30af\u30f3\u3092\u524a\u9664</p>\n",
 "<p>Reorder the requests in the descending order of the lengths, so that sequences with similar lengths are close </p>\n": "<p>\u540c\u3058\u9577\u3055\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u304c\u8fd1\u304f\u306a\u308b\u3088\u3046\u306b\u3001\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u9577\u3055\u306e\u964d\u9806\u306b\u4e26\u3079\u66ff\u3048\u307e\u3059</p>\n",
 "<p>Run </p>\n": "<p>\u5b9f\u884c</p>\n",
 "<p>Run <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a> evaluator </p>\n": "<p><a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/LM-\u8a55\u4fa1-\u30cf\u30fc\u30cd\u30b9\u30a8\u30d0\u30ea\u30e5\u30a8\u30fc\u30bf\u30fc\u3092\u5b9f\u884c</a></p>\n",
 "<p>Size of the vocabulary </p>\n": "<p>\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u30fc\u306e\u30b5\u30a4\u30ba</p>\n",
 "<p>The continuations for the batch </p>\n": "<p>\u30d0\u30c3\u30c1\u306e\u7d99\u7d9a</p>\n",
 "<p>To store the inputs for the batch </p>\n": "<p>\u30d0\u30c3\u30c1\u306e\u5165\u529b\u3092\u4fdd\u5b58\u3059\u308b\u306b\u306f</p>\n",
 "<p>Truncate from left if the size exceeds the <span translate=no>_^_0_^_</span> </p>\n": "<p>\u30b5\u30a4\u30ba\u304c <span translate=no>_^_0_^_</span></p>\n",
 "<p>Whether there&#x27;s an exact match </p>\n": "<p>\u5b8c\u5168\u306b\u4e00\u81f4\u3059\u308b\u304b\u3069\u3046\u304b</p>\n",
 "<p>padded_length = padded_length if padded_length is not None else inplen </p>\n": "<p>padded_length = padded_length \u304c Padded_length \u3067\u306a\u3044\u5834\u5408\u306f\u30d1\u30c7\u30a3\u30f3\u30b0\u3055\u308c\u305f_length\u3001\u305d\u308c\u4ee5\u5916\u306f\u30d7\u30ec\u30f3\u306a\u3057</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is model </li>\n<li><span translate=no>_^_1_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_2_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_3_^_</span>  is the batch size </li>\n<li><span translate=no>_^_4_^_</span>  is the device of the model</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u30e2\u30c7\u30eb\u3067\u3059</li>\n<li><span translate=no>_^_1_^_</span><a href=\"huggingface/tokenizers\">\u30cf\u30ae\u30f3\u30b0\u30d5\u30a7\u30a4\u30b9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3067\u3059</a></li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u306e\u30b5\u30a4\u30ba\u3067\u3059 (\u3053\u308c\u306f\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u30dc\u30ad\u30e3\u30d6\u30b5\u30a4\u30ba\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002neox\u306f\u57cb\u3081\u8fbc\u307f\u5c64\u30e2\u30c7\u30eb\u3092\u4e26\u5217\u5316\u3059\u308b\u305f\u3081\u306e\u8ffd\u52a0\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u3066\u3044\u308b\u304b\u3089\u3067\u3059)\u3002</li>\n<li><span translate=no>_^_3_^_</span>\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba</li>\n<li><span translate=no>_^_4_^_</span>\u30e2\u30c7\u30eb\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u3059</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_1_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_2_^_</span>  is the batch size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span><a href=\"huggingface/tokenizers\">\u30cf\u30ae\u30f3\u30b0\u30d5\u30a7\u30a4\u30b9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3067\u3059</a></li>\n<li><span translate=no>_^_1_^_</span>\u306f\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u306e\u30b5\u30a4\u30ba\u3067\u3059 (\u3053\u308c\u306f\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u30dc\u30ad\u30e3\u30d6\u30b5\u30a4\u30ba\u3068\u306f\u7570\u306a\u308a\u307e\u3059\u3002neox\u306f\u57cb\u3081\u8fbc\u307f\u5c64\u30e2\u30c7\u30eb\u3092\u4e26\u5217\u5316\u3059\u308b\u305f\u3081\u306e\u8ffd\u52a0\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u3066\u3044\u308b\u304b\u3089\u3067\u3059)\u3002</li>\n<li><span translate=no>_^_2_^_</span>\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba</li></ul>\n",
 "Code to evaluate the model on NLP tasks through lm-evaluation-harness": "LM \u8a55\u4fa1\u30cf\u30fc\u30cd\u30b9\u3092\u901a\u3058\u3066 NLP \u30bf\u30b9\u30af\u3067\u30e2\u30c7\u30eb\u3092\u8a55\u4fa1\u3059\u308b\u30b3\u30fc\u30c9",
 "Evaluation": "\u8a55\u4fa1"
}