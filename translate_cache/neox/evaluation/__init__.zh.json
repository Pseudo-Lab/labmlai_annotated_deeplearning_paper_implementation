{
 "<h1>Evaluation</h1>\n<p>This is the code to test the model on <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a>.</p>\n<ul><li><a href=\"half_precision.html\">Evaluating half precision model on a single GPU</a></li></ul>\n": "<h1>\u8bc4\u4f30</h1>\n<p>\u8fd9\u662f\u5728 Ele <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">utherai/LM-Evaluation-Harnes</a> s \u4e0a\u6d4b\u8bd5\u6a21\u578b\u7684\u4ee3\u7801\u3002</p>\n<ul><li><a href=\"half_precision.html\">\u5728\u5355\u4e2a GPU \u4e0a\u8bc4\u4f30\u534a\u7cbe\u5ea6\u6a21\u578b</a></li></ul>\n",
 "<h2>Evaluation Harness Adapter</h2>\n<p>This is based on the <a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">adapter from EleutherAI/gpt-neox</a></p>\n": "<h2>\u8bc4\u4f30\u7ebf\u675f\u9002\u914d\u5668</h2>\n<p>\u8fd9\u662f\u57fa\u4e8e ele <a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">utherai/GPT-NEOX \u7684\u9002\u914d\u5668</a></p>\n",
 "<h2>Run evaluation harness with a given model</h2>\n": "<h2>\u4f7f\u7528\u7ed9\u5b9a\u6a21\u578b\u8fd0\u884c\u8bc4\u4f30\u5de5\u5177</h2>\n",
 "<h3>Get log-likelihoods of the next tokens</h3>\n<ul><li><span translate=no>_^_0_^_</span>  List of requests containing the context and the expected continuation. </li>\n<li><span translate=no>_^_1_^_</span>  If True, disable tqdm progress bar.</li></ul>\n": "<h3>\u83b7\u53d6\u4e0b\u4e00\u4e2a\u4ee3\u5e01\u7684\u5bf9\u6570\u53ef\u80fd\u6027</h3>\n<ul><li><span translate=no>_^_0_^_</span>\u5305\u542b\u4e0a\u4e0b\u6587\u548c\u9884\u671f\u5ef6\u7eed\u7684\u8bf7\u6c42\u5217\u8868\u3002</li>\n<li><span translate=no>_^_1_^_</span>\u5982\u679c\u4e3a True\uff0c\u5219\u7981\u7528 tqdm \u8fdb\u5ea6\u6761\u3002</li></ul>\n",
 "<h3>Run given evaluations</h3>\n": "<h3>\u8fd0\u884c\u7ed9\u5b9a\u7684\u8bc4\u4f30</h3>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p> Batch size</p>\n": "<p>\u6279\u91cf\u5927\u5c0f</p>\n",
 "<p> Call the model</p>\n": "<p>\u7ed9\u6a21\u7279\u6253\u7535\u8bdd</p>\n",
 "<p> Decode text from token ids</p>\n": "<p>\u89e3\u7801\u6765\u81ea\u4ee4\u724c ID \u7684\u6587\u672c</p>\n",
 "<p> Encode a given text</p>\n": "<p>\u5bf9\u7ed9\u5b9a\u6587\u672c\u8fdb\u884c\u7f16\u7801</p>\n",
 "<p>Add configs </p>\n": "<p>\u6dfb\u52a0\u914d\u7f6e</p>\n",
 "<p>Add padding </p>\n": "<p>\u6dfb\u52a0\u586b\u5145</p>\n",
 "<p>Add the total log-likelihoods and whether there was a match to the results </p>\n": "<p>\u5c06\u603b\u5bf9\u6570\u4f3c\u7136\u4ee5\u53ca\u7ed3\u679c\u662f\u5426\u5b58\u5728\u5339\u914d\u9879\u76f8\u52a0</p>\n",
 "<p>All tasks if nothing is specified </p>\n": "<p>\u5982\u679c\u672a\u6307\u5b9a\u4efb\u4f55\u5185\u5bb9\uff0c\u5219\u4e3a\u6240\u6709\u4efb\u52a1</p>\n",
 "<p>Concatenate the context and continuation </p>\n": "<p>\u8fde\u63a5\u4e0a\u4e0b\u6587\u548c\u5ef6\u7eed</p>\n",
 "<p>Create a tensor </p>\n": "<p>\u521b\u5efa\u5f20\u91cf</p>\n",
 "<p>Create the adapter </p>\n": "<p>\u521b\u5efa\u9002\u914d\u5668</p>\n",
 "<p>Determine the padded length. Shorter sequences will get padded. </p>\n": "<p>\u786e\u5b9a\u586b\u5145\u7684\u957f\u5ea6\u3002\u8f83\u77ed\u7684\u5e8f\u5217\u5c06\u88ab\u586b\u5145\u3002</p>\n",
 "<p>End-of-text token </p>\n": "<p>\u6587\u672c\u7ed3\u5c3e\u4ee4\u724c</p>\n",
 "<p>For results </p>\n": "<p>\u4e3a\u4e86\u7ed3\u679c</p>\n",
 "<p>Get log softmaxes </p>\n": "<p>\u83b7\u53d6\u65e5\u5fd7 softmaxes</p>\n",
 "<p>Get logits of those </p>\n": "<p>\u83b7\u53d6\u8fd9\u4e9b\u65e5\u5fd7</p>\n",
 "<p>Get model logits </p>\n": "<p>\u83b7\u53d6\u6a21\u578b\u65e5\u5fd7</p>\n",
 "<p>Get number of predicted tokens </p>\n": "<p>\u83b7\u53d6\u9884\u6d4b\u7684\u4ee3\u5e01\u6570\u91cf</p>\n",
 "<p>Get the target tokens </p>\n": "<p>\u83b7\u53d6\u76ee\u6807\u4ee3\u5e01</p>\n",
 "<p>Get the tokens with the highest probabilities </p>\n": "<p>\u83b7\u5f97\u6982\u7387\u6700\u9ad8\u7684\u4ee3\u5e01</p>\n",
 "<p>Input length </p>\n": "<p>\u8f93\u5165\u957f\u5ea6</p>\n",
 "<p>Lengths of the input sequences </p>\n": "<p>\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6</p>\n",
 "<p>Load the tokenizer </p>\n": "<p>\u52a0\u8f7d\u5206\u8bcd\u5668</p>\n",
 "<p>Log-likelihoods of the target tokens </p>\n": "<p>\u76ee\u6807\u4ee3\u5e01\u7684\u5bf9\u6570\u53ef\u80fd\u6027</p>\n",
 "<p>Loop through each request in the chunk and collect them into PyTorch tensors with paddings </p>\n": "<p>\u5faa\u73af\u904d\u5386\u533a\u5757\u4e2d\u7684\u6bcf\u4e2a\u8bf7\u6c42\uff0c\u5e76\u5c06\u5b83\u4eec\u6536\u96c6\u5230\u5e26\u586b\u5145\u7684 PyTorch \u5f20\u91cf\u4e2d</p>\n",
 "<p>Loop through requests with <span translate=no>_^_0_^_</span> number of requests at a time </p>\n": "<p>\u5faa\u73af\u6d4f\u89c8\u4e00\u6b21\u5305\u542b<span translate=no>_^_0_^_</span>\u591a\u4e2a\u8bf7\u6c42\u7684\u8bf7\u6c42</p>\n",
 "<p>Loop through the input/output pairs of the batch </p>\n": "<p>\u5faa\u73af\u6d4f\u89c8\u6279\u6b21\u7684\u8f93\u5165/\u8f93\u51fa\u5bf9</p>\n",
 "<p>Maximum number of tokens to generate </p>\n": "<p>\u8981\u751f\u6210\u7684\u4ee4\u724c\u7684\u6700\u5927\u6570\u91cf</p>\n",
 "<p>Maximum sequence length </p>\n": "<p>\u6700\u5927\u5e8f\u5217\u957f\u5ea6</p>\n",
 "<p>Padded length for the batch </p>\n": "<p>\u6279\u6b21\u7684\u586b\u5145\u957f\u5ea6</p>\n",
 "<p>Padding </p>\n": "<p>\u586b\u5145</p>\n",
 "<p>Re-order and return results </p>\n": "<p>\u91cd\u65b0\u6392\u5e8f\u5e76\u8fd4\u56de\u7ed3\u679c</p>\n",
 "<p>Remove final token </p>\n": "<p>\u79fb\u9664\u6700\u7ec8\u4ee4\u724c</p>\n",
 "<p>Reorder the requests in the descending order of the lengths, so that sequences with similar lengths are close </p>\n": "<p>\u6309\u957f\u5ea6\u7684\u964d\u5e8f\u5bf9\u8bf7\u6c42\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ee5\u4f7f\u957f\u5ea6\u76f8\u4f3c\u7684\u5e8f\u5217\u63a5\u8fd1</p>\n",
 "<p>Run </p>\n": "<p>\u8dd1</p>\n",
 "<p>Run <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a> evaluator </p>\n": "<p>\u8fd0\u884c <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">eleutherai/LM-Evaluation-Harnes</a> s \u8bc4\u4f30\u5668</p>\n",
 "<p>Size of the vocabulary </p>\n": "<p>\u8bcd\u6c47\u91cf\u7684\u5927\u5c0f</p>\n",
 "<p>The continuations for the batch </p>\n": "<p>\u8be5\u6279\u6b21\u7684\u5ef6\u7eed</p>\n",
 "<p>To store the inputs for the batch </p>\n": "<p>\u5b58\u50a8\u6279\u6b21\u7684\u8f93\u5165</p>\n",
 "<p>Truncate from left if the size exceeds the <span translate=no>_^_0_^_</span> </p>\n": "<p>\u5982\u679c\u5927\u5c0f\u8d85\u8fc7<span translate=no>_^_0_^_</span></p>\n",
 "<p>Whether there&#x27;s an exact match </p>\n": "<p>\u662f\u5426\u5b58\u5728\u5b8c\u5168\u5339\u914d</p>\n",
 "<p>padded_length = padded_length if padded_length is not None else inplen </p>\n": "<p>\u5982\u679c padded_length \u4e0d\u662f padded_length \u5219\u4e3a padded_length \u5176\u4ed6\u6ca1\u6709 inplen</p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is model </li>\n<li><span translate=no>_^_1_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_2_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_3_^_</span>  is the batch size </li>\n<li><span translate=no>_^_4_^_</span>  is the device of the model</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f\u6a21\u7279</li>\n<li><span translate=no>_^_1_^_</span>\u662f <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a></li>\n<li><span translate=no>_^_2_^_</span>\u662f\u8bcd\u6c47\u91cf\u7684\u5927\u5c0f\uff08\u8fd9\u4e0e\u5206\u8bcd\u5668\u8bcd\u6c47\u5927\u5c0f\u4e0d\u540c\uff0c\u56e0\u4e3aneox\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u5185\u5bb9\u6765\u4f7f\u5d4c\u5165\u5c42\u6a21\u578b\u5e76\u884c\u3002\uff09</li>\n<li><span translate=no>_^_3_^_</span>\u662f\u6279\u6b21\u5927\u5c0f</li>\n<li><span translate=no>_^_4_^_</span>\u662f\u8be5\u578b\u53f7\u7684\u8bbe\u5907</li></ul>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_1_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_2_^_</span>  is the batch size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span>\u662f <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a></li>\n<li><span translate=no>_^_1_^_</span>\u662f\u8bcd\u6c47\u91cf\u7684\u5927\u5c0f\uff08\u8fd9\u4e0e\u5206\u8bcd\u5668\u8bcd\u6c47\u5927\u5c0f\u4e0d\u540c\uff0c\u56e0\u4e3aneox\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u5185\u5bb9\u6765\u4f7f\u5d4c\u5165\u5c42\u6a21\u578b\u5e76\u884c\u3002\uff09</li>\n<li><span translate=no>_^_2_^_</span>\u662f\u6279\u6b21\u5927\u5c0f</li></ul>\n",
 "Code to evaluate the model on NLP tasks through lm-evaluation-harness": "\u901a\u8fc7 lm-evaluation-harness \u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u6a21\u578b\u7684\u4ee3\u7801",
 "Evaluation": "\u8bc4\u4f30"
}