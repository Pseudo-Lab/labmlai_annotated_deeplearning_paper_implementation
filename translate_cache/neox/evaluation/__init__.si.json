{
 "<h1>Evaluation</h1>\n<p>This is the code to test the model on <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a>.</p>\n<ul><li><a href=\"half_precision.html\">Evaluating half precision model on a single GPU</a></li></ul>\n": "<h1>\u0d87\u0d9c\u0dba\u0dd3\u0db8</h1>\n<p><a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">Eleutherai/LM-\u0d87\u0d9c\u0dba\u0dd3\u0db8\u0dca-\u0db4\u0da7\u0dd2</a>\u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0db4\u0dbb\u0dd3\u0d9a\u0dca\u0dc2\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dda \u0d9a\u0dda\u0dad\u0dba \u0db8\u0dd9\u0dba\u0dba\u0dd2. </p>\n<ul><li><a href=\"half_precision.html\">\u0dad\u0db1\u0dd2 GPU \u0db8\u0dad \u0d85\u0dbb\u0dca\u0db0 \u0db1\u0dd2\u0dbb\u0dc0\u0daf\u0dca\u0dba\u0dad\u0dcf \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0d87\u0d9c\u0dba\u0dd3\u0db8</a></li></ul>\n",
 "<h2>Evaluation Harness Adapter</h2>\n<p>This is based on the <a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">adapter from EleutherAI/gpt-neox</a></p>\n": "<h2>\u0d87\u0d9c\u0dba\u0dd3\u0db8\u0dca\u0db4\u0da7\u0dd2 \u0d87\u0da9\u0db4\u0dca\u0da7\u0dbb</h2>\n<p>\u0db8\u0dd9\u0dba <a href=\"https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py\">Eleutherai/GPT-neox \u0dc0\u0dd9\u0dad\u0dd2\u0db1\u0dca \u0d87\u0da9\u0db4\u0dca\u0da7\u0dbb\u0dba</a>\u0db8\u0dad \u0db4\u0daf\u0db1\u0db8\u0dca \u0dc0\u0dda</p>\n",
 "<h2>Run evaluation harness with a given model</h2>\n": "<h2>\u0daf\u0dd3\u0d87\u0dad\u0dd2 \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d9a\u0dca \u0dc3\u0db8\u0d9f \u0d87\u0d9c\u0dba\u0dd3\u0db8\u0dca \u0db4\u0da7\u0dd2 \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h2>\n",
 "<h3>Get log-likelihoods of the next tokens</h3>\n<ul><li><span translate=no>_^_0_^_</span>  List of requests containing the context and the expected continuation. </li>\n<li><span translate=no>_^_1_^_</span>  If True, disable tqdm progress bar.</li></ul>\n": "<h3>\u0d8a\u0dc5\u0d9f\u0da7\u0ddd\u0d9a\u0db1 \u0dc0\u0dbd \u0dbd\u0ddc\u0d9c\u0dca \u0dc0\u0dd3\u0db8\u0dda \u0dc3\u0db8\u0dcf\u0db1\u0d9a\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1</h3>\n<ul><li><span translate=no>_^_0_^_</span> \u0dc3\u0db1\u0dca\u0daf\u0dbb\u0dca\u0db7\u0dba \u0dc3\u0dc4 \u0d85\u0db4\u0dda\u0d9a\u0dca\u0dc2\u0dd2\u0dad \u0d85\u0d9b\u0dab\u0dca\u0da9 \u0db4\u0dd0\u0dc0\u0dd0\u0dad\u0dca\u0db8 \u0d85\u0da9\u0d82\u0d9c\u0dd4 \u0d89\u0dbd\u0dca\u0dbd\u0dd3\u0db8\u0dca \u0dbd\u0dd0\u0dba\u0dd2\u0dc3\u0dca\u0dad\u0dd4\u0dc0. </li>\n<li><span translate=no>_^_1_^_</span> \u0dc3\u0dad\u0dca\u0dba \u0db1\u0db8\u0dca, tqdm \u0db4\u0dca\u0dbb\u0d9c\u0dad\u0dd2 \u0dad\u0dd3\u0dbb\u0dd4\u0dc0 \u0d85\u0d9a\u0dca\u0dbb\u0dd3\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. </li></ul>\n",
 "<h3>Run given evaluations</h3>\n": "<h3>\u0dbd\u0db6\u0dcf\u0daf\u0dd3 \u0d87\u0dad\u0dd2 \u0d87\u0d9c\u0dba\u0dd3\u0db8\u0dca \u0d9a\u0dca\u0dbb\u0dd2\u0dba\u0dcf\u0dad\u0dca\u0db8\u0d9a \u0d9a\u0dbb\u0db1\u0dca\u0db1</h3>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p> Batch size</p>\n": "<p> \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dca\u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba</p>\n",
 "<p> Call the model</p>\n": "<p> \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0d85\u0db8\u0dad\u0db1\u0dca\u0db1</p>\n",
 "<p> Decode text from token ids</p>\n": "<p> \u0da7\u0ddd\u0d9a\u0db1\u0dca\u0dc4\u0dd0\u0db3\u0dd4\u0db1\u0dd4\u0db8\u0dca\u0db4\u0dad\u0dca \u0dc0\u0dbd\u0dd2\u0db1\u0dca \u0db4\u0dd9\u0dc5 \u0dc0\u0dd2\u0d9a\u0dda\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p> Encode a given text</p>\n": "<p> \u0daf\u0dd3\u0d87\u0dad\u0dd2 \u0db4\u0dd9\u0dc5\u0d9a\u0dca \u0d9a\u0dda\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</p>\n",
 "<p>Add configs </p>\n": "<p>\u0dc0\u0dd2\u0db1\u0dca\u0dba\u0dcf\u0dc3\u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Add padding </p>\n": "<p>\u0db4\u0dd1\u0da9\u0dd2\u0db1\u0dca\u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Add the total log-likelihoods and whether there was a match to the results </p>\n": "<p>\u0db8\u0dd4\u0dc5\u0dd4\u0dbd\u0ddc\u0d9c\u0dca-Likehoods \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 \u0dc3\u0dc4 \u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db5\u0dbd \u0dc3\u0db3\u0dc4\u0dcf \u0dad\u0dbb\u0d9c\u0dba \u0dad\u0dd2\u0db6\u0dd4\u0dab\u0dda \u0daf \u0dba\u0db1\u0dca\u0db1 </p>\n",
 "<p>All tasks if nothing is specified </p>\n": "<p>\u0d9a\u0dd2\u0dc3\u0dd2\u0dc0\u0d9a\u0dca\u0db1\u0dd2\u0dba\u0db8 \u0d9a\u0dbb \u0db1\u0ddc\u0db8\u0dd0\u0dad\u0dd2 \u0db1\u0db8\u0dca \u0dc3\u0dd2\u0dba\u0dbd\u0dd4 \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba\u0db1\u0dca </p>\n",
 "<p>Concatenate the context and continuation </p>\n": "<p>\u0dc3\u0db1\u0dca\u0daf\u0dbb\u0dca\u0db7\u0dba\u0dc3\u0dc4 \u0d85\u0d9b\u0dab\u0dca\u0da9 \u0db4\u0dd0\u0dc0\u0dd0\u0dad\u0dca\u0db8 \u0dc3\u0d82\u0dba\u0dd4\u0d9a\u0dca\u0dad \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create a tensor </p>\n": "<p>\u0d86\u0dad\u0dad\u0dd2\u0dba\u0d9a\u0dca\u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create the adapter </p>\n": "<p>\u0d87\u0da9\u0dd0\u0db4\u0dca\u0da7\u0dbb\u0dba\u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Determine the padded length. Shorter sequences will get padded. </p>\n": "<p>\u0db4\u0dd1\u0da9\u0dca\u0daf\u0dd2\u0d9c \u0dad\u0dd3\u0dbb\u0dab\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0d9a\u0dd9\u0da7\u0dd2 \u0d85\u0db1\u0dd4\u0db4\u0dd2\u0dc5\u0dd2\u0dc0\u0dd9\u0dbd\u0dc0\u0dbd\u0dca \u0db4\u0dd1\u0da9\u0dca \u0dbd\u0dd0\u0db6\u0dd9\u0db1\u0dd4 \u0d87\u0dad. </p>\n",
 "<p>End-of-text token </p>\n": "<p>\u0db4\u0dd9\u0dc5\u0d85\u0dc0\u0dc3\u0db1\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba </p>\n",
 "<p>For results </p>\n": "<p>\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0db5\u0dbd\u0dc3\u0db3\u0dc4\u0dcf </p>\n",
 "<p>Get log softmaxes </p>\n": "<p>\u0dbd\u0ddc\u0d9c\u0dca\u0dc3\u0ddc\u0dc6\u0dca\u0da7\u0dca\u0db8\u0dd0\u0d9a\u0dca\u0dc3\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get logits of those </p>\n": "<p>\u0d85\u0dba\u0d9c\u0dda\u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get model logits </p>\n": "<p>\u0d86\u0daf\u0dbb\u0dca\u0dc1\u0db4\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get number of predicted tokens </p>\n": "<p>\u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba\u0d9a\u0dc5 \u0da7\u0ddd\u0d9a\u0db1 \u0d9c\u0dab\u0db1 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the target tokens </p>\n": "<p>\u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0d9c\u0dad\u0da7\u0ddd\u0d9a\u0db1 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Get the tokens with the highest probabilities </p>\n": "<p>\u0d89\u0dc4\u0dc5\u0db8\u0dc3\u0db8\u0dca\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf\u0dc0\u0db1\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0da7\u0ddd\u0d9a\u0db1 \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Input length </p>\n": "<p>\u0d86\u0daf\u0dcf\u0db1\u0daf\u0dd2\u0d9c </p>\n",
 "<p>Lengths of the input sequences </p>\n": "<p>\u0d86\u0daf\u0dcf\u0db1\u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dc0\u0dbd \u0daf\u0dd2\u0d9c </p>\n",
 "<p>Load the tokenizer </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca\u0db4\u0da7\u0dc0\u0db1\u0dca\u0db1 </p>\n",
 "<p>Log-likelihoods of the target tokens </p>\n": "<p>\u0d89\u0dbd\u0d9a\u0dca\u0d9a\u0d9c\u0dad\u0da7\u0ddd\u0d9a\u0db1 \u0dc0\u0dbd \u0dbd\u0ddc\u0d9c\u0dca \u0dc0\u0dd3\u0db8\u0dda \u0dc4\u0dd0\u0d9a\u0dd2\u0dba\u0dcf\u0dc0 </p>\n",
 "<p>Loop through each request in the chunk and collect them into PyTorch tensors with paddings </p>\n": "<p>\u0d9a\u0dd4\u0da7\u0dca\u0da7\u0dd2\u0dba\u0dda\u0d87\u0dad\u0dd2 \u0d91\u0d9a\u0dca \u0d91\u0d9a\u0dca \u0d89\u0dbd\u0dca\u0dbd\u0dd3\u0db8 \u0dc4\u0dbb\u0dc4\u0dcf \u0dbd\u0dd6\u0db4\u0dca \u0d9a\u0dbb \u0d92\u0dc0\u0dcf \u0db4\u0dd1\u0da9\u0dd2\u0db1\u0dca \u0dc3\u0db8\u0d9f \u0db4\u0dba\u0dd2\u0da7\u0ddd\u0da0\u0dca \u0da7\u0dd9\u0db1\u0dca\u0dc3\u0dbb\u0dca\u0dc0\u0dbd\u0da7 \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Loop through requests with <span translate=no>_^_0_^_</span> number of requests at a time </p>\n": "<p>\u0dc0\u0dbb\u0d9a\u0da7\u0d89\u0dbd\u0dca\u0dbd\u0dd3\u0db8\u0dca <span translate=no>_^_0_^_</span> \u0d9c\u0dab\u0db1\u0dcf\u0dc0\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad \u0d89\u0dbd\u0dca\u0dbd\u0dd3\u0db8\u0dca \u0dc4\u0dbb\u0dc4\u0dcf \u0dba\u0dd0\u0dc0\u0dd3\u0db8\u0d9a\u0dca </p>\n",
 "<p>Loop through the input/output pairs of the batch </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dda\u0d86\u0daf\u0dcf\u0db1/\u0db4\u0dca\u0dbb\u0dad\u0dd2\u0daf\u0dcf\u0db1 \u0dba\u0dd4\u0d9c\u0dbd \u0dc4\u0dbb\u0dc4\u0dcf \u0dbd\u0dd6\u0db4 </p>\n",
 "<p>Maximum number of tokens to generate </p>\n": "<p>\u0d8b\u0dad\u0dca\u0db4\u0dcf\u0daf\u0db1\u0dba\u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0d8b\u0db4\u0dbb\u0dd2\u0db8 \u0da7\u0ddd\u0d9a\u0db1 \u0d9c\u0dab\u0db1 </p>\n",
 "<p>Maximum sequence length </p>\n": "<p>\u0d8b\u0db4\u0dbb\u0dd2\u0db8\u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba \u0daf\u0dd2\u0d9c </p>\n",
 "<p>Padded length for the batch </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dc3\u0db3\u0dc4\u0dcf \u0db4\u0dd1\u0da9\u0dca \u0daf\u0dd2\u0d9c </p>\n",
 "<p>Padding </p>\n": "<p>\u0db4\u0dd1\u0da9\u0dd2\u0db1\u0dca </p>\n",
 "<p>Re-order and return results </p>\n": "<p>\u0db1\u0dd0\u0dc0\u0dad\u0d87\u0dab\u0dc0\u0dd4\u0db8\u0dca \u0d9a\u0dbb \u0db1\u0dd0\u0dc0\u0dad \u0db4\u0dca\u0dbb\u0dad\u0dd2. \u0dbd </p>\n",
 "<p>Remove final token </p>\n": "<p>\u0d85\u0dc0\u0dc3\u0dcf\u0db1\u0da7\u0ddd\u0d9a\u0db1\u0dba \u0d89\u0dc0\u0dad\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Reorder the requests in the descending order of the lengths, so that sequences with similar lengths are close </p>\n": "<p>\u0daf\u0dd2\u0d9c\u0db4\u0dc4\u0dc5 \u0d85\u0db1\u0dd4\u0db4\u0dd2\u0dc5\u0dd2\u0dc0\u0dd9\u0dbd\u0dd9\u0dc4\u0dd2 \u0d89\u0dbd\u0dca\u0dbd\u0dd3\u0db8\u0dca \u0db1\u0dd0\u0dc0\u0dad \u0dc3\u0d9a\u0dc3\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1, \u0d91\u0dc0\u0dd2\u0da7 \u0dc3\u0db8\u0dcf\u0db1 \u0daf\u0dd2\u0d9c \u0dc3\u0dc4\u0dd2\u0dad \u0d85\u0db1\u0dd4\u0d9a\u0dca\u0dbb\u0db8\u0dba\u0db1\u0dca \u0dc3\u0db8\u0dd3\u0db4 \u0dc0\u0dda </p>\n",
 "<p>Run </p>\n": "<p>\u0daf\u0dd4\u0dc0\u0db1\u0dca\u0db1 </p>\n",
 "<p>Run <a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">EleutherAI/lm-evaluation-harness</a> evaluator </p>\n": "<p><a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">Eleutherai/LM \u0d87\u0d9c\u0dba\u0dd3\u0db8-\u0db4\u0da7\u0dd2</a> \u0d87\u0d9c\u0dba\u0dd4\u0db8\u0dca\u0d9a\u0dbb\u0dd4 \u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Size of the vocabulary </p>\n": "<p>\u0dc0\u0da0\u0db1\u0db8\u0dcf\u0dbd\u0dcf\u0dc0\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba </p>\n",
 "<p>The continuations for the batch </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0d9b\u0dab\u0dca\u0da9\u0dc0 </p>\n",
 "<p>To store the inputs for the batch </p>\n": "<p>\u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8\u0dc3\u0db3\u0dc4\u0dcf \u0dba\u0dd9\u0daf\u0dc0\u0dd4\u0db8\u0dca \u0d9c\u0db6\u0da9\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 </p>\n",
 "<p>Truncate from left if the size exceeds the <span translate=no>_^_0_^_</span> </p>\n": "<p>\u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0d89\u0d9a\u0dca\u0db8\u0dc0\u0dcf \u0d9c\u0dd2\u0dba\u0dc4\u0ddc\u0dad\u0dca \u0dc0\u0db8\u0dda \u0dc3\u0dd2\u0da7 \u0d9a\u0db4\u0dcf \u0d9c\u0db1\u0dca\u0db1 <span translate=no>_^_0_^_</span> </p>\n",
 "<p>Whether there&#x27;s an exact match </p>\n": "<p>\u0db1\u0dd2\u0dc1\u0dca\u0da0\u0dd2\u0dad\u0d9c\u0dd0\u0dbd\u0db4\u0dd3\u0db8\u0d9a\u0dca \u0dad\u0dd2\u0db6\u0dda\u0daf \u0dba\u0db1\u0dca\u0db1 </p>\n",
 "<p>padded_length = padded_length if padded_length is not None else inplen </p>\n": "<p>padded_length= padded_length \u0db1\u0db8\u0dca padded_length \u0dc0\u0dd9\u0db1 \u0d9a\u0dd2\u0dc3\u0dd2\u0dc0\u0d9a\u0dca \u0db1\u0dd0\u0dad </p>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is model </li>\n<li><span translate=no>_^_1_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_2_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_3_^_</span>  is the batch size </li>\n<li><span translate=no>_^_4_^_</span>  is the device of the model</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc0\u0dda </li>\n<li><span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 <a href=\"huggingface/tokenizers\">\u0dc4\u0d9c\u0dd2\u0d82\u0dc6\u0dda\u0dc3\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca</a> \u0dba </li>\n<li><span translate=no>_^_2_^_</span> \u0dba\u0db1\u0dd4 \u0dc0\u0da0\u0db1 \u0db8\u0dcf\u0dbd\u0dcf\u0dc0\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2 (\u0db8\u0dd9\u0dba \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0dc0\u0ddc\u0d9a\u0dcf\u0db6\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0da7 \u0dc0\u0da9\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0dc0\u0db1\u0dca\u0db1\u0dda \u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc3\u0db8\u0dcf\u0db1\u0dca\u0dad\u0dbb\u0dc0 \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db8\u0dad\u0dbb \u0d85\u0db8\u0dad\u0dbb \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0d9a\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dd2.) </li>\n<li><span translate=no>_^_3_^_</span> \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc0\u0dda </li>\n</ul><li><span translate=no>_^_4_^_</span> \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0dda \u0d8b\u0db4\u0dcf\u0d82\u0d9c\u0dba \u0dc0\u0dda</li>\n",
 "<ul><li><span translate=no>_^_0_^_</span>  is the <a href=\"huggingface/tokenizers\">Huggingface Tokenizer</a> </li>\n<li><span translate=no>_^_1_^_</span>  is the size of the vocabulary  (this differs from the tokenizer vocab size since neox adds some extra to make the embedding layer  model parallel.) </li>\n<li><span translate=no>_^_2_^_</span>  is the batch size</li></ul>\n": "<ul><li><span translate=no>_^_0_^_</span> \u0dba\u0db1\u0dd4 <a href=\"huggingface/tokenizers\">\u0dc4\u0d9c\u0dd2\u0d82\u0dc6\u0dda\u0dc3\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca</a> \u0dba </li>\n<li><span translate=no>_^_1_^_</span> \u0dba\u0db1\u0dd4 \u0dc0\u0da0\u0db1 \u0db8\u0dcf\u0dbd\u0dcf\u0dc0\u0dda \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0dba\u0dd2 (\u0db8\u0dd9\u0dba \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0dc0\u0ddc\u0d9a\u0dcf\u0db6\u0dca \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0da7 \u0dc0\u0da9\u0dcf \u0dc0\u0dd9\u0db1\u0dc3\u0dca \u0dc0\u0db1\u0dca\u0db1\u0dda \u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0d9a\u0dcf\u0dc0\u0dd0\u0daf\u0dca\u0daf\u0dd3\u0db8\u0dda \u0dc3\u0dca\u0dae\u0dbb \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc3\u0db8\u0dcf\u0db1\u0dca\u0dad\u0dbb\u0dc0 \u0dc3\u0dd2\u0daf\u0dd4 \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0db8\u0dad\u0dbb \u0d85\u0db8\u0dad\u0dbb \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba\u0d9a\u0dca \u0d91\u0d9a\u0dad\u0dd4 \u0d9a\u0dbb\u0db1 \u0db6\u0dd0\u0dc0\u0dd2\u0db1\u0dd2.) </li>\n<li><span translate=no>_^_2_^_</span> \u0d9a\u0dab\u0dca\u0da9\u0dcf\u0dba\u0db8 \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0dba \u0dc0\u0dda</li></ul>\n",
 "Code to evaluate the model on NLP tasks through lm-evaluation-harness": "Lm-\u0d87\u0d9c\u0dba\u0dd3\u0db8-\u0db4\u0da7\u0dd2 \u0dc4\u0dbb\u0dc4\u0dcf NLP \u0d9a\u0dcf\u0dbb\u0dca\u0dba\u0dba\u0db1\u0dca \u0db4\u0dd2\u0dc5\u0dd2\u0db6\u0db3 \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0d87\u0d9c\u0dba\u0dd3\u0db8 \u0dc3\u0db3\u0dc4\u0dcf \u0d9a\u0dda\u0dad\u0dba",
 "Evaluation": "\u0d87\u0d9c\u0dba\u0dd3\u0db8"
}