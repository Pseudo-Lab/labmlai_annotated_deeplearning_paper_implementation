{
 "<h1>Evaluate GPT-NeoX using LLM.int8() quantization on test suite</h1>\n<p>This code evaluate <a href=\"../index.html\">GPT-NeoX</a> using <a href=\"../utils/llm_int8.html\">LLM.int8() quantization</a>, on a suite of tasks.</p>\n": "<h1>\u5728\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u4f7f\u7528 llm.int8 () \u91cf\u5316\u6765\u8bc4\u4f30 GPT-NEOX</h1>\n<p>\u8fd9\u6bb5\u4ee3\u7801\u5728\u4e00\u5957\u4efb\u52a1\u4e2d\u4f7f\u7528 <a href=\"../utils/llm_int8.html\">llm.int8 () \u91cf\u5316</a>\u6765\u8bc4\u4f30 <a href=\"../index.html\">GPT-NEOX</a>\u3002</p>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> model </p>\n": "<p>\u521b\u5efa<span translate=no>_^_0_^_</span>\u6a21\u578b</p>\n",
 "<p>Device </p>\n": "<p>\u8bbe\u5907</p>\n",
 "<p>Load layers </p>\n": "<p>\u52a0\u8f7d\u56fe\u5c42</p>\n",
 "<p>Load layers in float16 into CPU. We convert the layers to int8 later, because doing that on the fly after loading layers to GPU causes CUDA memory fragmentation (about 3GB memory can get lost due to fragmentation). </p>\n": "\u5c06 <p>float16 \u4e2d\u7684\u5c42\u52a0\u8f7d\u5230 CPU \u4e2d\u3002\u6211\u4eec\u7a0d\u540e\u5c06\u56fe\u5c42\u8f6c\u6362\u4e3aint8\uff0c\u56e0\u4e3a\u5728\u5c06\u56fe\u5c42\u52a0\u8f7d\u5230GPU\u540e\u5373\u65f6\u6267\u884c\u6b64\u64cd\u4f5c\u4f1a\u5bfc\u81f4CUDA\u5185\u5b58\u788e\u7247\uff08\u5927\u7ea63GB\u7684\u5185\u5b58\u53ef\u80fd\u4f1a\u7531\u4e8e\u788e\u7247\u800c\u4e22\u5931\uff09\u3002</p>\n",
 "<p>Run <a href=\"index.html\">evaluation harness</a> </p>\n": "<p>\u8fd0\u884c<a href=\"index.html\">\u8bc4\u4f30\u5de5\u5177</a></p>\n",
 "<p>This reduces CUDA memory fragmentation </p>\n": "<p>\u8fd9\u51cf\u5c11\u4e86 CUDA \u5185\u5b58\u788e\u7247</p>\n",
 "Evaluate GPT-NeoX using LLM.int8() quantization on test suite": "\u5728\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u4f7f\u7528 llm.int8 () \u91cf\u5316\u6765\u8bc4\u4f30 GPT-NEOX"
}