{
 "<h1>Generate Text with GPT-NeoX using LLM.int8() quantization</h1>\n<p>This shows how to generate text from GPT-NeoX using <a href=\"../utils/llm_int8.html\">LLM.int8() quantization</a>.</p>\n<p>This needs a GPU with 24GB memory.</p>\n": "<h1>\u4f7f\u7528 llm.int8 () \u91cf\u5316\u4f7f\u7528 GPT-NEOX \u751f\u6210\u6587\u672c</h1>\n<p>\u8fd9\u8bf4\u660e\u4e86\u5982\u4f55\u4f7f\u7528 <a href=\"../utils/llm_int8.html\">llm.int8 () \u91cf\u5316</a>\u4ece GPT-NEOX \u751f\u6210\u6587\u672c\u3002</p>\n<p>\u8fd9\u9700\u8981\u4e00\u4e2a\u5177\u6709 24GB \u5185\u5b58\u7684 GPU\u3002</p>\n",
 "<h2>Generate text</h2>\n": "<h2>\u751f\u6210\u6587\u672c</h2>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Append the predicted token </p>\n": "<p>\u8ffd\u52a0\u9884\u6d4b\u7684\u4ee4\u724c</p>\n",
 "<p>Clear cache and print memory summary for debugging </p>\n": "<p>\u6e05\u9664\u7f13\u5b58\u548c\u6253\u5370\u5185\u5b58\u6458\u8981\u4ee5\u8fdb\u884c\u8c03\u8bd5</p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> model </p>\n": "<p>\u521b\u5efa<span translate=no>_^_0_^_</span>\u6a21\u578b</p>\n",
 "<p>Device </p>\n": "<p>\u8bbe\u5907</p>\n",
 "<p>Get next token. Note that we only feed the last token to the model because we cache the key/value pairs of previous tokens. </p>\n": "<p>\u83b7\u53d6\u4e0b\u4e00\u4e2a\u4ee4\u724c\u3002\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u53ea\u5c06\u6700\u540e\u4e00\u4e2a\u4ee4\u724c\u63d0\u4f9b\u7ed9\u6a21\u578b\uff0c\u56e0\u4e3a\u6211\u4eec\u7f13\u5b58\u4e86\u5148\u524d\u4ee4\u724c\u7684\u952e/\u503c\u5bf9\u3002</p>\n",
 "<p>Get token ids </p>\n": "<p>\u83b7\u53d6\u4ee3\u5e01 ID</p>\n",
 "<p>Load layers in float16 into CPU. We convert the layers to int8 later, because doing that on the fly after loading layers to GPU causes CUDA memory fragmentation (about 3GB memory can get lost due to fragmentation). </p>\n": "\u5c06 <p>float16 \u4e2d\u7684\u5c42\u52a0\u8f7d\u5230 CPU \u4e2d\u3002\u6211\u4eec\u7a0d\u540e\u5c06\u56fe\u5c42\u8f6c\u6362\u4e3aint8\uff0c\u56e0\u4e3a\u5728\u5c06\u56fe\u5c42\u52a0\u8f7d\u5230GPU\u540e\u5373\u65f6\u6267\u884c\u6b64\u64cd\u4f5c\u4f1a\u5bfc\u81f4CUDA\u5185\u5b58\u788e\u7247\uff08\u5927\u7ea63GB\u7684\u5185\u5b58\u53ef\u80fd\u4f1a\u7531\u4e8e\u788e\u7247\u800c\u4e22\u5931\uff09\u3002</p>\n",
 "<p>Predict 100 tokens </p>\n": "<p>\u9884\u6d4b 100 \u4e2a\u4ee3\u5e01</p>\n",
 "<p>Print </p>\n": "<p>\u6253\u5370</p>\n",
 "<p>Run the model. We use the <a href=\"generate.html\"><span translate=no>_^_0_^_</span></a> function defined in <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a> </p>\n": "<p>\u8fd0\u884c\u6a21\u578b\u3002\u6211\u4eec\u4f7f\u7528\u4e2d\u5b9a\u4e49\u7684<a href=\"generate.html\"><span translate=no>_^_0_^_</span></a>\u51fd\u6570 <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a></p>\n",
 "<p>Set the state to use cached activations </p>\n": "<p>\u8bbe\u7f6e\u72b6\u6001\u4ee5\u4f7f\u7528\u7f13\u5b58\u7684\u6fc0\u6d3b</p>\n",
 "<p>Setup <a href=\"../utils/cache.html\">cache</a> to cache intermediate key/value pairs for faster generation </p>\n": "<p>\u8bbe\u7f6e<a href=\"../utils/cache.html\">\u7f13\u5b58</a>\u4ee5\u7f13\u5b58\u4e2d\u95f4\u952e/\u503c\u5bf9\u4ee5\u52a0\u5feb\u751f\u6210\u901f\u5ea6</p>\n",
 "<p>This reduces CUDA memory fragmentation </p>\n": "<p>\u8fd9\u51cf\u5c11\u4e86 CUDA \u5185\u5b58\u788e\u7247</p>\n",
 "Generate Text with GPT-NeoX using LLM.int8() quantization": "\u4f7f\u7528 llm.int8 () \u91cf\u5316\u4f7f\u7528 GPT-NEOX \u751f\u6210\u6587\u672c"
}