{
 "<h1>Generate Text with GPT-NeoX using LLM.int8() quantization</h1>\n<p>This shows how to generate text from GPT-NeoX using <a href=\"../utils/llm_int8.html\">LLM.int8() quantization</a>.</p>\n<p>This needs a GPU with 24GB memory.</p>\n": "<h1>LLM.INT8() \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca GPT-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0dc3\u0db8\u0d9f \u0db4\u0dd9\u0dc5 \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h1>\n<p><a href=\"../utils/llm_int8.html\">LLM.INT8 () \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0d9a\u0dbb\u0dab\u0dba</a>\u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca GPT-Neox \u0dc0\u0dd9\u0dad\u0dd2\u0db1\u0dca \u0db4\u0dd9\u0dc5 \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1\u0dda \u0d9a\u0dd9\u0dc3\u0dda\u0daf \u0dba\u0db1\u0dca\u0db1 \u0db8\u0dd9\u0dba\u0dd2\u0db1\u0dca \u0db4\u0dd9\u0db1\u0dca\u0dc0\u0dba\u0dd2. </p>\n<p>\u0db8\u0dda\u0dc3\u0db3\u0dc4\u0dcf 24GB \u0db8\u0dad\u0d9a\u0dba\u0d9a\u0dca \u0dc3\u0dc4\u0dd2\u0dad GPU \u0d91\u0d9a\u0d9a\u0dca \u0d85\u0dc0\u0dc1\u0dca\u0dba \u0dc0\u0dda. </p>\n",
 "<h2>Generate text</h2>\n": "<h2>\u0db4\u0dd9\u0dc5\u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1</h2>\n",
 "<p> </p>\n": "<p> </p>\n",
 "<p>Append the predicted token </p>\n": "<p>\u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba\u0d9a\u0dc5 \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0d91\u0d9a\u0dca \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Clear cache and print memory summary for debugging </p>\n": "<p>\u0db1\u0dd2\u0daf\u0ddc\u0dc3\u0dca\u0d9a\u0dbb\u0dab\u0dba\u0dc3\u0db3\u0dc4\u0dcf \u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2 \u0dc3\u0dc4 \u0db8\u0dd4\u0daf\u0dca\u0dbb\u0dd2\u0dad \u0db8\u0dad\u0d9a \u0dc3\u0dcf\u0dbb\u0dcf\u0d82\u0dc1\u0dba \u0db4\u0dd0\u0dc4\u0dd0\u0daf\u0dd2\u0dbd\u0dd2 \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> model </p>\n": "<p><span translate=no>_^_0_^_</span> \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba \u0dc3\u0dcf\u0daf\u0db1\u0dca\u0db1 </p>\n",
 "<p>Device </p>\n": "<p>\u0d8b\u0db4\u0dcf\u0d82\u0d9c\u0dba </p>\n",
 "<p>Get next token. Note that we only feed the last token to the model because we cache the key/value pairs of previous tokens. </p>\n": "<p>\u0d8a\u0dc5\u0d9f\u0da7\u0ddd\u0d9a\u0db1\u0dba \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1. \u0db4\u0dd9\u0dbb \u0da7\u0ddd\u0d9a\u0db1 \u0dc0\u0dbd \u0dba\u0dad\u0dd4\u0dbb/\u0d85\u0d9c\u0dba \u0dba\u0dd4\u0d9c\u0dbd \u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2 \u0d9a\u0dbb\u0db1 \u0db1\u0dd2\u0dc3\u0dcf \u0d85\u0db4\u0dd2 \u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0da7 \u0d85\u0dc0\u0dc3\u0dcf\u0db1 \u0da7\u0ddd\u0d9a\u0db1\u0dba \u0db4\u0db8\u0dab\u0d9a\u0dca \u0db4\u0ddd\u0dc2\u0dab\u0dba \u0d9a\u0dbb\u0db1 \u0db6\u0dc0 \u0dc3\u0dbd\u0d9a\u0db1\u0dca\u0db1. </p>\n",
 "<p>Get token ids </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1\u0dca\u0dc4\u0dd0\u0db3\u0dd4\u0db1\u0dd4\u0db8\u0dca\u0db4\u0dad\u0dca \u0dbd\u0db6\u0dcf \u0d9c\u0db1\u0dca\u0db1 </p>\n",
 "<p>Load layers in float16 into CPU. We convert the layers to int8 later, because doing that on the fly after loading layers to GPU causes CUDA memory fragmentation (about 3GB memory can get lost due to fragmentation). </p>\n": "<p>\u0db4\u0dcf\u0dc0\u0dd9\u0db116 \u0dc4\u0dd2 \u0dc3\u0dca\u0dae\u0dbb CPU \u0dad\u0dd4\u0dc5\u0da7 \u0db4\u0da7\u0dc0\u0db1\u0dca\u0db1. \u0d85\u0db4\u0dd2 \u0dc3\u0dca\u0dae\u0dbb \u0db4\u0dc3\u0dd4\u0dc0 int8 \u0db6\u0dc0\u0da7 \u0db4\u0dbb\u0dd2\u0dc0\u0dbb\u0dca\u0dad\u0db1\u0dba \u0d9a\u0dbb\u0db8\u0dd4, \u0db8\u0db1\u0dca\u0daf \u0dc3\u0dca\u0dae\u0dbb GPU \u0dc0\u0dd9\u0dad \u0db4\u0dd0\u0da7\u0dc0\u0dd3\u0db8\u0dd9\u0db1\u0dca \u0db4\u0dc3\u0dd4 \u0db4\u0dd2\u0dba\u0dcf\u0dc3\u0dbb \u0d9a\u0dd2\u0dbb\u0dd3\u0db8 CUDA \u0db8\u0dad\u0d9a \u0d9b\u0dab\u0dca\u0da9\u0db1\u0dba \u0dc0\u0dd3\u0db8\u0da7 \u0dc4\u0dda\u0dad\u0dd4 \u0dc0\u0dda (3GB \u0db4\u0db8\u0dab \u0db8\u0dad\u0d9a\u0dba \u0d9a\u0dd0\u0db6\u0dbd\u0dd2 \u0dc0\u0dd3\u0db8 \u0db1\u0dd2\u0dc3\u0dcf \u0d85\u0dc4\u0dd2\u0db8\u0dd2 \u0dc0\u0dd2\u0dba \u0dc4\u0dd0\u0d9a). </p>\n",
 "<p>Predict 100 tokens </p>\n": "<p>\u0da7\u0ddd\u0d9a\u0db1100 \u0d9a\u0dca \u0db4\u0dd4\u0dbb\u0ddd\u0d9a\u0dae\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1 </p>\n",
 "<p>Print </p>\n": "<p>\u0db8\u0dd4\u0daf\u0dca\u0dbb\u0dab\u0dba </p>\n",
 "<p>Run the model. We use the <a href=\"generate.html\"><span translate=no>_^_0_^_</span></a> function defined in <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a> </p>\n": "<p>\u0d86\u0d9a\u0dd8\u0dad\u0dd2\u0dba\u0db0\u0dcf\u0dc0\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1. \u0d85\u0db4\u0dd2 \u0d85\u0dbb\u0dca\u0dae \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0d87\u0dad\u0dd2 <a href=\"generate.html\"><span translate=no>_^_0_^_</span></a> \u0dc1\u0dca\u0dbb\u0dd2\u0dad\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd4 <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a> </p>\n",
 "<p>Set the state to use cached activations </p>\n": "<p>\u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2\u0dc3\u0d9a\u0dca\u0dbb\u0dd2\u0dba \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0dca \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dbb\u0dcf\u0da2\u0dca\u0dba\u0dba \u0dc3\u0d9a\u0dc3\u0db1\u0dca\u0db1 </p>\n",
 "<p>Setup <a href=\"../utils/cache.html\">cache</a> to cache intermediate key/value pairs for faster generation </p>\n": "<p>\u0dc0\u0dda\u0d9c\u0dc0\u0dad\u0dca\u0d8b\u0dad\u0dca\u0db4\u0dcf\u0daf\u0db1\u0dba \u0dc3\u0db3\u0dc4\u0dcf \u0d85\u0dad\u0dbb\u0db8\u0dd0\u0daf\u0dd2 \u0dba\u0dad\u0dd4\u0dbb/\u0dc0\u0da7\u0dd2\u0db1\u0dcf\u0d9a\u0db8\u0dca \u0dba\u0dd4\u0d9c\u0dbd <a href=\"../utils/cache.html\">\u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2</a> \u0d9a\u0dd2\u0dbb\u0dd3\u0db8\u0da7 \u0dc4\u0dd0\u0db9\u0dd2\u0dbd\u0dd2\u0dba \u0db4\u0dd2\u0dc4\u0dd2\u0da7\u0dd4\u0dc0\u0db1\u0dca\u0db1 </p>\n",
 "<p>This reduces CUDA memory fragmentation </p>\n": "<p>\u0db8\u0dd9\u0dbaCUDA \u0db8\u0dad\u0d9a \u0d9b\u0dab\u0dca\u0da9\u0db1\u0dba \u0d85\u0da9\u0dd4 \u0d9a\u0dbb\u0dba\u0dd2 </p>\n",
 "Generate Text with GPT-NeoX using LLM.int8() quantization": "LLM.INT8 () \u0db4\u0dca\u0dbb\u0db8\u0dcf\u0dab\u0d9a\u0dbb\u0dab\u0dba \u0db7\u0dcf\u0dc0\u0dd2\u0dad\u0dcf \u0d9a\u0dbb\u0db8\u0dd2\u0db1\u0dca GPT-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0dc3\u0db8\u0d9f \u0db4\u0dd9\u0dc5 \u0da2\u0db1\u0db1\u0dba \u0d9a\u0dbb\u0db1\u0dca\u0db1"
}