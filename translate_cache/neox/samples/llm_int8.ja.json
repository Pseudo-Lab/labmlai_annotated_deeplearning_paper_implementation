{
 "<h1>Generate Text with GPT-NeoX using LLM.int8() quantization</h1>\n<p>This shows how to generate text from GPT-NeoX using <a href=\"../utils/llm_int8.html\">LLM.int8() quantization</a>.</p>\n<p>This needs a GPU with 24GB memory.</p>\n": "<h1>LLM.int8 () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066 GPT-Neox \u3067\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210</h1>\n<p>\u3053\u308c\u306f\u3001<a href=\"../utils/llm_int8.html\">LLM.int8</a> () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066 GPT-Neox \u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\u3059\u308b\u65b9\u6cd5\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p>\n<p>\u3053\u308c\u306b\u306f 24 GB \u306e\u30e1\u30e2\u30ea\u3092\u642d\u8f09\u3057\u305f GPU \u304c\u5fc5\u8981\u3067\u3059\u3002</p>\n",
 "<h2>Generate text</h2>\n": "<h2>\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210</h2>\n",
 "<p> </p>\n": "<p></p>\n",
 "<p>Append the predicted token </p>\n": "<p>\u4e88\u6e2c\u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0</p>\n",
 "<p>Clear cache and print memory summary for debugging </p>\n": "<p>\u30c7\u30d0\u30c3\u30b0\u7528\u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u30af\u30ea\u30a2\u3057\u3066\u30e1\u30e2\u30ea\u306e\u6982\u8981\u3092\u5370\u5237</p>\n",
 "<p>Create <span translate=no>_^_0_^_</span> model </p>\n": "<p><span translate=no>_^_0_^_</span>\u30e2\u30c7\u30eb\u4f5c\u6210</p>\n",
 "<p>Device </p>\n": "<p>\u7aef\u672b</p>\n",
 "<p>Get next token. Note that we only feed the last token to the model because we cache the key/value pairs of previous tokens. </p>\n": "<p>\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u5165\u624b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u524d\u306e\u30c8\u30fc\u30af\u30f3\u306e\u30ad\u30fc\u3068\u5024\u306e\u30da\u30a2\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u306e\u3067\u3001\u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u307f\u3092\u30e2\u30c7\u30eb\u306b\u30d5\u30a3\u30fc\u30c9\u3059\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044</p>\u3002\n",
 "<p>Get token ids </p>\n": "<p>\u30c8\u30fc\u30af\u30f3 ID \u3092\u53d6\u5f97</p>\n",
 "<p>Load layers in float16 into CPU. We convert the layers to int8 later, because doing that on the fly after loading layers to GPU causes CUDA memory fragmentation (about 3GB memory can get lost due to fragmentation). </p>\n": "<p>float16 \u306e\u30ec\u30a4\u30e4\u30fc\u3092 CPU \u306b\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u30ec\u30a4\u30e4\u30fc\u3092GPU\u306b\u30ed\u30fc\u30c9\u3057\u305f\u5f8c\u306b\u305d\u306e\u5834\u3067\u3053\u308c\u3092\u884c\u3046\u3068\u3001CUDA\u30e1\u30e2\u30ea\u306e\u65ad\u7247\u5316\u304c\u767a\u751f\u3059\u308b\u305f\u3081\u3001\u5f8c\u3067\u30ec\u30a4\u30e4\u30fc\u3092int8\u306b\u5909\u63db\u3057\u307e\u3059\uff08\u30d5\u30e9\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308a\u7d043GB\u306e\u30e1\u30e2\u30ea\u304c\u5931\u308f\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059</p>\uff09\u3002\n",
 "<p>Predict 100 tokens </p>\n": "<p>\u30c8\u30fc\u30af\u30f3\u3092100\u500b\u4e88\u6e2c\u3059\u308b</p>\n",
 "<p>Print </p>\n": "<p>\u30d7\u30ea\u30f3\u30c8</p>\n",
 "<p>Run the model. We use the <a href=\"generate.html\"><span translate=no>_^_0_^_</span></a> function defined in <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a> </p>\n": "<p>\u30e2\u30c7\u30eb\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002<a href=\"generate.html\"><span translate=no>_^_0_^_</span></a>\u3067\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3059 <a href=\"generate.html\"><span translate=no>_^_1_^_</span></a></p>\n",
 "<p>Set the state to use cached activations </p>\n": "<p>\u30ad\u30e3\u30c3\u30b7\u30e5\u3055\u308c\u305f\u30a2\u30af\u30c6\u30a3\u30d9\u30fc\u30b7\u30e7\u30f3\u3092\u4f7f\u7528\u3059\u308b\u3088\u3046\u306b\u72b6\u614b\u3092\u8a2d\u5b9a\u3057\u307e\u3059</p>\n",
 "<p>Setup <a href=\"../utils/cache.html\">cache</a> to cache intermediate key/value pairs for faster generation </p>\n": "<p><a href=\"../utils/cache.html\">\u751f\u6210\u3092\u9ad8\u901f\u5316\u3059\u308b\u305f\u3081\u306b\u4e2d\u9593\u30ad\u30fc\u3068\u5024\u306e\u30da\u30a2\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u3088\u3046\u306b\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u8a2d\u5b9a</a></p>\n",
 "<p>This reduces CUDA memory fragmentation </p>\n": "<p>\u3053\u308c\u306b\u3088\u308a\u3001CUDA \u30e1\u30e2\u30ea\u306e\u65ad\u7247\u5316\u304c\u6e1b\u5c11\u3057\u307e\u3059\u3002</p>\n",
 "Generate Text with GPT-NeoX using LLM.int8() quantization": "LLM.int8 () \u91cf\u5b50\u5316\u3092\u4f7f\u7528\u3057\u3066 GPT-Neox \u3067\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210"
}