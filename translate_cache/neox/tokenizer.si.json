{
 "<h1>GPT-NeoX Tokenizer</h1>\n<p>This initializes a Hugging Face tokenizer from the downloaded vocabulary.</p>\n": "<h1>\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca\u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca</h1>\n<p>\u0db8\u0dd9\u0dba\u0db6\u0dcf\u0d9c\u0dad \u0d9a\u0dc5 \u0dc0\u0da0\u0db1 \u0db8\u0dcf\u0dbd\u0dcf\u0dc0\u0dd9\u0db1\u0dca \u0dc4\u0dd4\u0da2\u0dd2\u0d82 \u0dc6\u0dda\u0dc3\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0d86\u0dbb\u0db8\u0dca\u0db7 \u0d9a\u0dbb\u0dba\u0dd2. </p>\n",
 "<h3>Load NeoX Tokenizer</h3>\n<ul><p><em>Returns</em>  the tokenizer</p></ul>\n": "<h3>Neox\u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0db4\u0dd0\u0da7\u0dc0\u0dd3\u0db8</h3>\n<ul><p>\u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca<em>\u0d86\u0db4\u0dc3\u0dd4 \u0dbd\u0db6\u0dcf \u0daf\u0dd9\u0dba\u0dd2</em> </p></ul>\n",
 "GPT-NeoX Tokenizer": "\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca",
 "Loads the GPT-NeoX tokenizer": "\u0da2\u0dd3\u0db4\u0dd3\u0da7\u0dd3-\u0db1\u0dd2\u0dba\u0ddd\u0d9a\u0dca\u0dc3\u0dca \u0da7\u0ddd\u0d9a\u0db1\u0dba\u0dd2\u0dc3\u0dbb\u0dca \u0db4\u0da7\u0dc0\u0db1\u0dd4 \u0dbd\u0dd0\u0db6\u0dda"
}